<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/75406198 -->
<html lang="zh" data-hairline="true" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" content="最近在工作中碰到一些序列标注任务，如命名实体识别（Name Entity Recognition，NER）、中文分词（Chinese Word Segmentation，CWS）和词性标注（Part-Of-Speech Tagging, PosTag）等。序列标注任务，简而言之，就…"><meta data-react-helmet="true" property="og:title" content="概率图模型系列-隐马尔可夫模型（HMM）的numpy实现"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/75406198"><meta data-react-helmet="true" property="og:description" content="最近在工作中碰到一些序列标注任务，如命名实体识别（Name Entity Recognition，NER）、中文分词（Chinese Word Segmentation，CWS）和词性标注（Part-Of-Speech Tagging, PosTag）等。序列标注任务，简而言之，就…"><meta data-react-helmet="true" property="og:image" content=""><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/app.391b8103ca4b736cf82c.css" rel="stylesheet"><link rel="stylesheet"><script defer="" crossorigin="anonymous" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;2585-63ecbf50&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/modals.e7bf80400b159ca850db.css"><script charset="utf-8" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/column.modals.733fdf68dd8fa9bbbad9.js"></script><link rel="stylesheet" type="text/css" href="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/richinput.f881b5e9f1d8fbc8b13c.css"><script charset="utf-8" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/column.richinput.0f8bc574e96e204701ab.js"></script></head><body class="WhiteBg-body" data-react-helmet="class"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;soko-60&quot;}" data-zop="{&quot;authorName&quot;:&quot;壮哉我贾诩文和&quot;,&quot;itemId&quot;:75406198,&quot;title&quot;:&quot;概率图模型系列-隐马尔可夫模型（HMM）的numpy实现&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;75406198&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1478.4px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/liushaoweihua"><img class="Avatar Avatar--round" width="30" height="30" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/4b70deef7_is.jpg" srcset="https://pic2.zhimg.com/4b70deef7_im.jpg 2x" alt="那就权当做是个记笔记的地方吧"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/liushaoweihua">那就权当做是个记笔记的地方吧</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button FollowButton ColumnPageHeader-FollowButton Button--primary Button--blue">关注专栏</button><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button><div class="Popover"><button title="更多" type="button" id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg class="Zi Zi--Dots" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">概率图模型系列-隐马尔可夫模型（HMM）的numpy实现</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="壮哉我贾诩文和"><meta itemprop="image" content="https://pic2.zhimg.com/a6327e828920d145469b9daf28c877c6_l.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/di-san-zhi-jie"><meta itemprop="zhihu:followerCount"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/di-san-zhi-jie"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/a6327e828920d145469b9daf28c877c6_xs.jpg" srcset="https://pic2.zhimg.com/a6327e828920d145469b9daf28c877c6_l.jpg 2x" alt="壮哉我贾诩文和"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/di-san-zhi-jie">壮哉我贾诩文和</a></div></div><a class="UserLink-badge" data-tooltip="已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--BadgeCert" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M9.78 15.728l-2.633-2.999s-.458-.705.242-1.362c.7-.657 1.328-.219 1.328-.219l1.953 2.132 4.696-4.931s.663-.348 1.299.198c.636.545.27 1.382.27 1.382s-3.466 3.858-5.376 5.782c-.98.93-1.778.017-1.778.017z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">东南大学 交通运输工程硕士</div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--grey">已关注</button></div><div><span class="Voters"><button type="button" class="Button Button--plain">138 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>最近在工作中碰到一些<b>序列标注任务</b>，如命名实体识别（Name Entity Recognition，NER）、中文分词（Chinese Word Segmentation，CWS）和词性标注（Part-Of-Speech Tagging, PosTag）等。</p><p>序列标注任务，简而言之，就是在<b><i>给定不定长观测序列的基础上，尝试找到决定该观测序列的另一组序列</i>。</b>如给定一个一维输入序列：</p><p><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="X=x_1,x_2,...,x_i,...,x_n"> </p><p>对该序列中的每个元素打上标签的过程：</p><p><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="Y=y_1,y_2,...,y_i,...,y_n"> </p><p><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula="yi \in S"> </p><p>其本质是基于序列的每个元素的上下文内容，对序列元素进行多分类的过程。</p><h2>序列标注算法演化</h2><p>在深度学习之前，隐马尔可夫模型（HMM）、最大熵马尔科夫模型（MEMM）和条件随机场（CRF），尤其是条件随机场CRF，是进行序列标注的常用方法。这些方法的局限在于上下文理解上。</p><p>深度学习的发展，RNN、LSTM等深度模型的出现较好地解决上述缺陷，引申出了现在主流的序列标注方法：biLSTM+CRF。（提升上下文理解）</p><p>Attention机制的出现又极大地解决了循环神经网络不能并行的缺陷，并且在上下文理解上有了进一步的提升。（提升模型效率、提升上下文理解）</p><p>现在，在Bert、XLNet等大规模预训练模型大行其道的时代下，预训练模型+微调的两阶段套路基本就能达到较好的效果。（加入大规模先验）</p><h2>HMM、MEMM和CRF的区别与联系</h2><p>区别：直观理解上，HMM模型属于生成模型，是贝叶斯派（Bayesian）成果。与频率学派对条件概率 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="P(Y|X)"> 建模的思路不同，贝叶斯派的是对联合概率分布 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula="P(X,Y)"> 进行建模（learning），在模型的基础上进行推断(inference)，得到 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula="P(Y|X)"> 。MEMM和CRF均属于判别模型。</p><p>联系：</p><p><b>隐马尔可夫模型的两条重要假设就是：</b></p><p>（1）齐次马尔科夫假设： 当前隐状态仅由上一个隐状态决定。</p><p><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula="p(i_{t+1}|o_1,...,o_t,i_1,...,i_t) = p(i_{t+1}|i_t)"></p><p> （2）观测独立性假设：当前观测状态仅由当前隐状态决定。</p><p><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula="p(o_t|o_1,...,o_{t-1},i_1,...,i_t) = p(o_t|i_t)"> </p><p><b>在概率图上，HMM的输入是隐状态，输出是观测序列。</b></p><p><b>MEMM则将观测序列作为输入，隐状态作为输出，</b>在给定观测序列的情况下，<b>打破了隐马尔可夫的观测独立性假设。</b></p><p><b>HMM和MEMM均属于有向图，有向图天然的局部归一化问题会导致标注偏差问题（label bias problem）的出现，而无向图则是天然的全局归一化，因此不会落入上述现象。这也就是CRF和MEMM的唯一差别。</b></p><p><b>这里强推一个b站UP主的视频机器学习-白板推导视频，这一节详细介绍了三个模型之间的联系：</b></p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av34444816/%3Fspm_id_from%3D333.788.videocard.1" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">https://www.bilibili.com/video/av34444816/?spm_id_from=333.788.videocard.1</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>www.bilibili.com</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><h2>隐马尔可夫模型理论推导</h2><p>关于隐马尔可夫模型的细节，这里不做过多介绍，网上有很多资源，但是这里还是要再强推一下上面这个UP主的系列，本文的代码实现是基于这个系列中的<b><i>隐马尔可夫模型HMM</i></b>一节以及李航老师的<b><i>《统计学习方法》</i></b>得到。</p><a target="_blank" href="https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av32471608" data-draft-node="block" data-draft-type="link-card" class="LinkCard LinkCard--noImage" data-za-detail-view-id="172"><span class="LinkCard-content"><span class="LinkCard-text"><span class="LinkCard-title" data-text="true">https://www.bilibili.com/video/av32471608</span><span class="LinkCard-meta"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--InsertLink" fill="currentColor" viewBox="0 0 24 24" width="17" height="17"><path d="M6.77 17.23c-.905-.904-.94-2.333-.08-3.193l3.059-3.06-1.192-1.19-3.059 3.058c-1.489 1.489-1.427 3.954.138 5.519s4.03 1.627 5.519.138l3.059-3.059-1.192-1.192-3.059 3.06c-.86.86-2.289.824-3.193-.08zm3.016-8.673l1.192 1.192 3.059-3.06c.86-.86 2.289-.824 3.193.08.905.905.94 2.334.08 3.194l-3.059 3.06 1.192 1.19 3.059-3.058c1.489-1.489 1.427-3.954-.138-5.519s-4.03-1.627-5.519-.138L9.786 8.557zm-1.023 6.68c.33.33.863.343 1.177.029l5.34-5.34c.314-.314.3-.846-.03-1.176-.33-.33-.862-.344-1.176-.03l-5.34 5.34c-.314.314-.3.846.03 1.177z" fill-rule="evenodd"></path></svg></span>www.bilibili.com</span></span><span class="LinkCard-imageCell"><div class="LinkCard-image LinkCard-image--default"><svg class="Zi Zi--Browser" fill="currentColor" viewBox="0 0 24 24" width="32" height="32"><path d="M11.991 3C7.023 3 3 7.032 3 12s4.023 9 8.991 9C16.968 21 21 16.968 21 12s-4.032-9-9.009-9zm6.237 5.4h-2.655a14.084 14.084 0 0 0-1.242-3.204A7.227 7.227 0 0 1 18.228 8.4zM12 4.836A12.678 12.678 0 0 1 13.719 8.4h-3.438A12.678 12.678 0 0 1 12 4.836zM5.034 13.8A7.418 7.418 0 0 1 4.8 12c0-.621.09-1.224.234-1.8h3.042A14.864 14.864 0 0 0 7.95 12c0 .612.054 1.206.126 1.8H5.034zm.738 1.8h2.655a14.084 14.084 0 0 0 1.242 3.204A7.188 7.188 0 0 1 5.772 15.6zm2.655-7.2H5.772a7.188 7.188 0 0 1 3.897-3.204c-.54.999-.954 2.079-1.242 3.204zM12 19.164a12.678 12.678 0 0 1-1.719-3.564h3.438A12.678 12.678 0 0 1 12 19.164zm2.106-5.364H9.894A13.242 13.242 0 0 1 9.75 12c0-.612.063-1.215.144-1.8h4.212c.081.585.144 1.188.144 1.8 0 .612-.063 1.206-.144 1.8zm.225 5.004c.54-.999.954-2.079 1.242-3.204h2.655a7.227 7.227 0 0 1-3.897 3.204zm1.593-5.004c.072-.594.126-1.188.126-1.8 0-.612-.054-1.206-.126-1.8h3.042c.144.576.234 1.179.234 1.8s-.09 1.224-.234 1.8h-3.042z"></path></svg></div></span></span></a><h2>隐马尔可夫模型代码实现</h2><p>本文代码主要分为三个部分：</p><p>一、Evaluating任务：Forward-Backward算法</p><p>二、Learning任务：Baum-Welch算法</p><p>三、Decoding任务：Viterbi算法</p><p>具体算法的公式推导已经附在各个函数里。</p><div class="highlight"><pre><code class="language-text">import numpy as np
import logging
logging.basicConfig(level = logging.INFO, format = '%(asctime)s - %(name)s - %(message)s')

class HMM(object):
    
    def __init__(self,num_latent_states,num_observation_states,**kwargs):
        self.num_latent_states = num_latent_states
        self.num_observation_states = num_observation_states
        # 初始概率分布 init_prob_dist
        if 'init_prob_dist' not in kwargs:
            self.init_prob_dist = np.random.random((self.num_latent_states,))
            self.init_prob_dist = self.init_prob_dist/np.sum(self.init_prob_dist)
        else:
            self.init_prob_dist = init_prob_dist
        # 状态转移矩阵 state_trans_matrix
        if 'state_trans_matrix' not in kwargs:
            self.state_trans_matrix = np.ones((self.num_latent_states,self.num_latent_states))
            self.state_trans_matrix = (self.state_trans_matrix.T/np.sum(self.state_trans_matrix,axis=1)).T
        else:
            self.state_trans_matrix = state_trans_matrix
        # 发射矩阵 emission_matrix
        if 'emission_matrix' not in kwargs:
            self.emission_matrix = np.random.random((self.num_latent_states,self.num_observation_states))
            self.emission_matrix = (self.emission_matrix.T/np.sum(self.emission_matrix,axis=1)).T
        else:
            self.emission_matrix = emission_matrix
        
    def forward(self,inputs):
        """
        一、目标：给定参数lambda=(init_prob_dist,state_trans_matrix,emission_matrix)，用前向算法求p(O)
        二、假设：
            1. 齐次Markov假设：p(i_t+1|o_1,...,o_t,o_t+1,i_1,...,i_t) = p(i_t+1|i_t)
            2. 观测独立性假设：p(o_t+1|o_1,...,o_t,i_1,...,i_t,i_t+1) = p(o_t+1|i_t+1)
        三、求解
        1. 标记：
            alpha_t(i) = p(o_1,...,o_t,i_t=q_i)
        2. 初始化：
            alpha_1(i) = p(o_1,i_1=q_i)
                       = p(o_1|i_1=q_i)*p(i_1=q_i)
                       = emission_matrix[q_i,o_1]*init_prob_dist[q_i]
            alpha_1 = emission_matrix[:,o_1]*init_prob_dist 
            注意：这里是position_wise乘法，不是点乘
            shape = (num_latent_state,)*(num_latent_state,) = (num_latent_state,)
        3. 迭代关系：
            alpha_t(i) = p(o_1,...,o_t,i_t=q_i)
            alpha_t+1(j) = p(o_1,...,o_t,o_t+1,i_t+1=q_j)
                         = p(o_t+1|o_1,...,o_t,i_t+1=q_j)*p(o_1,...,o_t,i_t+1=q_j)
                         = p(o_t+1|i_t+1=q_j)*sum_i(p(o_1,...,o_t,i_t=q_i,i_t+1=q_j)) #观测独立性假设
                         = p(o_t+1|i_t+1=q_j)*sum_i(p(i_t+1=q_j|o_1,...,o_t,i_t=q_i)*p(o_1,...,o_t,i_t=q_i))
                         = p(o_t+1|i_t+1=q_j)*sum_i(p(i_t+1=q_j|i_t=q_i)*p(o_1,...,o_t,i_t=q_i)) #齐次Markov假设
                         = emission_matrix[q_j,o_t+1]*sum_i(state_trans_matrix[q_i,q_j]*alpha_t(i))
            alpha_t+1 = emission_matrix[:,o_t+1]*np.dot(state_trans_matrix.T,alpha_t.T)
            注意：这里是position_wise乘法(*)，不是点乘(.*)
            shape = (num_latent_state,)*((num_latent_state,num_latent_state).*(num_latent_state,))
                  = (num_latent_state,)*(num_latent_state,)
                  = (num_latent,)
        4. 目标与alpha的关系：
            alpha_T = p(O,i_T=q_i)
            p(O) = sum_i(p(O,i_T=q_i))
                 = sum_i(alpha_T)
        """
        # 初始化
        T = len(inputs)
        alpha = []
        alpha_t = self.emission_matrix[:,inputs[0]]*self.init_prob_dist
        alpha.append(alpha_t)
        # 迭代
        for t in range(0,T-1):
            alpha_t = self.emission_matrix[:,inputs[t+1]]*np.dot(self.state_trans_matrix.T,alpha_t.T)
            alpha.append(alpha_t)
        # 返回p(O)
        p_O = np.sum(alpha_t,axis=0)
        # 返回的p_O,alpha可用于baum-welch算法
        return p_O,np.array(alpha)
        
    def backward(self,inputs):
        """
        一、目标：给定参数lambda=(init_prob_dist,state_trans_matrix,emission_matrix)，用后向算法求p(O)
        二、假设：
            1. 齐次Markov假设：p(i_t+1|o_1,...,o_t,o_t+1,i_1,...,i_t) = p(i_t+1|i_t)
            2. 观测独立性假设：p(o_t+1|o_1,...,o_t,i_1,...,i_t,i_t+1) = p(o_t+1|i_t+1)
        三、求解
        1. 标记：
            beta_t(i) = p(o_t+1,...,o_T|i_t=q_i)
        2. 初始化：
            beta_T-1(i) = p(o_T|i_T-1=q_i)
                        = sum_j(p(o_T,i_T=q_j|i_T-1=q_i))
                        = sum_j(p(o_T|i_T=q_j,i_T-1=q_i)*p(i_T=q_j|i_T-1=q_i))
                        = sum_j(p(o_T|i_T=q_j)*p(i_T=q_j|i_T-1=q_i)) #观测独立性假设
                        = sum_j(emission_matrix[q_j,o_T]*state_trans_matrix[q_i,q_j])
            beta_T-1 = np.dot(state_trans_matrix,emission_matrix[:,o_T])
            shape = (num_latent_state,num_latent_state).*(num_latent_state,)
                  = (num_latent_state,)
            注：beta_T(i) = [1,...,1]   shape=(num_latent_state,)
        3. 迭代关系：
            beta_t+1(j) = p(o_t+2,...,o_T|i_t+1=q_j)
            beta_t(i) = p(o_t+1,...,o_T|i_t=q_i)
                      = sum_j(p(o_t+1,...,o_T,i_t+1=q_j|i_t=q_i))
                      = sum_j(p(o_t+1|o_t+2,...,o_T,i_t+1=q_j,i_t=q_i)*p(o_t+2,...,o_T,i_t+1=q_j|i_t=q_i))
                      = sum_j(p(o_t+1|i_t+1=q_j)*p(o_t+2,...,o_T|i_t=q_i,i_t+1=q_j)*p(i_t+1=q_j|i_t=q_i)) #观测独立性假设
                      = sum_j(p(o_t+1|i_t+1=q_j)*p(o_t+2,...,o_T|i_t+1=q_j)*p(i_t+1=q_j|i_t=q_i)) #概率图阻断原理：p(o_t+2,...,o_T|i_t=q_i,i_t+1=q_j) = p(o_t+2,...,o_T|i_t+1=q_j)
                      = sum_j(emission_matrix[q_j,o_t+1]*beta_t+1(j)*state_trans_matrix[q_i,q_j])
            beta_t = np.dot(beta_t+1,(emission_matrix[:,o_t+1]*state_trans_matrix).T)
            注意：这里是position_wise乘法(*)，不是点乘(.*)
            shape = np.sum((num_latent_states,)*(num_latent_states,)*(num_latent_states,num_latent_states),axis=0)
                  = np.sum((num_latent_states,num_latent_states),axis=0)
                  = (num_latent_states,)
        4. 目标与beta的关系：
            beta_1 = p(o_2,...,o_T|i_1=q_i)
            p(O) = sum_i(p(O,i_1=q_i))
                 = sum_i(p(O|i_1=q_i)*p(i_1=q_i))
                 = sum_i(p(o_1,o_2,...,o_T|i_1=q_i)*p(i_1=q_i))
                 = sum_i(p(o_1|o_2,...,o_T,i_1=q_i)*p(o_2,...,o_T|i_1=q_i)*p(i_1=q_i))
                 = sum_i(p(o_1|i_1=q_i))*p(o_2,...,o_T|i_1=q_i)*p(i_1=q_i)) #观测独立性假设
                 = sum_i(emission_matrix[q_i,o_1]*beta_1*init_prob_dist[q_i])
                 = np.dot(beta_1,emission_matrix[:,o_1]*init_prob_dist[q_i])
        """
        # 初始化
        T = len(inputs)
        beta = []
        beta_T = np.array([1.]*self.num_latent_states)
        beta.append(beta_T)
        beta_t = np.dot(self.state_trans_matrix,self.emission_matrix[:,inputs[-1]])
        beta.append(beta_t)
        # 迭代
        for t in range(0,T-2)[::-1]:
            beta_t = np.sum(self.state_trans_matrix*beta_t*self.emission_matrix[:,inputs[t+1]],axis=1)
            beta.append(beta_t)
        p_O = sum(beta_t*self.init_prob_dist*self.emission_matrix[:,inputs[0]])
        # 返回的p_O,beta可用于baum-welch算法
        return p_O,np.array(beta[::-1])
    
    def train(self,inputs,conv_loss=1e-8):
        """
        一、目标：利用Baum-Welch算法无监督训练HMM
        二、迭代公式（EM算法）：
            lambda_t+1 = argmax_lambda sum_I[log(p(O,I|lambda))*p(I|O,lambda_t)]
                       = argmax_lambda sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)/p(O|lambda_t)]
                       = argmax_lambda sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)]
            lambda_t+1 = (init_prob_dist_t+1,state_trans_matrix_t+1,emission_matrix_t+1)
            设Q函数为：
                Q(lambda,lambda_t) = sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)]
            由于：
                #假设序列长为T
                p(O|lambda) = sum_I[p(O,I|lambda)]
                            = sum_i_1[sum_i_2[...[sum_i_T(init_prob_dist[i_1]*prod_i(state_trans_matrix[i_t-1,i_t])*prod_i(emission_matrix[i_t,o_t]))]]]
            所以Q函数为：
                Q(lambda,lambda_t) = sum_I[(log(init_prob_dist[i_1])+sum_i(log(state_trans_matrix[i_t-1,i_t]))+sum_i(log(emission_matrix[i_t,o_t])))*p(O,I|lambda_t)]
            分别拆分为：
                init_prob_dist_t+1 = argmax_init_prob_dist sum_I[log(init_prob_dist[i_1])*p(O,I|lambda_t)]
                state_trans_matrix_t+1 = argmax_state_trans_matrix sum_I[sum_i(log(state_trans_matrix[i_t-1,i_t]))*p(O,I|lambda_t)]
                emission_matrix_t+1 = argmax_emission_matrix sum_I[sum_i(log(emission_matrix[i_t,o_t]))*p(O,I|lambda_t)]
                这里涉及到约束优化问题：
                    np.sum(init_prob_dist_i) = 1
                    np.sum(state_trans_matrix,axis=0) = [1,...,1]     shape=(state_trans_matrix.shape[0],)
                    np.sum(emission_matrix,axis=0) = [1,...,1]     shape=(emission_matrix.shape[0],)
                需要用到拉格朗日法求解，这里不做过多展开，具体请参照李航《统计学习方法》的章节《隐马尔可夫模型》：Baum-Welch算法
            求得各迭代公式可以用李航《统计学习方法》中给出的公式：
                gamma_t(i) = p(i_t=q_i|O)
                           = p(i_t=q_i,O)/p(O)
                           = alpha_t(i)*beta_t(i)/sum_j(alpha_t(j)*beta_t(j))
                gamma = alpha*beta/np.sum(alpha*beta,axis=1)
                      = alpha*beta/p(O) # np.sum(alpha*beta,axis=1)这个值实际上就是p(O)值
                xi_t(i,j) = alpha_t(i)*state_trans_matrix[q_i,q_j]*emission_matrix[q_j,o_t+1]*beta_t+1(j)/sum_i[sum_j(alpha_t(i)*state_trans_matrix[q_i,q_j]*emission_matrix[q_j,o_t+1]*beta_t+1(j))]
                xi_t = (alpha_t*state_trans_matrix.T).T*emission_matrix[:,o_t+1]*beta_t+1/np.sum((alpha_t*state_trans_matrix.T).T*emission_matrix[:,o_t+1]*beta_t+1)
            1. init_prob_dist：
                init_prob_dist[q_i] = gamma_1(i)
                init_prob_dist = gamma_1
            2. state_trans_matrix：
                state_trans_matrix[q_i,q_j] = sum_(t=1~T-1)[xi_t(i,j)]/sum_(t=1~T-1)[gamma_t(i)]
                                            = np.sum(xi(i,j),axis=0) / np.sum(gamma[:-1](i),axis=0)
                state_trans_matrix = np.sum(xi,axis=0)/np.sum(gamma[:-1],axis=0)
            3. emission_matrix:
                emission_matrix[q_j,o_t=v_k] = sum_(t=1~T,o_t=v_k)[gamma_t(j)]/sum_(t=1~T)[gamma_t(j)]
                # 这里o_t=v_k暂时还没想好怎么直接用numpy处理，暂时只能用循环处理
                
                #emission_from_gamma.shape = (num_latent_states,num_observation_states)
                emission_from_gamma = np.zeros((num_latent_states,num_observation_states))
                for gamma_i,o_i in zip(gamma,o):
                    #gamma_i.shape = (1,num_latent_states)
                    emission_from_gamma[:,o_i] += gamma_i 
                emission_matrix = (emission_from_gamma.T/np.sum([gamma_1,gamma_2,...,gamma_T],axis=0)).T
        """
        logger = logging.getLogger('Baum-Welch')
        epochs = 1
        l2_loss = np.Inf
        while True:
            
            init_prob_dist = np.zeros((self.num_latent_states,))
            state_trans_matrix = np.zeros((self.num_latent_states,self.num_latent_states))
            emission_matrix = np.zeros((self.num_latent_states,self.num_observation_states))
            
            for input_item in inputs:
                p_O,alpha = self.forward(input_item)
                p_O,beta = self.backward(input_item)
                gamma = alpha*beta/p_O
                xi = []
                for t in range(len(input_item)-1):
                    xi_t = (alpha[t]*self.state_trans_matrix.T).T*self.emission_matrix[:,input_item[t+1]]*beta[t+1]/np.sum((alpha[t]*self.state_trans_matrix.T).T*self.emission_matrix[:,input_item[t+1]]*beta[t+1])
                    xi.append(xi_t)
                xi = np.array(xi)
                #计算init_prob_dist
                init_prob_dist += gamma[0]
                #计算state_trans_matrix
                state_trans_matrix += (np.sum(xi,axis=0).T/np.sum(gamma[:-1],axis=0)).T
                #计算emission_matrix
                emission_from_gamma = np.zeros((self.num_latent_states,self.num_observation_states))
                for gamma_i,o_i in zip(gamma,input_item):
                    emission_from_gamma[:,o_i] += gamma_i
                emission_matrix += (emission_from_gamma.T/np.sum(gamma,axis=0)).T
            init_prob_dist /= len(inputs)
            state_trans_matrix /= len(inputs)
            emission_matrix /= len(inputs)
            
            l2_loss_old = l2_loss
            l2_loss = np.sum(np.power(init_prob_dist-self.init_prob_dist,2))+np.sum(np.power(state_trans_matrix-self.state_trans_matrix,2))+np.sum(np.power(emission_matrix-self.emission_matrix,2))
            if l2_loss_old-l2_loss &lt;= 0 or l2_loss &lt;= conv_loss:
                logger.info('training finished!')
                break
            logger.info('epochs:{}\tloss:{}'.format(epochs,l2_loss))
            self.init_prob_dist = init_prob_dist
            self.state_trans_matrix = state_trans_matrix
            self.emission_matrix = emission_matrix
            epochs += 1
        return
    
    def decode(self,inputs):
        """
        一、目标：利用Viterbi算法解码
        二、Viterbi算法：
            1. 初始化：
                delta_1(i) = init_prob_dist[q_i]*emission_matrix[q_i,o_1]  --&gt; delta_1 = init_prob_dist*emission_matrix[:,o_1]
                psi_1(i) = 0
            2. 迭代公式：
                对于t=2,3,...,T
                    delta_t(i) = max_(1&lt;=j&lt;=num_latent_states)[[delta_t-1(j)*state_trans_matrix[q_j,q_i]]*emission_matrix[q_i,o_t]]
                    psi_t(i) = argmax_(1&lt;=j&lt;=num_latent_states)[delta_t-1(j)*state_trans_matrix[q_j,q_i]]  # 这个式子可以带上最后一项emission_matrix[q_i,o_t]]，argmax和最后一项没关系
            3. 终止：
                P* = max_(1&lt;=i&lt;=num_latent_states)[delta_T(i)]
                i*_T = argmax_(1&lt;=i&lt;=num_latent_states)[delta_T(i)]
            4. 最优路径回溯
                i*_t = psi_t+1(i*_t+1)
        """
        logger = logging.getLogger('Viterbi')
        logger.info('start decoding...')
        delta,psi,route = [],[],[]
        # 初始化
        delta_1 = self.init_prob_dist*self.emission_matrix[:,inputs[0]]
        psi_1 = np.zeros((self.num_latent_states,))
        delta.append(delta_1)
        psi.append(psi_1)
        # 迭代
        for t in range(1,len(inputs)):
            iter_func = (delta[-1] * self.state_trans_matrix.T).T*self.emission_matrix[:,inputs[t]]
            delta_t = np.max(iter_func,axis=0)
            psi_t = np.argmax(iter_func,axis=0)
            delta.append(delta_t)
            psi.append(psi_t)
        # 最优路径回溯
        route_T = np.argmax(delta[-1])
        route.append(route_T)
        for t in range(len(inputs)-1)[::-1]:
            route_t = psi[t+1][route[-1]]
            route.append(route_t)
        route = route[::-1]
        logger.info('decoding finished!')
        return route</code></pre></div><h2>模型使用</h2><p>一、Evaluating任务</p><p>这里以李航老师《统计学习方法》（第一版）第177页的10.2例题为例：考虑盒子和球模型 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(7)" alt="[公式]" eeimg="1" data-formula="\lambda=(A,B,\pi)"> ，状态集合 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(8)" alt="[公式]" eeimg="1" data-formula="Q=\{1,2,3\}"> ，观测集合 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(9)" alt="[公式]" eeimg="1" data-formula="V=\{红，白\}"> ， <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(10)" alt="[公式]" eeimg="1" data-formula="A=[[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]]"> ， <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula="B=[[0.5,0.5],[0.4,0.6],[0.7,0.3]]"> ， <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula="\pi=[0.2,0.4,0.4]^T"> 。设 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(13)" alt="[公式]" eeimg="1" data-formula="T=3"> ， <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="O=(红，白，红)"> ，试用前向算法计算 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(15)" alt="[公式]" eeimg="1" data-formula="P(O|\lambda)"> 。</p><div class="highlight"><pre><code class="language-text"># 初始化状态概率向量
init_prob_dist = np.array((0.2,0.4,0.4))
# 状态转移矩阵
state_trans_matrix = np.array([[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]])
# 发射矩阵
emission_matrix = np.array([[0.5,0.5],[0.4,0.6],[0.7,0.3]])

# 初始化隐马尔可夫参数
hmm = HMM(num_latent_states=3,num_observation_states=2,init_prob_dist=init_prob_dist,state_trans_matrix=state_trans_matrix,emission_matrix=emission_matrix)
# 输入状态序列
inputs = [0,1,0]

# 前向算法
p_O,alpha = hmm.forward(inputs)
print('p(O):',p_O)
print('alpha:\n',alpha)

# 后向算法
p_O,beta = hmm.backward(inputs)
print('p(O):',p_O)
print('beta:\n',beta)</code></pre></div><p>输出结果如下：</p><div class="highlight"><pre><code class="language-text">p(O): 0.130218
alpha:
 [[ 0.1       0.16      0.28    ]
 [ 0.077     0.1104    0.0606  ]
 [ 0.04187   0.035512  0.052836]]
p(O): 0.130218
beta:
 [[ 0.2451  0.2622  0.2277]
 [ 0.54    0.49    0.57  ]
 [ 1.      1.      1.    ]]</code></pre></div><p>二、Learning任务</p><ol><li>随机初始化学习：</li></ol><div class="highlight"><pre><code class="language-text"># 随机初始化学习
hmm = HMM(num_latent_states=3,num_observation_states=2)

# 给定训练序列
hmm.train([[0,1,0],[1,1,0,1,0,0,0,1]])

# 输出三参数
print('init_prob_dist:\n',hmm.init_prob_dist)
print('state_trans_matrix:\n',hmm.state_trans_matrix)
print('emission_matrix:\n',hmm.emission_matrix)</code></pre></div><p>输出结果如下：</p><div class="highlight"><pre><code class="language-text">2019-07-27 12:13:36,492 - Baum-Welch - epochs:1	loss:0.20527632330570128
2019-07-27 12:13:36,495 - Baum-Welch - epochs:2	loss:0.023973925497813152
2019-07-27 12:13:36,497 - Baum-Welch - training finished!

init_prob_dist:
 [ 0.32769973  0.26571133  0.40658893]
state_trans_matrix:
 [[ 0.32888918  0.29758607  0.37352475]
 [ 0.3265049   0.1370833   0.5364118 ]
 [ 0.31297094  0.42866714  0.25836193]]
emission_matrix:
 [[ 0.60303459  0.39696541]
 [ 0.18463565  0.81536435]
 [ 0.82953924  0.17046076]]</code></pre></div><p>2. 指定三参数学习：</p><div class="highlight"><pre><code class="language-text"># 初始化状态概率向量
init_prob_dist = np.array((0.2,0.4,0.4))
# 状态转移矩阵
state_trans_matrix = np.array([[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]])
# 发射矩阵
emission_matrix = np.array([[0.5,0.5],[0.4,0.6],[0.7,0.3]])

# 初始化隐马尔可夫参数
hmm = HMM(num_latent_states=3,num_observation_states=2,init_prob_dist=init_prob_dist,state_trans_matrix=state_trans_matrix,emission_matrix=emission_matrix)

# 给定训练序列
hmm.train([[0,1,0],[1,1,0,1,0,0,0,1]])

# 输出三参数
print('init_prob_dist:\n',hmm.init_prob_dist)
print('state_trans_matrix:\n',hmm.state_trans_matrix)
print('emission_matrix:\n',hmm.emission_matrix)</code></pre></div><p>输出结果如下：</p><div class="highlight"><pre><code class="language-text">2019-07-27 12:17:24,354 - Baum-Welch - epochs:1	loss:0.02249436309015495
2019-07-27 12:17:24,357 - Baum-Welch - epochs:2	loss:0.005045660333614989
2019-07-27 12:17:24,358 - Baum-Welch - epochs:3	loss:0.0028643221735452492
2019-07-27 12:17:24,360 - Baum-Welch - epochs:4	loss:0.0018477726244472792
2019-07-27 12:17:24,362 - Baum-Welch - epochs:5	loss:0.0013193653589663862
2019-07-27 12:17:24,363 - Baum-Welch - epochs:6	loss:0.0010374857556465027
2019-07-27 12:17:24,365 - Baum-Welch - epochs:7	loss:0.0009009395450544214
2019-07-27 12:17:24,367 - Baum-Welch - epochs:8	loss:0.0008644768741977516
2019-07-27 12:17:24,369 - Baum-Welch - training finished!

init_prob_dist:
 [ 0.17390824  0.61238671  0.21370505]
state_trans_matrix:
 [[ 0.48535613  0.15118683  0.36345704]
 [ 0.33463896  0.48322213  0.18213891]
 [ 0.24597706  0.35745292  0.39657003]]
emission_matrix:
 [[ 0.54980183  0.45019817]
 [ 0.54087062  0.45912938]
 [ 0.67230652  0.32769348]]</code></pre></div><p>三、Decoding任务</p><p>以前述Evaluating任务的用例求解隐状态序列，这里的隐状态为 <img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula="(0,1,2)"> 。</p><div class="highlight"><pre><code class="language-text"># 指定观测序列
decode_res = hmm.decode([0,1,0])
# 输出序列解码结果
print('Viterbi decode result:\n',decode_res)</code></pre></div><p>输出结果如下：</p><div class="highlight"><pre><code class="language-text">2019-07-27 12:20:35,423 - Viterbi - start decoding...
2019-07-27 12:20:35,425 - Viterbi - decoding finished!

Viterbi decode result:
 [2, 2, 2]</code></pre></div><p></p></div></div><div class="ContentItem-time">编辑于 2019-07-29</div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20682981&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20682981" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">隐马尔可夫模型</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19559450&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19559450" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">机器学习</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19560026&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19560026" target="_blank"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content">自然语言处理</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 394.2px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;75406198&quot;}}}"><span><button aria-label="赞同 138" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 138</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>1 条评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><img class="ShareMenu-fakeQRCode" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/qrcode" alt="微信二维码"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Post-SideActions" data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;75406198&quot;}}}" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 139">赞同 138</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover21-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover21-content"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/liushaoweihua"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/4b70deef7_xs.jpg" srcset="https://pic2.zhimg.com/4b70deef7_l.jpg 2x" alt="那就权当做是个记笔记的地方吧"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/liushaoweihua"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content">那就权当做是个记笔记的地方吧</div></div></a></h2><div class="ContentItem-meta">记笔记</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton Button--primary Button--blue">关注专栏</button></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1478px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled=""><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/85454896" class="PostItem"><div><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/v2-994b0c15a01456ab027e62bf66e79c16_250x0.jpg" srcset="https://pic1.zhimg.com/v2-994b0c15a01456ab027e62bf66e79c16_qhd.jpg 2x" class="PostItem-TitleImage" alt="一站式解决：隐马尔可夫模型（HMM）全过程推导及实现"><h1 class="PostItem-Title">一站式解决：隐马尔可夫模型（HMM）全过程推导及实现</h1><div class="PostItem-Footer"><span>永远在你身...</span><span class="PostItem-FooterTitle">发表于技术部落联...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/23114198" class="PostItem"><div><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/v2-883ac9db7f1cbd7325b2450cd225a897_250x0.jpg" srcset="https://pic2.zhimg.com/v2-883ac9db7f1cbd7325b2450cd225a897_qhd.jpg 2x" class="PostItem-TitleImage" alt="玩点高级的--带你入门Topic模型LDA（小改进+附源码）"><h1 class="PostItem-Title">玩点高级的--带你入门Topic模型LDA（小改进+附源码）</h1><div class="PostItem-Footer"><span>笑虎</span><span class="PostItem-FooterTitle">发表于撸代码，学...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/27056207" class="PostItem"><div><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/v2-c5fb7727c63deef78633579b128d5de2_250x0.jpg" srcset="https://pic3.zhimg.com/v2-c5fb7727c63deef78633579b128d5de2_qhd.jpg 2x" class="PostItem-TitleImage" alt="隐马尔科夫模型（HMM）一前向与后向算法"><h1 class="PostItem-Title">隐马尔科夫模型（HMM）一前向与后向算法</h1><div class="PostItem-Footer"><span>忆臻</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/33397147" class="PostItem"><div><img src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/v2-48dd591b8bc4775b95dd032983c5e729_250x0.jpg" srcset="https://pic2.zhimg.com/v2-48dd591b8bc4775b95dd032983c5e729_qhd.jpg 2x" class="PostItem-TitleImage" alt="概率图模型体系：HMM、MEMM、CRF"><h1 class="PostItem-Title">概率图模型体系：HMM、MEMM、CRF</h1><div class="PostItem-Footer"><span>Scofield</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">1 条评论</h2></div><div class="Topbar-options"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Switch Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.004 7V4.232c0-.405.35-.733.781-.733.183 0 .36.06.501.17l6.437 5.033c.331.26.376.722.1 1.033a.803.803 0 0 1-.601.264H2.75a.75.75 0 0 1-.75-.75V7.75A.75.75 0 0 1 2.75 7h10.254zm-1.997 9.999v2.768c0 .405-.35.733-.782.733a.814.814 0 0 1-.5-.17l-6.437-5.034a.702.702 0 0 1-.1-1.032.803.803 0 0 1 .6-.264H21.25a.75.75 0 0 1 .75.75v1.499a.75.75 0 0 1-.75.75H11.007z" fill-rule="evenodd"></path></svg></span>切换为时间排序</button></div></div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="CommentEditorV2-input Input-wrapper Input-wrapper--spread Input-wrapper--large Input-wrapper--noPadding"><div class="Input Editable"><div class="Dropzone RichText RichText--editable RichText--clearBoth ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-bj0hn" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-bj0hn" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="bj0hn" data-offset-key="ee07m-0-0"><div data-offset-key="ee07m-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="ee07m-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button type="button" class="Button CommentEditorV2-singleButton Button--primary Button--blue" disabled="">发布</button></div><div><div class="CommentListV2"><ul class="NestComment"><li class="NestComment--rootCommentNoChild"><div class="CommentItemV2"><div><div class="CommentItemV2-meta"><span class="UserLink CommentItemV2-avatar"><div class="Popover"><div id="Popover17-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover17-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/soko-60"><img class="Avatar UserLink-avatar" width="24" height="24" src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/v2-cf78af5b83d89198edfc9b3b1e79577b_s.jpg" srcset="https://pic4.zhimg.com/v2-cf78af5b83d89198edfc9b3b1e79577b_xs.jpg 2x" alt="soko"></a></div></div></span><span class="UserLink"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/soko-60">soko</a></span><span class="CommentItemV2-time">刚刚</span></div><div class="CommentItemV2-metaSibling"><div class="CommentRichText CommentItemV2-content"><div class="RichText ztext"><p>很有用，用了两天零散时间研读了一遍源码，对于些矩阵计算稍微改进了那么些，很有用！</p></div></div><div class="CommentItemV2-footer"><button disabled="" type="button" class="Button CommentItemV2-likeBtn Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Like" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M14.445 9h5.387s2.997.154 1.95 3.669c-.168.51-2.346 6.911-2.346 6.911s-.763 1.416-2.86 1.416H8.989c-1.498 0-2.005-.896-1.989-2v-7.998c0-.987.336-2.032 1.114-2.639 4.45-3.773 3.436-4.597 4.45-5.83.985-1.13 3.2-.5 3.037 2.362C15.201 7.397 14.445 9 14.445 9zM3 9h2a1 1 0 0 1 1 1v10a1 1 0 0 1-1 1H3a1 1 0 0 1-1-1V10a1 1 0 0 1 1-1z" fill-rule="evenodd"></path></svg></span>赞</button><button type="button" class="Button Button--plain"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Trash" fill="currentColor" viewBox="0 0 24 24" width="16" height="16" style="margin-right: 5px;"><path d="M16.464 4s.051-2-1.479-2H9C7.194 2 7.465 4 7.465 4H4.752c-2.57 0-2.09 3.5 0 3.5l1.213 13.027S5.965 22 7.475 22h8.987c1.502 0 1.502-1.473 1.502-1.473l1.2-13.027c2.34 0 2.563-3.5 0-3.5h-2.7zM8.936 18.5l-.581-9h1.802v9H8.936zm4.824 0v-9h1.801l-.61 9H13.76z" fill-rule="evenodd"></path></svg></span>删除</button></div></div></div></div></li></ul></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","apiHost":"api.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"entities":{"users":{"3c6b05b6fb1b1e26fdc0c495d2032cf5":{"uid":900994210918989800,"userType":"people","id":"3c6b05b6fb1b1e26fdc0c495d2032cf5"},"di-san-zhi-jie":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_{size}.jpg","uid":"592978442464137216","userType":"people","isFollowing":true,"urlToken":"di-san-zhi-jie","id":"7da50b58e4a1703c1df1fdb456577739","description":"大家好","name":"壮哉我贾诩文和","isAdvertiser":false,"headline":"自然语言处理入门中。原方向：交通数据挖掘，公交线网优化。","gender":1,"url":"\u002Fpeople\u002F7da50b58e4a1703c1df1fdb456577739","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_l.jpg","isOrg":false,"type":"people","vipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4812630bc27d642f7cafcd6cdeca3d7a_r.png","nightModeUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c9686ff064ea3579730756ac6c289978_r.png"}},"badge":[{"type":"identity","topics":[],"description":"东南大学 交通运输工程硕士"}],"exposedMedal":{"medalId":"972465637922213888","medalName":"给自己证明","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-95e2940e8968c11fac86cb9f3c59ca97_r.png","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b8cb24c96e95d6274794c29f35c912e_is.png","description":"已完善全部个人资料"}}},"questions":{},"answers":{},"articles":{"75406198":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGNXAmN5CV5yCkMO-E7sIKsd&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__"],"id":75406198,"title":"概率图模型系列-隐马尔可夫模型（HMM）的numpy实现","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F75406198","imageUrl":"","titleImage":"","excerpt":"最近在工作中碰到一些\u003Cb\u003E序列标注任务\u003C\u002Fb\u003E，如命名实体识别（Name Entity Recognition，NER）、中文分词（Chinese Word Segmentation，CWS）和词性标注（Part-Of-Speech Tagging, PosTag）等。序列标注任务，简而言之，就是在\u003Cb\u003E\u003Ci\u003E给定不定长观测序列的基础上，尝试找到…\u003C\u002Fi\u003E\u003C\u002Fb\u003E","created":1564201353,"updated":1564384896,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_{size}.jpg","uid":"592978442464137216","userType":"people","isFollowing":true,"urlToken":"di-san-zhi-jie","id":"7da50b58e4a1703c1df1fdb456577739","description":"大家好","name":"壮哉我贾诩文和","isAdvertiser":false,"headline":"自然语言处理入门中。原方向：交通数据挖掘，公交线网优化。","gender":1,"url":"\u002Fpeople\u002F7da50b58e4a1703c1df1fdb456577739","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_l.jpg","isOrg":false,"type":"people","vipInfo":{"isVip":true,"vipIcon":{"url":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-4812630bc27d642f7cafcd6cdeca3d7a_r.png","nightModeUrl":"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-c9686ff064ea3579730756ac6c289978_r.png"}},"badge":[{"type":"identity","topics":[],"description":"东南大学 交通运输工程硕士"}],"exposedMedal":{"medalId":"972465637922213888","medalName":"给自己证明","avatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-95e2940e8968c11fac86cb9f3c59ca97_r.png","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-2b8cb24c96e95d6274794c29f35c912e_is.png","description":"已完善全部个人资料"}},"commentPermission":"all","state":"published","imageWidth":0,"imageHeight":0,"content":"\u003Cp\u003E最近在工作中碰到一些\u003Cb\u003E序列标注任务\u003C\u002Fb\u003E，如命名实体识别（Name Entity Recognition，NER）、中文分词（Chinese Word Segmentation，CWS）和词性标注（Part-Of-Speech Tagging, PosTag）等。\u003C\u002Fp\u003E\u003Cp\u003E序列标注任务，简而言之，就是在\u003Cb\u003E\u003Ci\u003E给定不定长观测序列的基础上，尝试找到决定该观测序列的另一组序列\u003C\u002Fi\u003E。\u003C\u002Fb\u003E如给定一个一维输入序列：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=X%3Dx_1%2Cx_2%2C...%2Cx_i%2C...%2Cx_n\" alt=\"X=x_1,x_2,...,x_i,...,x_n\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E对该序列中的每个元素打上标签的过程：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Y%3Dy_1%2Cy_2%2C...%2Cy_i%2C...%2Cy_n\" alt=\"Y=y_1,y_2,...,y_i,...,y_n\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=yi+%5Cin+S\" alt=\"yi \\in S\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其本质是基于序列的每个元素的上下文内容，对序列元素进行多分类的过程。\u003C\u002Fp\u003E\u003Ch2\u003E序列标注算法演化\u003C\u002Fh2\u003E\u003Cp\u003E在深度学习之前，隐马尔可夫模型（HMM）、最大熵马尔科夫模型（MEMM）和条件随机场（CRF），尤其是条件随机场CRF，是进行序列标注的常用方法。这些方法的局限在于上下文理解上。\u003C\u002Fp\u003E\u003Cp\u003E深度学习的发展，RNN、LSTM等深度模型的出现较好地解决上述缺陷，引申出了现在主流的序列标注方法：biLSTM+CRF。（提升上下文理解）\u003C\u002Fp\u003E\u003Cp\u003EAttention机制的出现又极大地解决了循环神经网络不能并行的缺陷，并且在上下文理解上有了进一步的提升。（提升模型效率、提升上下文理解）\u003C\u002Fp\u003E\u003Cp\u003E现在，在Bert、XLNet等大规模预训练模型大行其道的时代下，预训练模型+微调的两阶段套路基本就能达到较好的效果。（加入大规模先验）\u003C\u002Fp\u003E\u003Ch2\u003EHMM、MEMM和CRF的区别与联系\u003C\u002Fh2\u003E\u003Cp\u003E区别：直观理解上，HMM模型属于生成模型，是贝叶斯派（Bayesian）成果。与频率学派对条件概率 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28Y%7CX%29\" alt=\"P(Y|X)\" eeimg=\"1\"\u002F\u003E 建模的思路不同，贝叶斯派的是对联合概率分布 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28X%2CY%29\" alt=\"P(X,Y)\" eeimg=\"1\"\u002F\u003E 进行建模（learning），在模型的基础上进行推断(inference)，得到 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28Y%7CX%29\" alt=\"P(Y|X)\" eeimg=\"1\"\u002F\u003E 。MEMM和CRF均属于判别模型。\u003C\u002Fp\u003E\u003Cp\u003E联系：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E隐马尔可夫模型的两条重要假设就是：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E（1）齐次马尔科夫假设： 当前隐状态仅由上一个隐状态决定。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28i_%7Bt%2B1%7D%7Co_1%2C...%2Co_t%2Ci_1%2C...%2Ci_t%29+%3D+p%28i_%7Bt%2B1%7D%7Ci_t%29\" alt=\"p(i_{t+1}|o_1,...,o_t,i_1,...,i_t) = p(i_{t+1}|i_t)\" eeimg=\"1\"\u002F\u003E\u003C\u002Fp\u003E\u003Cp\u003E （2）观测独立性假设：当前观测状态仅由当前隐状态决定。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=p%28o_t%7Co_1%2C...%2Co_%7Bt-1%7D%2Ci_1%2C...%2Ci_t%29+%3D+p%28o_t%7Ci_t%29\" alt=\"p(o_t|o_1,...,o_{t-1},i_1,...,i_t) = p(o_t|i_t)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E在概率图上，HMM的输入是隐状态，输出是观测序列。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EMEMM则将观测序列作为输入，隐状态作为输出，\u003C\u002Fb\u003E在给定观测序列的情况下，\u003Cb\u003E打破了隐马尔可夫的观测独立性假设。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003EHMM和MEMM均属于有向图，有向图天然的局部归一化问题会导致标注偏差问题（label bias problem）的出现，而无向图则是天然的全局归一化，因此不会落入上述现象。这也就是CRF和MEMM的唯一差别。\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E这里强推一个b站UP主的视频机器学习-白板推导视频，这一节详细介绍了三个模型之间的联系：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.bilibili.com\u002Fvideo\u002Fav34444816\u002F%3Fspm_id_from%3D333.788.videocard.1\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ebilibili.com\u002Fvideo\u002Fav34\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E444816\u002F?spm_id_from=333.788.videocard.1\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Ch2\u003E隐马尔可夫模型理论推导\u003C\u002Fh2\u003E\u003Cp\u003E关于隐马尔可夫模型的细节，这里不做过多介绍，网上有很多资源，但是这里还是要再强推一下上面这个UP主的系列，本文的代码实现是基于这个系列中的\u003Cb\u003E\u003Ci\u003E隐马尔可夫模型HMM\u003C\u002Fi\u003E\u003C\u002Fb\u003E一节以及李航老师的\u003Cb\u003E\u003Ci\u003E《统计学习方法》\u003C\u002Fi\u003E\u003C\u002Fb\u003E得到。\u003C\u002Fp\u003E\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.bilibili.com\u002Fvideo\u002Fav32471608\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E\u003Cspan class=\"invisible\"\u003Ehttps:\u002F\u002Fwww.\u003C\u002Fspan\u003E\u003Cspan class=\"visible\"\u003Ebilibili.com\u002Fvideo\u002Fav32\u003C\u002Fspan\u003E\u003Cspan class=\"invisible\"\u003E471608\u003C\u002Fspan\u003E\u003Cspan class=\"ellipsis\"\u003E\u003C\u002Fspan\u003E\u003C\u002Fa\u003E\u003Ch2\u003E隐马尔可夫模型代码实现\u003C\u002Fh2\u003E\u003Cp\u003E本文代码主要分为三个部分：\u003C\u002Fp\u003E\u003Cp\u003E一、Evaluating任务：Forward-Backward算法\u003C\u002Fp\u003E\u003Cp\u003E二、Learning任务：Baum-Welch算法\u003C\u002Fp\u003E\u003Cp\u003E三、Decoding任务：Viterbi算法\u003C\u002Fp\u003E\u003Cp\u003E具体算法的公式推导已经附在各个函数里。\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003Eimport numpy as np\nimport logging\nlogging.basicConfig(level = logging.INFO, format = &#39;%(asctime)s - %(name)s - %(message)s&#39;)\n\nclass HMM(object):\n    \n    def __init__(self,num_latent_states,num_observation_states,**kwargs):\n        self.num_latent_states = num_latent_states\n        self.num_observation_states = num_observation_states\n        # 初始概率分布 init_prob_dist\n        if &#39;init_prob_dist&#39; not in kwargs:\n            self.init_prob_dist = np.random.random((self.num_latent_states,))\n            self.init_prob_dist = self.init_prob_dist\u002Fnp.sum(self.init_prob_dist)\n        else:\n            self.init_prob_dist = init_prob_dist\n        # 状态转移矩阵 state_trans_matrix\n        if &#39;state_trans_matrix&#39; not in kwargs:\n            self.state_trans_matrix = np.ones((self.num_latent_states,self.num_latent_states))\n            self.state_trans_matrix = (self.state_trans_matrix.T\u002Fnp.sum(self.state_trans_matrix,axis=1)).T\n        else:\n            self.state_trans_matrix = state_trans_matrix\n        # 发射矩阵 emission_matrix\n        if &#39;emission_matrix&#39; not in kwargs:\n            self.emission_matrix = np.random.random((self.num_latent_states,self.num_observation_states))\n            self.emission_matrix = (self.emission_matrix.T\u002Fnp.sum(self.emission_matrix,axis=1)).T\n        else:\n            self.emission_matrix = emission_matrix\n        \n    def forward(self,inputs):\n        &#34;&#34;&#34;\n        一、目标：给定参数lambda=(init_prob_dist,state_trans_matrix,emission_matrix)，用前向算法求p(O)\n        二、假设：\n            1. 齐次Markov假设：p(i_t+1|o_1,...,o_t,o_t+1,i_1,...,i_t) = p(i_t+1|i_t)\n            2. 观测独立性假设：p(o_t+1|o_1,...,o_t,i_1,...,i_t,i_t+1) = p(o_t+1|i_t+1)\n        三、求解\n        1. 标记：\n            alpha_t(i) = p(o_1,...,o_t,i_t=q_i)\n        2. 初始化：\n            alpha_1(i) = p(o_1,i_1=q_i)\n                       = p(o_1|i_1=q_i)*p(i_1=q_i)\n                       = emission_matrix[q_i,o_1]*init_prob_dist[q_i]\n            alpha_1 = emission_matrix[:,o_1]*init_prob_dist \n            注意：这里是position_wise乘法，不是点乘\n            shape = (num_latent_state,)*(num_latent_state,) = (num_latent_state,)\n        3. 迭代关系：\n            alpha_t(i) = p(o_1,...,o_t,i_t=q_i)\n            alpha_t+1(j) = p(o_1,...,o_t,o_t+1,i_t+1=q_j)\n                         = p(o_t+1|o_1,...,o_t,i_t+1=q_j)*p(o_1,...,o_t,i_t+1=q_j)\n                         = p(o_t+1|i_t+1=q_j)*sum_i(p(o_1,...,o_t,i_t=q_i,i_t+1=q_j)) #观测独立性假设\n                         = p(o_t+1|i_t+1=q_j)*sum_i(p(i_t+1=q_j|o_1,...,o_t,i_t=q_i)*p(o_1,...,o_t,i_t=q_i))\n                         = p(o_t+1|i_t+1=q_j)*sum_i(p(i_t+1=q_j|i_t=q_i)*p(o_1,...,o_t,i_t=q_i)) #齐次Markov假设\n                         = emission_matrix[q_j,o_t+1]*sum_i(state_trans_matrix[q_i,q_j]*alpha_t(i))\n            alpha_t+1 = emission_matrix[:,o_t+1]*np.dot(state_trans_matrix.T,alpha_t.T)\n            注意：这里是position_wise乘法(*)，不是点乘(.*)\n            shape = (num_latent_state,)*((num_latent_state,num_latent_state).*(num_latent_state,))\n                  = (num_latent_state,)*(num_latent_state,)\n                  = (num_latent,)\n        4. 目标与alpha的关系：\n            alpha_T = p(O,i_T=q_i)\n            p(O) = sum_i(p(O,i_T=q_i))\n                 = sum_i(alpha_T)\n        &#34;&#34;&#34;\n        # 初始化\n        T = len(inputs)\n        alpha = []\n        alpha_t = self.emission_matrix[:,inputs[0]]*self.init_prob_dist\n        alpha.append(alpha_t)\n        # 迭代\n        for t in range(0,T-1):\n            alpha_t = self.emission_matrix[:,inputs[t+1]]*np.dot(self.state_trans_matrix.T,alpha_t.T)\n            alpha.append(alpha_t)\n        # 返回p(O)\n        p_O = np.sum(alpha_t,axis=0)\n        # 返回的p_O,alpha可用于baum-welch算法\n        return p_O,np.array(alpha)\n        \n    def backward(self,inputs):\n        &#34;&#34;&#34;\n        一、目标：给定参数lambda=(init_prob_dist,state_trans_matrix,emission_matrix)，用后向算法求p(O)\n        二、假设：\n            1. 齐次Markov假设：p(i_t+1|o_1,...,o_t,o_t+1,i_1,...,i_t) = p(i_t+1|i_t)\n            2. 观测独立性假设：p(o_t+1|o_1,...,o_t,i_1,...,i_t,i_t+1) = p(o_t+1|i_t+1)\n        三、求解\n        1. 标记：\n            beta_t(i) = p(o_t+1,...,o_T|i_t=q_i)\n        2. 初始化：\n            beta_T-1(i) = p(o_T|i_T-1=q_i)\n                        = sum_j(p(o_T,i_T=q_j|i_T-1=q_i))\n                        = sum_j(p(o_T|i_T=q_j,i_T-1=q_i)*p(i_T=q_j|i_T-1=q_i))\n                        = sum_j(p(o_T|i_T=q_j)*p(i_T=q_j|i_T-1=q_i)) #观测独立性假设\n                        = sum_j(emission_matrix[q_j,o_T]*state_trans_matrix[q_i,q_j])\n            beta_T-1 = np.dot(state_trans_matrix,emission_matrix[:,o_T])\n            shape = (num_latent_state,num_latent_state).*(num_latent_state,)\n                  = (num_latent_state,)\n            注：beta_T(i) = [1,...,1]   shape=(num_latent_state,)\n        3. 迭代关系：\n            beta_t+1(j) = p(o_t+2,...,o_T|i_t+1=q_j)\n            beta_t(i) = p(o_t+1,...,o_T|i_t=q_i)\n                      = sum_j(p(o_t+1,...,o_T,i_t+1=q_j|i_t=q_i))\n                      = sum_j(p(o_t+1|o_t+2,...,o_T,i_t+1=q_j,i_t=q_i)*p(o_t+2,...,o_T,i_t+1=q_j|i_t=q_i))\n                      = sum_j(p(o_t+1|i_t+1=q_j)*p(o_t+2,...,o_T|i_t=q_i,i_t+1=q_j)*p(i_t+1=q_j|i_t=q_i)) #观测独立性假设\n                      = sum_j(p(o_t+1|i_t+1=q_j)*p(o_t+2,...,o_T|i_t+1=q_j)*p(i_t+1=q_j|i_t=q_i)) #概率图阻断原理：p(o_t+2,...,o_T|i_t=q_i,i_t+1=q_j) = p(o_t+2,...,o_T|i_t+1=q_j)\n                      = sum_j(emission_matrix[q_j,o_t+1]*beta_t+1(j)*state_trans_matrix[q_i,q_j])\n            beta_t = np.dot(beta_t+1,(emission_matrix[:,o_t+1]*state_trans_matrix).T)\n            注意：这里是position_wise乘法(*)，不是点乘(.*)\n            shape = np.sum((num_latent_states,)*(num_latent_states,)*(num_latent_states,num_latent_states),axis=0)\n                  = np.sum((num_latent_states,num_latent_states),axis=0)\n                  = (num_latent_states,)\n        4. 目标与beta的关系：\n            beta_1 = p(o_2,...,o_T|i_1=q_i)\n            p(O) = sum_i(p(O,i_1=q_i))\n                 = sum_i(p(O|i_1=q_i)*p(i_1=q_i))\n                 = sum_i(p(o_1,o_2,...,o_T|i_1=q_i)*p(i_1=q_i))\n                 = sum_i(p(o_1|o_2,...,o_T,i_1=q_i)*p(o_2,...,o_T|i_1=q_i)*p(i_1=q_i))\n                 = sum_i(p(o_1|i_1=q_i))*p(o_2,...,o_T|i_1=q_i)*p(i_1=q_i)) #观测独立性假设\n                 = sum_i(emission_matrix[q_i,o_1]*beta_1*init_prob_dist[q_i])\n                 = np.dot(beta_1,emission_matrix[:,o_1]*init_prob_dist[q_i])\n        &#34;&#34;&#34;\n        # 初始化\n        T = len(inputs)\n        beta = []\n        beta_T = np.array([1.]*self.num_latent_states)\n        beta.append(beta_T)\n        beta_t = np.dot(self.state_trans_matrix,self.emission_matrix[:,inputs[-1]])\n        beta.append(beta_t)\n        # 迭代\n        for t in range(0,T-2)[::-1]:\n            beta_t = np.sum(self.state_trans_matrix*beta_t*self.emission_matrix[:,inputs[t+1]],axis=1)\n            beta.append(beta_t)\n        p_O = sum(beta_t*self.init_prob_dist*self.emission_matrix[:,inputs[0]])\n        # 返回的p_O,beta可用于baum-welch算法\n        return p_O,np.array(beta[::-1])\n    \n    def train(self,inputs,conv_loss=1e-8):\n        &#34;&#34;&#34;\n        一、目标：利用Baum-Welch算法无监督训练HMM\n        二、迭代公式（EM算法）：\n            lambda_t+1 = argmax_lambda sum_I[log(p(O,I|lambda))*p(I|O,lambda_t)]\n                       = argmax_lambda sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)\u002Fp(O|lambda_t)]\n                       = argmax_lambda sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)]\n            lambda_t+1 = (init_prob_dist_t+1,state_trans_matrix_t+1,emission_matrix_t+1)\n            设Q函数为：\n                Q(lambda,lambda_t) = sum_I[log(p(I,O|lambda))*p(I,O|lambda_t)]\n            由于：\n                #假设序列长为T\n                p(O|lambda) = sum_I[p(O,I|lambda)]\n                            = sum_i_1[sum_i_2[...[sum_i_T(init_prob_dist[i_1]*prod_i(state_trans_matrix[i_t-1,i_t])*prod_i(emission_matrix[i_t,o_t]))]]]\n            所以Q函数为：\n                Q(lambda,lambda_t) = sum_I[(log(init_prob_dist[i_1])+sum_i(log(state_trans_matrix[i_t-1,i_t]))+sum_i(log(emission_matrix[i_t,o_t])))*p(O,I|lambda_t)]\n            分别拆分为：\n                init_prob_dist_t+1 = argmax_init_prob_dist sum_I[log(init_prob_dist[i_1])*p(O,I|lambda_t)]\n                state_trans_matrix_t+1 = argmax_state_trans_matrix sum_I[sum_i(log(state_trans_matrix[i_t-1,i_t]))*p(O,I|lambda_t)]\n                emission_matrix_t+1 = argmax_emission_matrix sum_I[sum_i(log(emission_matrix[i_t,o_t]))*p(O,I|lambda_t)]\n                这里涉及到约束优化问题：\n                    np.sum(init_prob_dist_i) = 1\n                    np.sum(state_trans_matrix,axis=0) = [1,...,1]     shape=(state_trans_matrix.shape[0],)\n                    np.sum(emission_matrix,axis=0) = [1,...,1]     shape=(emission_matrix.shape[0],)\n                需要用到拉格朗日法求解，这里不做过多展开，具体请参照李航《统计学习方法》的章节《隐马尔可夫模型》：Baum-Welch算法\n            求得各迭代公式可以用李航《统计学习方法》中给出的公式：\n                gamma_t(i) = p(i_t=q_i|O)\n                           = p(i_t=q_i,O)\u002Fp(O)\n                           = alpha_t(i)*beta_t(i)\u002Fsum_j(alpha_t(j)*beta_t(j))\n                gamma = alpha*beta\u002Fnp.sum(alpha*beta,axis=1)\n                      = alpha*beta\u002Fp(O) # np.sum(alpha*beta,axis=1)这个值实际上就是p(O)值\n                xi_t(i,j) = alpha_t(i)*state_trans_matrix[q_i,q_j]*emission_matrix[q_j,o_t+1]*beta_t+1(j)\u002Fsum_i[sum_j(alpha_t(i)*state_trans_matrix[q_i,q_j]*emission_matrix[q_j,o_t+1]*beta_t+1(j))]\n                xi_t = (alpha_t*state_trans_matrix.T).T*emission_matrix[:,o_t+1]*beta_t+1\u002Fnp.sum((alpha_t*state_trans_matrix.T).T*emission_matrix[:,o_t+1]*beta_t+1)\n            1. init_prob_dist：\n                init_prob_dist[q_i] = gamma_1(i)\n                init_prob_dist = gamma_1\n            2. state_trans_matrix：\n                state_trans_matrix[q_i,q_j] = sum_(t=1~T-1)[xi_t(i,j)]\u002Fsum_(t=1~T-1)[gamma_t(i)]\n                                            = np.sum(xi(i,j),axis=0) \u002F np.sum(gamma[:-1](i),axis=0)\n                state_trans_matrix = np.sum(xi,axis=0)\u002Fnp.sum(gamma[:-1],axis=0)\n            3. emission_matrix:\n                emission_matrix[q_j,o_t=v_k] = sum_(t=1~T,o_t=v_k)[gamma_t(j)]\u002Fsum_(t=1~T)[gamma_t(j)]\n                # 这里o_t=v_k暂时还没想好怎么直接用numpy处理，暂时只能用循环处理\n                \n                #emission_from_gamma.shape = (num_latent_states,num_observation_states)\n                emission_from_gamma = np.zeros((num_latent_states,num_observation_states))\n                for gamma_i,o_i in zip(gamma,o):\n                    #gamma_i.shape = (1,num_latent_states)\n                    emission_from_gamma[:,o_i] += gamma_i \n                emission_matrix = (emission_from_gamma.T\u002Fnp.sum([gamma_1,gamma_2,...,gamma_T],axis=0)).T\n        &#34;&#34;&#34;\n        logger = logging.getLogger(&#39;Baum-Welch&#39;)\n        epochs = 1\n        l2_loss = np.Inf\n        while True:\n            \n            init_prob_dist = np.zeros((self.num_latent_states,))\n            state_trans_matrix = np.zeros((self.num_latent_states,self.num_latent_states))\n            emission_matrix = np.zeros((self.num_latent_states,self.num_observation_states))\n            \n            for input_item in inputs:\n                p_O,alpha = self.forward(input_item)\n                p_O,beta = self.backward(input_item)\n                gamma = alpha*beta\u002Fp_O\n                xi = []\n                for t in range(len(input_item)-1):\n                    xi_t = (alpha[t]*self.state_trans_matrix.T).T*self.emission_matrix[:,input_item[t+1]]*beta[t+1]\u002Fnp.sum((alpha[t]*self.state_trans_matrix.T).T*self.emission_matrix[:,input_item[t+1]]*beta[t+1])\n                    xi.append(xi_t)\n                xi = np.array(xi)\n                #计算init_prob_dist\n                init_prob_dist += gamma[0]\n                #计算state_trans_matrix\n                state_trans_matrix += (np.sum(xi,axis=0).T\u002Fnp.sum(gamma[:-1],axis=0)).T\n                #计算emission_matrix\n                emission_from_gamma = np.zeros((self.num_latent_states,self.num_observation_states))\n                for gamma_i,o_i in zip(gamma,input_item):\n                    emission_from_gamma[:,o_i] += gamma_i\n                emission_matrix += (emission_from_gamma.T\u002Fnp.sum(gamma,axis=0)).T\n            init_prob_dist \u002F= len(inputs)\n            state_trans_matrix \u002F= len(inputs)\n            emission_matrix \u002F= len(inputs)\n            \n            l2_loss_old = l2_loss\n            l2_loss = np.sum(np.power(init_prob_dist-self.init_prob_dist,2))+np.sum(np.power(state_trans_matrix-self.state_trans_matrix,2))+np.sum(np.power(emission_matrix-self.emission_matrix,2))\n            if l2_loss_old-l2_loss &lt;= 0 or l2_loss &lt;= conv_loss:\n                logger.info(&#39;training finished!&#39;)\n                break\n            logger.info(&#39;epochs:{}\\tloss:{}&#39;.format(epochs,l2_loss))\n            self.init_prob_dist = init_prob_dist\n            self.state_trans_matrix = state_trans_matrix\n            self.emission_matrix = emission_matrix\n            epochs += 1\n        return\n    \n    def decode(self,inputs):\n        &#34;&#34;&#34;\n        一、目标：利用Viterbi算法解码\n        二、Viterbi算法：\n            1. 初始化：\n                delta_1(i) = init_prob_dist[q_i]*emission_matrix[q_i,o_1]  --&gt; delta_1 = init_prob_dist*emission_matrix[:,o_1]\n                psi_1(i) = 0\n            2. 迭代公式：\n                对于t=2,3,...,T\n                    delta_t(i) = max_(1&lt;=j&lt;=num_latent_states)[[delta_t-1(j)*state_trans_matrix[q_j,q_i]]*emission_matrix[q_i,o_t]]\n                    psi_t(i) = argmax_(1&lt;=j&lt;=num_latent_states)[delta_t-1(j)*state_trans_matrix[q_j,q_i]]  # 这个式子可以带上最后一项emission_matrix[q_i,o_t]]，argmax和最后一项没关系\n            3. 终止：\n                P* = max_(1&lt;=i&lt;=num_latent_states)[delta_T(i)]\n                i*_T = argmax_(1&lt;=i&lt;=num_latent_states)[delta_T(i)]\n            4. 最优路径回溯\n                i*_t = psi_t+1(i*_t+1)\n        &#34;&#34;&#34;\n        logger = logging.getLogger(&#39;Viterbi&#39;)\n        logger.info(&#39;start decoding...&#39;)\n        delta,psi,route = [],[],[]\n        # 初始化\n        delta_1 = self.init_prob_dist*self.emission_matrix[:,inputs[0]]\n        psi_1 = np.zeros((self.num_latent_states,))\n        delta.append(delta_1)\n        psi.append(psi_1)\n        # 迭代\n        for t in range(1,len(inputs)):\n            iter_func = (delta[-1] * self.state_trans_matrix.T).T*self.emission_matrix[:,inputs[t]]\n            delta_t = np.max(iter_func,axis=0)\n            psi_t = np.argmax(iter_func,axis=0)\n            delta.append(delta_t)\n            psi.append(psi_t)\n        # 最优路径回溯\n        route_T = np.argmax(delta[-1])\n        route.append(route_T)\n        for t in range(len(inputs)-1)[::-1]:\n            route_t = psi[t+1][route[-1]]\n            route.append(route_t)\n        route = route[::-1]\n        logger.info(&#39;decoding finished!&#39;)\n        return route\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Ch2\u003E模型使用\u003C\u002Fh2\u003E\u003Cp\u003E一、Evaluating任务\u003C\u002Fp\u003E\u003Cp\u003E这里以李航老师《统计学习方法》（第一版）第177页的10.2例题为例：考虑盒子和球模型 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda%3D%28A%2CB%2C%5Cpi%29\" alt=\"\\lambda=(A,B,\\pi)\" eeimg=\"1\"\u002F\u003E ，状态集合 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=Q%3D%5C%7B1%2C2%2C3%5C%7D\" alt=\"Q=\\{1,2,3\\}\" eeimg=\"1\"\u002F\u003E ，观测集合 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=V%3D%5C%7B%E7%BA%A2%EF%BC%8C%E7%99%BD%5C%7D\" alt=\"V=\\{红，白\\}\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=A%3D%5B%5B0.5%2C0.2%2C0.3%5D%2C%5B0.3%2C0.5%2C0.2%5D%2C%5B0.2%2C0.3%2C0.5%5D%5D\" alt=\"A=[[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]]\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=B%3D%5B%5B0.5%2C0.5%5D%2C%5B0.4%2C0.6%5D%2C%5B0.7%2C0.3%5D%5D\" alt=\"B=[[0.5,0.5],[0.4,0.6],[0.7,0.3]]\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cpi%3D%5B0.2%2C0.4%2C0.4%5D%5ET\" alt=\"\\pi=[0.2,0.4,0.4]^T\" eeimg=\"1\"\u002F\u003E 。设 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=T%3D3\" alt=\"T=3\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=O%3D%28%E7%BA%A2%EF%BC%8C%E7%99%BD%EF%BC%8C%E7%BA%A2%29\" alt=\"O=(红，白，红)\" eeimg=\"1\"\u002F\u003E ，试用前向算法计算 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=P%28O%7C%5Clambda%29\" alt=\"P(O|\\lambda)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E# 初始化状态概率向量\ninit_prob_dist = np.array((0.2,0.4,0.4))\n# 状态转移矩阵\nstate_trans_matrix = np.array([[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]])\n# 发射矩阵\nemission_matrix = np.array([[0.5,0.5],[0.4,0.6],[0.7,0.3]])\n\n# 初始化隐马尔可夫参数\nhmm = HMM(num_latent_states=3,num_observation_states=2,init_prob_dist=init_prob_dist,state_trans_matrix=state_trans_matrix,emission_matrix=emission_matrix)\n# 输入状态序列\ninputs = [0,1,0]\n\n# 前向算法\np_O,alpha = hmm.forward(inputs)\nprint(&#39;p(O):&#39;,p_O)\nprint(&#39;alpha:\\n&#39;,alpha)\n\n# 后向算法\np_O,beta = hmm.backward(inputs)\nprint(&#39;p(O):&#39;,p_O)\nprint(&#39;beta:\\n&#39;,beta)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E输出结果如下：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003Ep(O): 0.130218\nalpha:\n [[ 0.1       0.16      0.28    ]\n [ 0.077     0.1104    0.0606  ]\n [ 0.04187   0.035512  0.052836]]\np(O): 0.130218\nbeta:\n [[ 0.2451  0.2622  0.2277]\n [ 0.54    0.49    0.57  ]\n [ 1.      1.      1.    ]]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E二、Learning任务\u003C\u002Fp\u003E\u003Col\u003E\u003Cli\u003E随机初始化学习：\u003C\u002Fli\u003E\u003C\u002Fol\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E# 随机初始化学习\nhmm = HMM(num_latent_states=3,num_observation_states=2)\n\n# 给定训练序列\nhmm.train([[0,1,0],[1,1,0,1,0,0,0,1]])\n\n# 输出三参数\nprint(&#39;init_prob_dist:\\n&#39;,hmm.init_prob_dist)\nprint(&#39;state_trans_matrix:\\n&#39;,hmm.state_trans_matrix)\nprint(&#39;emission_matrix:\\n&#39;,hmm.emission_matrix)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E输出结果如下：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E2019-07-27 12:13:36,492 - Baum-Welch - epochs:1\tloss:0.20527632330570128\n2019-07-27 12:13:36,495 - Baum-Welch - epochs:2\tloss:0.023973925497813152\n2019-07-27 12:13:36,497 - Baum-Welch - training finished!\n\ninit_prob_dist:\n [ 0.32769973  0.26571133  0.40658893]\nstate_trans_matrix:\n [[ 0.32888918  0.29758607  0.37352475]\n [ 0.3265049   0.1370833   0.5364118 ]\n [ 0.31297094  0.42866714  0.25836193]]\nemission_matrix:\n [[ 0.60303459  0.39696541]\n [ 0.18463565  0.81536435]\n [ 0.82953924  0.17046076]]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E2. 指定三参数学习：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E# 初始化状态概率向量\ninit_prob_dist = np.array((0.2,0.4,0.4))\n# 状态转移矩阵\nstate_trans_matrix = np.array([[0.5,0.2,0.3],[0.3,0.5,0.2],[0.2,0.3,0.5]])\n# 发射矩阵\nemission_matrix = np.array([[0.5,0.5],[0.4,0.6],[0.7,0.3]])\n\n# 初始化隐马尔可夫参数\nhmm = HMM(num_latent_states=3,num_observation_states=2,init_prob_dist=init_prob_dist,state_trans_matrix=state_trans_matrix,emission_matrix=emission_matrix)\n\n# 给定训练序列\nhmm.train([[0,1,0],[1,1,0,1,0,0,0,1]])\n\n# 输出三参数\nprint(&#39;init_prob_dist:\\n&#39;,hmm.init_prob_dist)\nprint(&#39;state_trans_matrix:\\n&#39;,hmm.state_trans_matrix)\nprint(&#39;emission_matrix:\\n&#39;,hmm.emission_matrix)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E输出结果如下：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E2019-07-27 12:17:24,354 - Baum-Welch - epochs:1\tloss:0.02249436309015495\n2019-07-27 12:17:24,357 - Baum-Welch - epochs:2\tloss:0.005045660333614989\n2019-07-27 12:17:24,358 - Baum-Welch - epochs:3\tloss:0.0028643221735452492\n2019-07-27 12:17:24,360 - Baum-Welch - epochs:4\tloss:0.0018477726244472792\n2019-07-27 12:17:24,362 - Baum-Welch - epochs:5\tloss:0.0013193653589663862\n2019-07-27 12:17:24,363 - Baum-Welch - epochs:6\tloss:0.0010374857556465027\n2019-07-27 12:17:24,365 - Baum-Welch - epochs:7\tloss:0.0009009395450544214\n2019-07-27 12:17:24,367 - Baum-Welch - epochs:8\tloss:0.0008644768741977516\n2019-07-27 12:17:24,369 - Baum-Welch - training finished!\n\ninit_prob_dist:\n [ 0.17390824  0.61238671  0.21370505]\nstate_trans_matrix:\n [[ 0.48535613  0.15118683  0.36345704]\n [ 0.33463896  0.48322213  0.18213891]\n [ 0.24597706  0.35745292  0.39657003]]\nemission_matrix:\n [[ 0.54980183  0.45019817]\n [ 0.54087062  0.45912938]\n [ 0.67230652  0.32769348]]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E三、Decoding任务\u003C\u002Fp\u003E\u003Cp\u003E以前述Evaluating任务的用例求解隐状态序列，这里的隐状态为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%280%2C1%2C2%29\" alt=\"(0,1,2)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E# 指定观测序列\ndecode_res = hmm.decode([0,1,0])\n# 输出序列解码结果\nprint(&#39;Viterbi decode result:\\n&#39;,decode_res)\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E输出结果如下：\u003C\u002Fp\u003E\u003Cdiv class=\"highlight\"\u003E\u003Cpre\u003E\u003Ccode class=\"language-text\"\u003E2019-07-27 12:20:35,423 - Viterbi - start decoding...\n2019-07-27 12:20:35,425 - Viterbi - decoding finished!\n\nViterbi decode result:\n [2, 2, 2]\u003C\u002Fcode\u003E\u003C\u002Fpre\u003E\u003C\u002Fdiv\u003E\u003Cp\u003E\u003C\u002Fp\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20682981","type":"topic","id":"20682981","name":"隐马尔可夫模型"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19559450","type":"topic","id":"19559450","name":"机器学习"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19560026","type":"topic","id":"19560026","name":"自然语言处理"}],"voteupCount":138,"voting":0,"column":{"description":"记笔记","canManage":false,"intro":"记笔记","isFollowing":false,"urlToken":"liushaoweihua","id":"liushaoweihua","articlesCount":4,"acceptSubmission":true,"title":"那就权当做是个记笔记的地方吧","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fliushaoweihua","commentPermission":"all","created":1534909382,"updated":1564193877,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F4b70deef7_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_{size}.jpg","uid":"592978442464137216","userType":"people","isFollowing":false,"urlToken":"di-san-zhi-jie","id":"7da50b58e4a1703c1df1fdb456577739","description":"大家好","name":"壮哉我贾诩文和","isAdvertiser":false,"headline":"自然语言处理入门中。原方向：交通数据挖掘，公交线网优化。","gender":1,"url":"\u002Fpeople\u002F7da50b58e4a1703c1df1fdb456577739","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_l.jpg","isOrg":false,"type":"people"},"followers":95,"type":"column"},"commentCount":0,"contributions":[{"id":21371640,"state":"accepted","type":"first_publish","column":{"description":"记笔记","canManage":false,"intro":"记笔记","isFollowing":false,"urlToken":"liushaoweihua","id":"liushaoweihua","articlesCount":4,"acceptSubmission":true,"title":"那就权当做是个记笔记的地方吧","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fliushaoweihua","commentPermission":"all","created":1534909382,"updated":1564193877,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F4b70deef7_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_{size}.jpg","uid":"592978442464137216","userType":"people","isFollowing":false,"urlToken":"di-san-zhi-jie","id":"7da50b58e4a1703c1df1fdb456577739","description":"大家好","name":"壮哉我贾诩文和","isAdvertiser":false,"headline":"自然语言处理入门中。原方向：交通数据挖掘，公交线网优化。","gender":1,"url":"\u002Fpeople\u002F7da50b58e4a1703c1df1fdb456577739","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_l.jpg","isOrg":false,"type":"people"},"followers":95,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":false,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 来自知乎专栏「那就权当做是个记笔记的地方吧」，作者: 壮哉我贾诩文和 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F75406198 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1}},"columns":{"liushaoweihua":{"description":"记笔记","canManage":false,"intro":"记笔记","isFollowing":false,"urlToken":"liushaoweihua","id":"liushaoweihua","articlesCount":4,"acceptSubmission":true,"title":"那就权当做是个记笔记的地方吧","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fliushaoweihua","commentPermission":"all","created":1534909382,"updated":1564193877,"imageUrl":"https:\u002F\u002Fpic2.zhimg.com\u002F4b70deef7_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_{size}.jpg","uid":"592978442464137216","userType":"people","isFollowing":false,"urlToken":"di-san-zhi-jie","id":"7da50b58e4a1703c1df1fdb456577739","description":"大家好","name":"壮哉我贾诩文和","isAdvertiser":false,"headline":"自然语言处理入门中。原方向：交通数据挖掘，公交线网优化。","gender":1,"url":"\u002Fpeople\u002F7da50b58e4a1703c1df1fdb456577739","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fa6327e828920d145469b9daf28c877c6_l.jpg","isOrg":false,"type":"people"},"followers":95,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{}},"currentUser":"3c6b05b6fb1b1e26fdc0c495d2032cf5","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-li_se_heat-5","expPrefix":"li_se_heat","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-li_vertical_move-2","expPrefix":"li_vertical_move","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-nw_zhuantigaiban-2","expPrefix":"nw_zhuantigaiban","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-nw_zhuantikapian-2","expPrefix":"nw_zhuantikapian","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_answer_update-2","expPrefix":"qa_answer_update","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_column_invite-2","expPrefix":"qa_column_invite","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_item_cf-2","expPrefix":"rec_km_item_cf","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_zann-2","expPrefix":"rec_km_zann","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_slot_prerank-2","expPrefix":"rec_slot_prerank","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_cardrank_4-4","expPrefix":"se_cardrank_4","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_collegecm-3","expPrefix":"se_collegecm","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ctr_pyc-3","expPrefix":"se_ctr_pyc","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ctr_topic-3","expPrefix":"se_ctr_topic","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ctr_user-3","expPrefix":"se_ctr_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_dnn_unbias-4","expPrefix":"se_dnn_unbias","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_hot__timebox-2","expPrefix":"se_hot__timebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ios_spb309-5","expPrefix":"se_ios_spb309","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ltr_user-2","expPrefix":"se_ltr_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_payconsult-3","expPrefix":"se_payconsult","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_subtext-2","expPrefix":"se_subtext","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_wannasearch-5","expPrefix":"se_wannasearch","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_webtimebox-2","expPrefix":"se_webtimebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_whitelist-2","expPrefix":"se_whitelist","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-top_billsheep2-2","expPrefix":"top_billsheep2","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-top_hotchild-2","expPrefix":"top_hotchild","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-top_vote-2","expPrefix":"top_vote","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-tp_and_sticky-2","expPrefix":"tp_and_sticky","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_bigone-10","expPrefix":"us_bigone","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_newguide3-11","expPrefix":"us_newguide3","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_notification-2","expPrefix":"us_notification","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_n_web_msg-5","expPrefix":"us_n_web_msg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_ad-1","expPrefix":"vd_video_ad","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_qa_new_cover-2","expPrefix":"li_qa_new_cover","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_banner_type-16","expPrefix":"li_banner_type","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_search_answer-1","expPrefix":"li_search_answer","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_dnn_mt_new-2","expPrefix":"se_dnn_mt_new","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_mclick-6","expPrefix":"se_mclick","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_aa_base-1","expPrefix":"se_aa_base","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_zu_onebox-2","expPrefix":"se_zu_onebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_bignew-3","expPrefix":"us_bignew","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_update-8","expPrefix":"us_update","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_native_ans-10","expPrefix":"top_native_ans","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_vipconsume-8","expPrefix":"top_vipconsume","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"tp_club-6","expPrefix":"tp_club","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_hotctr-7","expPrefix":"top_hotctr","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_newchild-14","expPrefix":"top_newchild","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_answer_cp-2","expPrefix":"rec_answer_cp","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_test_aa1-7","expPrefix":"rec_test_aa1","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_paycqaedit-2","expPrefix":"qa_paycqaedit","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotctr","type":"String","value":"1","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"1"},{"id":"zr_cold_start","type":"String","value":"0","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"1","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt","type":"String","value":"0","chainId":"_all_"},{"id":"se_pro","type":"String","value":"0","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"top_vipconsume","type":"String","value":"3","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"web_column_auto_invite","type":"String","value":"1"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_prerank","type":"String","value":"new","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"1","chainId":"_all_"},{"id":"se_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"1","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_album_card","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_col_boost","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr_user","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_billboardhead","type":"String","value":"1","chainId":"_all_"},{"id":"ls_zvideo_license","type":"String","value":"0","chainId":"_all_"},{"id":"li_book_button","type":"String","value":"0","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"0","chainId":"_all_"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_ltr_dnn_cp","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"li_vip_no_ad_mon","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_item_prerank","type":"String","value":"old","chainId":"_all_"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_kv","type":"String","value":"0","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"3","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"zr_km_feed_prerank","type":"String","value":"new","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"0","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"1","chainId":"_all_"},{"id":"ls_new_upload","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_mclick","type":"String","value":"0","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_recall","type":"String","value":"default","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"a","chainId":"_all_"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"2","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_newchild","type":"String","value":"7","chainId":"_all_"},{"id":"li_se_vertical","type":"String","value":"1","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"1","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"3","chainId":"_all_"},{"id":"se_ctr_pyc","type":"String","value":"1","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_perf","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"1","chainId":"_all_"},{"id":"se_ltr_user","type":"String","value":"1","chainId":"_all_"},{"id":"tsp_billboardsheep2","type":"String","value":"2","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_km_item_cf","type":"String","value":"open","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"zr_search_xgb","type":"String","value":"1","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"1"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"6","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"li_search_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"tsp_childbillboard","type":"String","value":"2","chainId":"_all_"},{"id":"li_se_paid_answer","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_unbias","type":"String","value":"1","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"se_mclick1","type":"String","value":"2","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"5","chainId":"_all_"},{"id":"li_se_xgb","type":"String","value":"0","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"1"},{"id":"zr_item_nn_recall","type":"String","value":"close","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"zr_man_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_go_ztext","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"zr_infinity_member","type":"String","value":"close","chainId":"_all_"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr_topic","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"zr_km_topic_zann","type":"String","value":"new","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"1","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"soc_update","type":"String","value":"0","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"1","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"0"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"1","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; WOW64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F69.0.3947.100 Safari\u002F537.36"},"ctx":{"path":"\u002Fp\u002F75406198"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false},"theme":"light","enableShortcut":true,"referer":"","conf":{},"ipInfo":{"cityName":"Beijing","countryName":"China","regionName":"Beijing","countryCode":"CN"},"logged":true,"tdkInfo":{}},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"liushaoweihua"]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"mcn":{"bindInfo":{},"memberCategoryList":[],"categoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/vendor.1c14c07589f9cc850a64.js"></script><script src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/column.app.3116d7988c77f2e92161.js"></script><script></script><script src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/hm.js" async=""></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><script src="./概率图模型系列-隐马尔可夫模型（HMM）的numpy实现 - 知乎_files/zap.js"></script><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><div class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete13-0" id="Popover12-toggle" aria-haspopup="true" aria-owns="Popover12-content" class="Input" placeholder="选择语言" value=""><div class="Input-after"><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></div></div></div></div></div></div></div><div><div><div></div></div></div><div><div><div></div></div></div></body></html>