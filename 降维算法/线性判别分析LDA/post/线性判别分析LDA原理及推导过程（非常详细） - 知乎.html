<!DOCTYPE html>
<!-- saved from url=(0037)https://zhuanlan.zhihu.com/p/79696530 -->
<html lang="zh" data-hairline="true" data-theme="light"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><title>线性判别分析LDA原理及推导过程（非常详细） - 知乎</title><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta name="renderer" content="webkit"><meta name="force-rendering" content="webkit"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="google-site-verification" content="FTeR0c8arOPKh8c5DYh_9uu98_zJbaWw53J-Sch9MTg"><meta data-react-helmet="true" name="description" content="线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识…"><meta data-react-helmet="true" property="og:title" content="线性判别分析LDA原理及推导过程（非常详细）"><meta data-react-helmet="true" property="og:url" content="https://zhuanlan.zhihu.com/p/79696530"><meta data-react-helmet="true" property="og:description" content="线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识…"><meta data-react-helmet="true" property="og:image" content="https://pic4.zhimg.com/v2-6afb5e13569c05c07be1f17b3f573b32_b.jpg"><meta data-react-helmet="true" property="og:type" content="article"><meta data-react-helmet="true" property="og:site_name" content="知乎专栏"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-152.67c7b278.png" sizes="152x152"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-120.b3e6278d.png" sizes="120x120"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-76.7a750095.png" sizes="76x76"><link data-react-helmet="true" rel="apple-touch-icon" href="https://static.zhihu.com/heifetz/assets/apple-touch-icon-60.a4a761d4.png" sizes="60x60"><link rel="shortcut icon" type="image/x-icon" href="https://static.zhihu.com/static/favicon.ico"><link rel="search" type="application/opensearchdescription+xml" href="https://static.zhihu.com/static/search.xml" title="知乎"><link rel="dns-prefetch" href="https://static.zhimg.com/"><link rel="dns-prefetch" href="https://pic1.zhimg.com/"><link rel="dns-prefetch" href="https://pic2.zhimg.com/"><link rel="dns-prefetch" href="https://pic3.zhimg.com/"><link rel="dns-prefetch" href="https://pic4.zhimg.com/"><style>
.u-safeAreaInset-top {
  height: constant(safe-area-inset-top) !important;
  height: env(safe-area-inset-top) !important;
  
}
.u-safeAreaInset-bottom {
  height: constant(safe-area-inset-bottom) !important;
  height: env(safe-area-inset-bottom) !important;
  
}
</style><link href="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/app.ddefb90366efb2341155.css" rel="stylesheet"><link rel="stylesheet"><script defer="" crossorigin="anonymous" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/init.js" data-sentry-config="{&quot;dsn&quot;:&quot;https://65e244586890460588f00f2987137aa8@crash2.zhihu.com/193&quot;,&quot;sampleRate&quot;:0.1,&quot;release&quot;:&quot;574-91120cae&quot;,&quot;ignoreErrorNames&quot;:[&quot;NetworkError&quot;,&quot;SecurityError&quot;],&quot;ignoreErrors&quot;:[&quot;origin message&quot;,&quot;Network request failed&quot;,&quot;Loading chunk&quot;,&quot;这个系统不支持该功能。&quot;,&quot;Can&#39;t find variable: webkit&quot;,&quot;Can&#39;t find variable: $&quot;,&quot;内存不足&quot;,&quot;out of memory&quot;,&quot;DOM Exception 18&quot;,&quot;The operation is insecure&quot;,&quot;[object Event]&quot;,&quot;[object FileError]&quot;,&quot;[object DOMError]&quot;,&quot;[object Object]&quot;,&quot;拒绝访问。&quot;,&quot;Maximum call stack size exceeded&quot;,&quot;UploadError&quot;,&quot;无法 fetch&quot;,&quot;draft-js&quot;,&quot;缺少 JavaScript 对象&quot;,&quot;componentWillEnter&quot;,&quot;componentWillLeave&quot;,&quot;componentWillAppear&quot;,&quot;getInlineStyleAt&quot;,&quot;getCharacterList&quot;],&quot;whitelistUrls&quot;:[&quot;static.zhihu.com&quot;]}"></script><link rel="stylesheet" type="text/css" href="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/modals.be8fb3cda49d7af439fa.css"><script charset="utf-8" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/column.modals.ec37d3440eccc2380961.js"></script><link rel="stylesheet" type="text/css" href="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/richinput.9f4247a14dfc617ddf67.css"><script charset="utf-8" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/column.richinput.c42399f9a6e69884ffc3.js"></script></head><body class="WhiteBg-body" data-react-helmet="class"><div id="root"><div class="App"><div class="LoadingBar"></div><main role="main" class="App-main"><div class="Post-content" data-zop-usertoken="{&quot;userToken&quot;:&quot;soko-60&quot;}" data-zop="{&quot;authorName&quot;:&quot;陈楠&quot;,&quot;itemId&quot;:79696530,&quot;title&quot;:&quot;线性判别分析LDA原理及推导过程（非常详细）&quot;,&quot;type&quot;:&quot;article&quot;}" data-za-detail-view-path-module="PostItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;token&quot;:&quot;79696530&quot;}}}"><div class="ColumnPageHeader-Wrapper"><div><div class="Sticky ColumnPageHeader is-fixed" style="width: 1478.4px; top: 0px; left: 0px;"><div class="ColumnPageHeader-content"><a href="https://www.zhihu.com/" aria-label="知乎"><svg viewBox="0 0 200 91" class="Icon ZhihuLogo ZhihuLogo--blue Icon--logo" style="height:30px;width:64px" width="64" height="30" aria-hidden="true"><title></title><g><path d="M53.29 80.035l7.32.002 2.41 8.24 13.128-8.24h15.477v-67.98H53.29v67.978zm7.79-60.598h22.756v53.22h-8.73l-8.718 5.473-1.587-5.46-3.72-.012v-53.22zM46.818 43.162h-16.35c.545-8.467.687-16.12.687-22.955h15.987s.615-7.05-2.68-6.97H16.807c1.09-4.1 2.46-8.332 4.1-12.708 0 0-7.523 0-10.085 6.74-1.06 2.78-4.128 13.48-9.592 24.41 1.84-.2 7.927-.37 11.512-6.94.66-1.84.785-2.08 1.605-4.54h9.02c0 3.28-.374 20.9-.526 22.95H6.51c-3.67 0-4.863 7.38-4.863 7.38H22.14C20.765 66.11 13.385 79.24 0 89.62c6.403 1.828 12.784-.29 15.937-3.094 0 0 7.182-6.53 11.12-21.64L43.92 85.18s2.473-8.402-.388-12.496c-2.37-2.788-8.768-10.33-11.496-13.064l-4.57 3.627c1.363-4.368 2.183-8.61 2.46-12.71H49.19s-.027-7.38-2.372-7.38zm128.752-.502c6.51-8.013 14.054-18.302 14.054-18.302s-5.827-4.625-8.556-1.27c-1.874 2.548-11.51 15.063-11.51 15.063l6.012 4.51zm-46.903-18.462c-2.814-2.577-8.096.667-8.096.667s12.35 17.2 12.85 17.953l6.08-4.29s-8.02-11.752-10.83-14.33zM199.99 46.5c-6.18 0-40.908.292-40.953.292v-31.56c1.503 0 3.882-.124 7.14-.376 12.773-.753 21.914-1.25 27.427-1.504 0 0 3.817-8.496-.185-10.45-.96-.37-7.24 1.43-7.24 1.43s-51.63 5.153-72.61 5.64c.5 2.756 2.38 5.336 4.93 6.11 4.16 1.087 7.09.53 15.36.277 7.76-.5 13.65-.76 17.66-.76v31.19h-41.71s.88 6.97 7.97 7.14h33.73v22.16c0 4.364-3.498 6.87-7.65 6.6-4.4.034-8.15-.36-13.027-.566.623 1.24 1.977 4.496 6.035 6.824 3.087 1.502 5.054 2.053 8.13 2.053 9.237 0 14.27-5.4 14.027-14.16V53.93h38.235c3.026 0 2.72-7.432 2.72-7.432z" fill-rule="evenodd"></path></g></svg></a><i class="ColumnPageHeader-Line"></i><div class="ColumnPageHeader-Title"><a class="ColumnLink ColumnPageHeader-Link" href="https://zhuanlan.zhihu.com/chennan"><img class="Avatar Avatar--round" width="30" height="30" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-dac72633cc016530b3d119b21c373542_is.jpg" srcset="https://pic1.zhimg.com/v2-dac72633cc016530b3d119b21c373542_im.jpg 2x" alt="机器学习"></a><div class="ColumnPageHeader-TitleName"><span class="ColumnPageHeader-TitleMeta">首发于</span><a class="ColumnLink ColumnPageHeader-TitleColumn" href="https://zhuanlan.zhihu.com/chennan">机器学习</a></div></div><div class="ColumnPageHeader-Button"><button type="button" class="Button FollowButton ColumnPageHeader-FollowButton Button--primary Button--blue">关注专栏</button><button type="button" class="Button ColumnPageHeader-WriteButton Button--blue"><svg class="Zi Zi--EditSurround" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M18.453 7.992l-1.833-1.65.964-.978a1.223 1.223 0 0 1 1.73-.012l.005.006a1.24 1.24 0 0 1 .007 1.748l-.873.886zm-1.178 1.194l-5.578 5.66-1.935.697a.393.393 0 0 1-.504-.504l.697-1.935 5.488-5.567 1.832 1.65zM7.58 5.848l5.654.006-1.539 1.991-3.666.012A1.02 1.02 0 0 0 7 8.868v7.993c0 .558.46 1.01 1.029 1.01l7.941-.01c.568 0 1.03-.453 1.03-1.012v-4.061l2-1.442v6.002c0 1.397-1.2 2.501-2.62 2.501H7.574C6.153 19.85 5 18.717 5 17.32V8.35c0-1.397 1.16-2.502 2.58-2.502z"></path></svg>写文章</button><div class="Popover"><button title="更多" type="button" id="Popover1-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover1-content" class="Button ColumnPageHeader-MenuToggler Button--plain"><svg class="Zi Zi--Dots" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></button></div></div></div></div><div class="Sticky--holder" style="position: relative; top: 0px; right: 0px; bottom: 0px; left: 0px; display: block; float: none; margin: 0px; height: 52px;"></div></div></div><img class="TitleImage" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-6afb5e13569c05c07be1f17b3f573b32_1200x500.jpg" alt="线性判别分析LDA原理及推导过程（非常详细）"><article class="Post-Main Post-NormalMain" tabindex="-1"><header class="Post-Header"><h1 class="Post-Title">线性判别分析LDA原理及推导过程（非常详细）</h1><div class="Post-Author"><div class="AuthorInfo" itemprop="author" itemscope="" itemtype="http://schema.org/Person"><meta itemprop="name" content="陈楠"><meta itemprop="image" content="https://pic1.zhimg.com/961a81b83e4f59391d0401f0c77ee9ea_is.jpg"><meta itemprop="url" content="https://www.zhihu.com/people/yao0118"><meta itemprop="zhihu:followerCount" content="5248"><span class="UserLink AuthorInfo-avatarWrapper"><div class="Popover"><div id="Popover2-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover2-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yao0118"><img class="Avatar Avatar--round AuthorInfo-avatar" width="38" height="38" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/961a81b83e4f59391d0401f0c77ee9ea_xs.jpg" srcset="https://pic1.zhimg.com/961a81b83e4f59391d0401f0c77ee9ea_l.jpg 2x" alt="陈楠"></a></div></div></span><div class="AuthorInfo-content"><div class="AuthorInfo-head"><span class="UserLink AuthorInfo-name"><div class="Popover"><div id="Popover3-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover3-content"><a class="UserLink-link" data-za-detail-view-element_name="User" target="_blank" href="https://www.zhihu.com/people/yao0118">陈楠</a></div></div><a class="UserLink-badge" data-tooltip="已认证的个人" href="https://www.zhihu.com/question/48510028" target="_blank" rel="noopener noreferrer"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--BadgeCert" fill="currentColor" viewBox="0 0 24 24" width="18" height="18"><g fill="none" fill-rule="evenodd"><path fill="#0F88EB" d="M2.64 13.39c1.068.895 1.808 2.733 1.66 4.113l.022-.196c-.147 1.384.856 2.4 2.24 2.278l-.198.016c1.387-.122 3.21.655 4.083 1.734l-.125-.154c.876 1.084 2.304 1.092 3.195.027l-.127.152c.895-1.068 2.733-1.808 4.113-1.66l-.198-.022c1.386.147 2.402-.856 2.279-2.238l.017.197c-.122-1.388.655-3.212 1.734-4.084l-.154.125c1.083-.876 1.092-2.304.027-3.195l.152.127c-1.068-.895-1.808-2.732-1.66-4.113l-.022.198c.147-1.386-.856-2.4-2.24-2.279l.198-.017c-1.387.123-3.21-.654-4.083-1.733l.125.153c-.876-1.083-2.304-1.092-3.195-.027l.127-.152c-.895 1.068-2.733 1.808-4.113 1.662l.198.02c-1.386-.147-2.4.857-2.279 2.24L4.4 6.363c.122 1.387-.655 3.21-1.734 4.084l.154-.126c-1.083.878-1.092 2.304-.027 3.195l-.152-.127z"></path><path fill="#FFF" d="M9.78 15.728l-2.633-2.999s-.458-.705.242-1.362c.7-.657 1.328-.219 1.328-.219l1.953 2.132 4.696-4.931s.663-.348 1.299.198c.636.545.27 1.382.27 1.382s-3.466 3.858-5.376 5.782c-.98.93-1.778.017-1.778.017z"></path></g></svg></span></a></span></div><div class="AuthorInfo-detail"><div class="AuthorInfo-badge"><div class="AuthorInfo-badgeText">北京邮电大学 电子科学与技术硕士</div></div></div></div></div><button type="button" class="Button FollowButton Button--primary Button--blue"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Plus FollowButton-icon" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M13.491 10.488s-.012-5.387 0-5.998c-.037-1.987-3.035-1.987-2.997 0-.038 1.912 0 5.998 0 5.998H4.499c-1.999.01-1.999 3.009 0 3.009s5.995-.01 5.995-.01v5.999c0 2.019 3.006 2.019 2.997 0-.01-2.019 0-5.998 0-5.998s3.996.009 6.004.009c2.008 0 2.008-3-.01-3.009h-5.994z" fill-rule="evenodd"></path></svg></span>关注他</button></div><div><span class="Voters"><button type="button" class="Button Button--plain">14 人<!-- -->赞同了该文章</button></span></div></header><div class="Post-RichTextContainer"><div class="RichText ztext Post-RichText"><p>线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了解下它的算法原理。</p><h3>1. LDA的思想</h3><p>LDA的思想是：<b>最大化类间均值，最小化类内方差</b>。意思就是将数据投影在低维度上，并且投影后同种类别数据的投影点尽可能的接近，不同类别数据的投影点的中心点尽可能的远。</p><p>我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。</p><figure data-size="normal"><noscript><img src="https://pic4.zhimg.com/v2-3b91b15664720a0cb9198bca2c84bb3b_b.jpg" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="451" class="origin_image zh-lightbox-thumb" width="1308" data-original="https://pic4.zhimg.com/v2-3b91b15664720a0cb9198bca2c84bb3b_r.jpg"/></noscript><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-3b91b15664720a0cb9198bca2c84bb3b_hd.jpg" data-caption="" data-size="normal" data-rawwidth="1308" data-rawheight="451" class="origin_image zh-lightbox-thumb lazy" width="1308" data-original="https://pic4.zhimg.com/v2-3b91b15664720a0cb9198bca2c84bb3b_r.jpg" data-actualsrc="https://pic4.zhimg.com/v2-3b91b15664720a0cb9198bca2c84bb3b_b.jpg" data-lazy-status="ok"></figure><p>上图提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。</p><h3>2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)</h3><p>首先来看瑞利商的定义。瑞利商是指这样的函数 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation" alt="[公式]" eeimg="1" data-formula="R(A,x) "> :</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(1)" alt="[公式]" eeimg="1" data-formula="R(A, x)=\frac{x^{H} A x}{x^{H} x}"> </p><p>其中， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula=" x "> 是非零向量，而 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula=" A "> 为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula=" n\times n "> 的Hermitan矩阵。所谓的Hermitan矩阵就是满足<b>共轭转置矩阵</b>和自己相等的矩阵，即 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(5)" alt="[公式]" eeimg="1" data-formula=" A^H=A "> ，例如， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(6)" alt="[公式]" eeimg="1" data-formula=" A=\left(\begin{array}{cc}{1} &amp; {2+i} \\ {2-i} &amp; {1}\end{array}\right) "> 的共轭转置等于其本身。当 A 为实矩阵时，如果满足 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(7)" alt="[公式]" eeimg="1" data-formula=" A^T=A "> ，则 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(3)" alt="[公式]" eeimg="1" data-formula=" A "> 为Hermitan矩阵。</p><p>瑞利商 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(8)" alt="[公式]" eeimg="1" data-formula="R(A,x)"> 有一个非常重要的性质，即<b>它的最大值等于矩阵 A 最大的特征值，而最小值等于矩阵 A 的最小的特征值</b>，也就是满足  </p><p><b><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(9)" alt="[公式]" eeimg="1" data-formula="\lambda_{\min } \leq \frac{x^{H} A x}{x^{H} x} \leq \lambda_{\max }"></b> </p><p>当向量 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(2)" alt="[公式]" eeimg="1" data-formula=" x "> 是标准正交基，即满足 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(10)" alt="[公式]" eeimg="1" data-formula=" x^{H} x=1 "> 时，瑞利商退化为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(11)" alt="[公式]" eeimg="1" data-formula=" R(A, x)=x^{H} A x"> 。</p><p>下面看一下广义瑞利商。广义瑞利商是指这样的函数 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula=" R(A,B,x) "> :  </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(13)" alt="[公式]" eeimg="1" data-formula="R(A,B, x)=\frac{x^{H} A x}{x^{H} B x}"> </p><p>其中 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(14)" alt="[公式]" eeimg="1" data-formula="x"> 为非零向量，而 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(15)" alt="[公式]" eeimg="1" data-formula=" A,B "> 为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(4)" alt="[公式]" eeimg="1" data-formula=" n\times n "> 的Hermitan矩阵。 B 为正定矩阵。它的最大值和最小值是什么呢？其实我们只要通过将其通过标准化就可以转化为瑞利商的格式。令 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(16)" alt="[公式]" eeimg="1" data-formula=" x=B^{-1 / 2} x^{\prime}"> ，则分母转化为：  </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(17)" alt="[公式]" eeimg="1" data-formula="x^{H} B x=x^{\prime H}\left(B^{-1 / 2}\right)^{H} B B^{-1 / 2} x^{\prime}=x^{\prime H} B^{-1 / 2} B B^{-1 / 2} x^{\prime}=x^{\prime H} x^{\prime}"> </p><p>而分子转化为：  </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(18)" alt="[公式]" eeimg="1" data-formula="x^{H} A x=x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}"> </p><p>此时我们的 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(12)" alt="[公式]" eeimg="1" data-formula=" R(A,B,x) "> 转化为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(19)" alt="[公式]" eeimg="1" data-formula=" R(A,B,x&#39;)"> :</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(20)" alt="[公式]" eeimg="1" data-formula="   R\left(A, B, x^{\prime}\right)=\frac{x^{\prime H} B^{-1 / 2} A B^{-1 / 2} x^{\prime}}{x^{\prime H} x^{\prime}}"> </p><p>利用前面的瑞利商的性质，我们可以很快的知道， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(21)" alt="[公式]" eeimg="1" data-formula=" R(A,B,x&#39;) "> 的最大值为矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(22)" alt="[公式]" eeimg="1" data-formula=" B^{-1 / 2} A B^{-1 / 2} "> 的最大特征值，或者说矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(23)" alt="[公式]" eeimg="1" data-formula=" B^{−1}A"> 的最大特征值，而最小值为矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(24)" alt="[公式]" eeimg="1" data-formula="B^{−1}A "> 的最小特征值。</p><h3>2. LDA的原理及推导过程</h3><p>假设样本共有 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(25)" alt="[公式]" eeimg="1" data-formula="K"> 类，每一个类的样本的个数分别为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(26)" alt="[公式]" eeimg="1" data-formula="N_1,N_2,...,N_k "> </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(27)" alt="[公式]" eeimg="1" data-formula=" x_1^1,x_1^2,...,x_1^{N_1} "> 对应第1类</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(28)" alt="[公式]" eeimg="1" data-formula=" x_2^1,x_2^2,...,x_2^{N_2} "> 对应第2类</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(29)" alt="[公式]" eeimg="1" data-formula=" x_k^1,x_k^2,...,x_k^{N_k}"> 对应第 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(25)" alt="[公式]" eeimg="1" data-formula="K"> 类，其中每个样本 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(30)" alt="[公式]" eeimg="1" data-formula=" x_i^j "> 均为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(31)" alt="[公式]" eeimg="1" data-formula=" n "> 维向量 </p><p>设 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(32)" alt="[公式]" eeimg="1" data-formula=" \tilde{x}_{i}^{j} 为 x_i^j "> 变化后的样本，则 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(33)" alt="[公式]" eeimg="1" data-formula="\tilde{x}_{i}^{j}=&lt;x, u&gt;u=|x||u|cos\theta \cdot u =\left(x^{T} u\right) u"> </p><p>此处设 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(34)" alt="[公式]" eeimg="1" data-formula=" u "> 为单位向量，即 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(35)" alt="[公式]" eeimg="1" data-formula=" u^Tu=1"> .</p><p>假设第K类样本的数据集为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(36)" alt="[公式]" eeimg="1" data-formula=" D_k "> ，变化后的样本的均值向量为： <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(37)" alt="[公式]" eeimg="1" data-formula="\tilde{\mathrm{m}}=\frac{\sum_{\tilde{x}\in D_k}\tilde{x}}{N_k} "> ，那么，第K类样本的方差为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(38)" alt="[公式]" eeimg="1" data-formula=" \frac{S_k}{N_k} "> ，其中：   <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(39)" alt="[公式]" eeimg="1" data-formula="\begin{aligned} \mathrm{S}_{\mathrm{k}} &amp;=\sum_{\tilde{x} \in D_{k}}(\tilde{x}-\tilde{\mathrm{m}})^{T}(\tilde{x}-\tilde{\mathrm{m}})\\  &amp;=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u-\left(m^{T} u\right) u\right]^T\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right] \\ &amp;=\sum_{x \in D_{k}}\left[\left(x^{T} u\right) u^{T}-\left(m^{T} u\right) u^{T}\right]\left[\left(x^{T} u\right) u -\left(m^{T} u\right) u\right]  \quad (x^Tu，m^Tu为实数,转置仍是本身)\\  &amp;=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2} u^{T} u-2\left(x^{T} u\right)\left(m^{T} u\right) u^{T} u+\left(m^{T} u\right)^{2} u^{T} u\right] \\  &amp;=\sum_{x \in D_{k}}\left[\left(x^{T} u\right)^{2}-2\left(x^{T} u\right)\left(m^{T} u\right)+\left(m^{T} u\right)^{2}\right] \end{aligned}"> </p><p>第 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(25)" alt="[公式]" eeimg="1" data-formula="K"> 类样本的方差：   <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(40)" alt="[公式]" eeimg="1" data-formula="\begin{aligned} \frac{S_{k}}{N_{k}} &amp;=\frac{\sum_{x \in D_{k}}\left(x^{T} u\right)^{2}}{N_{k}}-2 \frac{\sum_{x \in D_{k}} x^{T}\left(u m^{T} u\right)}{N_{k}}+\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}} \\  &amp;=\frac{\sum_{x\in D_k} u^{T} x x^{T} u}{N_{k}}-2 \frac{\sum_{x\in D_k} x^{T}}{N_{k}} u m^{T} u+\left(m^{T} u\right)^{2}\quad (注：\frac{\sum_{x\in D_{k}}\left(m^{T} u\right)^{2}}{N_{k}}=(m^Tu)^2。因为m^Tu与x无关)\\  &amp;=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-\left(m^{T} u\right)^{2} \quad (注：\frac{\sum_{x \in D_k}x^T}{N_k}=m^T)\\  &amp;=u^{T} \frac{\sum_{x\in D_k} x x^{T}}{N_{k}} u-u^{T} m m^{T} u \\  &amp;=u^{T}\left(\frac{\sum_{x\in D_k} x x^{T}}{N_{k}}-m m^{T}\right) u \end{aligned}"> </p><p>各个类别的样本方差之和：  </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(41)" alt="[公式]" eeimg="1" data-formula="\begin{aligned} \sum_{k}\frac{S_k}{N_k} &amp;=\sum_{k=1}^{K}u^{T}\left(\frac{\sum_{x \in D_k} x x^{T}}{N_{k}}-m_k m_k^{T}\right) u \\ &amp;=u^T\sum_{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)u\\&amp;=u^TS_wu \end{aligned}"> </p><p>其中， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(42)" alt="[公式]" eeimg="1" data-formula="S_w=\sum_{k=1}^{K}(\frac{\sum_{x \in D_k}xx^T}{N_k}-m_km_k^T)"> ， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(43)" alt="[公式]" eeimg="1" data-formula=" S_w "> 一般被称为类内散度矩阵</p><p>不同类别 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(44)" alt="[公式]" eeimg="1" data-formula=" i,j "> 之间的中心距离：</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(45)" alt="[公式]" eeimg="1" data-formula=" \begin{aligned} S_{i j} &amp;=\left(\tilde{m}_{i}-\tilde{m}_{j}\right)^{T}\left(\tilde{m}_{i}-\tilde{m}_{j}\right) \\ &amp;=[(u^Tm_i)u-(u^Tm_j)u]^T[(m_i^Tu)u-(m_j^Tu)u] \\ &amp;=[(u^Tm_i)u^T-(u^Tm_j)u^T][u(m_i^Tu)-u(m_j^Tu)] \\ &amp;=u^T(m_i-m_j)u^Tu(m_i^T-m_j^T)u \\ &amp;=u^{T}\left(m_{i}-m_{j}\right)\left(m_{i}-m_{j}\right)^{T} u \end{aligned}   "> </p><p>所有类别之间的距离之和为：</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(46)" alt="[公式]" eeimg="1" data-formula="   \begin{aligned}     \sum_{i, j \atop i \neq j} s_{i j} &amp;= u^T\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]u \\     &amp;=u^TS_bu \end{aligned}"> </p><p>其中， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(47)" alt="[公式]" eeimg="1" data-formula="S_b=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]"> ， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(48)" alt="[公式]" eeimg="1" data-formula="S_b "> 一般被称为类间散度矩阵。</p><p>在已知条件下， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(49)" alt="[公式]" eeimg="1" data-formula=" S_w,S_b "> 均可求出，LDA算法的目标是<b>“类间距离尽可能大，类内方差尽可能小”</b>，即最大化 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(50)" alt="[公式]" eeimg="1" data-formula=" u^TS_bu "> ,最小化 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(51)" alt="[公式]" eeimg="1" data-formula="u^TS_wu"> .</p><p>令 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(52)" alt="[公式]" eeimg="1" data-formula=" J(u)=\frac{u^{T} S_{b} u}{u^{T} S_{w} u} "> ,则目标函数为：  </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(53)" alt="[公式]" eeimg="1" data-formula="\max J(u)"> </p><p>为了使所求最大，可假设 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(54)" alt="[公式]" eeimg="1" data-formula=" u^TS_wu=1"> ，则问题转化为：</p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(55)" alt="[公式]" eeimg="1" data-formula=" \begin{array}{l}{\max u^{T} S_{b} u} \\ s.t. \quad u^TS_wu=1  \end{array}   "> </p><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(56)" alt="[公式]" eeimg="1" data-formula="\begin{aligned}  L(u, \lambda)&amp;=u^{T} S_{b} u+\lambda\left(1-u^{T} S_{w} u\right) \\  \frac{\partial L}{\partial u} &amp;= S_bu+S_b^Tu-\lambda S_wu-\lambda S_w^Tu \\ &amp;=2(S_bu-\lambda S_wu)\quad (因为S_b，S_w为对称矩阵)\\ &amp;=0 \Rightarrow S_{b} u=\lambda S_{w} u \end{aligned}"> </p><blockquote>证明 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(57)" alt="[公式]" eeimg="1" data-formula="S_b"> 为对称矩阵：  <br> <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(58)" alt="[公式]" eeimg="1" data-formula="\begin{aligned}     S_b^T &amp;= (\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T])^T\\     &amp;=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T]^T \\     &amp;=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T] \\     &amp;=S_b \end{aligned}"><br>所以， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(59)" alt="[公式]" eeimg="1" data-formula=" S_b "> 为对称矩阵，同理可证明 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(43)" alt="[公式]" eeimg="1" data-formula=" S_w "> 为对称矩阵。</blockquote><p><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(60)" alt="[公式]" eeimg="1" data-formula="\begin{array}{l} S_{w}^{-1}{S_{b}  u=\lambda u} \end{array}   "> </p><p>计算矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula=" S_w^{-1}S_b "> 的最大的 d 个特征值和对应的 d 个特征向量 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(62)" alt="[公式]" eeimg="1" data-formula=" (w_1,w_2,...,w_d) "> ，得到投影矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(63)" alt="[公式]" eeimg="1" data-formula="W=(w_1,w_2,...,w_d)"> 。</p><blockquote>注意: (1)选取特征值时，如果一些特征值明显大于其他的特征值，则取这些取值较大的特征值，因为它们包含更多的数据分布的信息。相反，如果一些特征值接近于0，我们将这些特征值舍去。<br>(2)由于 W 是一个利用了样本类别得到的投影矩阵，因此它能够降维到的维度d的最大值为 K-1 。为什么不是 K 呢？因为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(59)" alt="[公式]" eeimg="1" data-formula=" S_b "> 中每个 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(64)" alt="[公式]" eeimg="1" data-formula=" m_i-m_j "> 的秩均为1(因为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(65)" alt="[公式]" eeimg="1" data-formula=" m_i "> 为1维向量).<br>由 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(66)" alt="[公式]" eeimg="1" data-formula="R(AB)\leq \min(R(A), R(B))"> 知，<br> <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(67)" alt="[公式]" eeimg="1" data-formula=" R((m_i-m_j)(m_i-m_j)^T)=1 "> <br>又  <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(68)" alt="[公式]" eeimg="1" data-formula="\because "> <br> <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(69)" alt="[公式]" eeimg="1" data-formula="S_b=\sum_{i, j \atop i \neq j}[(m_i-m_j)(m_i-m_j)^T], \\R(A+B)\leq R(A)+R(B)  "> <br> <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(70)" alt="[公式]" eeimg="1" data-formula="  \therefore R(S_b)\leq K-1 \Rightarrow R(S_w^{-1}S_b)\leq K-1  "> <br> <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(71)" alt="[公式]" eeimg="1" data-formula=" \therefore \lambda =0 "> 对应的特征向量至少有 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(72)" alt="[公式]" eeimg="1" data-formula="(K-(K-1))=1"> 个，也就是说 d 最大为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(73)" alt="[公式]" eeimg="1" data-formula="K-1 "> . </blockquote><h3>3. LDA算法流程</h3><p>输入：数据集 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(74)" alt="[公式]" eeimg="1" data-formula=" D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)} "> ,其中，任意样本 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(75)" alt="[公式]" eeimg="1" data-formula=" x_i "> 为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(31)" alt="[公式]" eeimg="1" data-formula=" n "> 维向量， <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(76)" alt="[公式]" eeimg="1" data-formula=" y_i\in \{C_1,C_2,...,C_k\}"> ,降维到的维度为 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(77)" alt="[公式]" eeimg="1" data-formula="d "> .</p><p>输出：降维后的数据集 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(78)" alt="[公式]" eeimg="1" data-formula="D&#39;"> .</p><p>1）计算类内散度矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(79)" alt="[公式]" eeimg="1" data-formula="S_w "> </p><p>2）计算类间散度矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(57)" alt="[公式]" eeimg="1" data-formula="S_b"> </p><p>3）计算矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(61)" alt="[公式]" eeimg="1" data-formula=" S_w^{-1}S_b "> </p><p>4）计算矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(80)" alt="[公式]" eeimg="1" data-formula="S_w^{-1}S_b"> 的特征值与特征向量，按从小到大的顺序选取前 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(77)" alt="[公式]" eeimg="1" data-formula="d "> 个特征值和对应的 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(81)" alt="[公式]" eeimg="1" data-formula="d"> 个特征向量 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(62)" alt="[公式]" eeimg="1" data-formula=" (w_1,w_2,...,w_d) "> ，得到投影矩阵 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(82)" alt="[公式]" eeimg="1" data-formula=" W"> .</p><p>5）对样本集中的每一个样本特征 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(75)" alt="[公式]" eeimg="1" data-formula=" x_i "> ，转化为新的样本 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(83)" alt="[公式]" eeimg="1" data-formula=" z_i=W^Tx_i "> </p><p>6）得到输出样本集 <img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/equation(84)" alt="[公式]" eeimg="1" data-formula=" D&#39;=\{(z_1,y_1),(z_2,y_2),...,(z_m,y_m)\} "> </p><h3>4. LDA与PCA对比</h3><p>LDA与PCA都可用于降维，因此有很多相同的地方，也有很多不同的地方</p><p><b>相同点：</b></p><ul><li>  两者均可用于数据降维</li><li>  两者在降维时均使用了矩阵特征分解的思想</li><li>  两者都假设数据符合高斯分布</li></ul><p><b>不同点：</b></p><ul><li>  LDA是有监督的降维方法，而PCA是无监督降维方法</li><li>  当总共有K个类别时，LDA最多降到K-1维，而PCA没有这个限制</li><li>  LDA除了用于降维，还可以用于分类</li><li>  LCA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优（如下图的左图）。当然，某些数据分布下PCA比LDA降维较优（如下图的右图）。</li></ul><figure data-size="normal"><noscript><img src="https://pic1.zhimg.com/v2-8821ac9f8448b5bf6d39c46c2e94a708_b.jpg" data-caption="" data-size="normal" data-rawwidth="787" data-rawheight="315" class="origin_image zh-lightbox-thumb" width="787" data-original="https://pic1.zhimg.com/v2-8821ac9f8448b5bf6d39c46c2e94a708_r.jpg"/></noscript><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-8821ac9f8448b5bf6d39c46c2e94a708_hd.jpg" data-caption="" data-size="normal" data-rawwidth="787" data-rawheight="315" class="origin_image zh-lightbox-thumb lazy" width="787" data-original="https://pic1.zhimg.com/v2-8821ac9f8448b5bf6d39c46c2e94a708_r.jpg" data-actualsrc="https://pic1.zhimg.com/v2-8821ac9f8448b5bf6d39c46c2e94a708_b.jpg" data-lazy-status="ok"></figure><h3>5. LDA算法小结</h3><p>LDA算法既可以用来降维，也可以用来分来，但是目前来说，LDA主要用于降维，在进行与图像识别相关的数据分析时，LDA是一个有力的工具。下面总结一下LDA算法的优缺点。</p><p><b>LDA算法的优点</b></p><ul><li>  在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习无法使用先验知识；</li><li>  LDA在样本分类信息依赖均值而不是方差的时候，比PCA算法较优。</li></ul><p><b>LDA算法的缺点</b></p><ul><li>  LDA与PCA均不适合对非高斯分布样本进行降维</li><li>  LDA降维算法最多降到类别数K-1的维度，当降维的维度大于K-1时，则不能使用LDA。当然目前有一些改进的LDA算法可以绕过这个问题</li><li>  LDA在样本分类信息依赖方差而非均值的时候，降维效果不好</li><li>  LDA可能过度拟合数据</li></ul><blockquote>参考：<a href="https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6244265.html" class=" wrap external" target="_blank" rel="nofollow noreferrer" data-za-detail-view-id="1043">线性判别分析LDA原理总结 - 刘建平Pinard - 博客园</a></blockquote></div></div><div class="ContentItem-time">编辑于 2019-08-31</div><div class="Reward"><div><div class="Reward-tagline">「真诚赞赏，手留余香」</div><button class="Reward-rewardBtn">赞赏</button></div><div class="Reward-countZero">还没有人赞赏，快来当第一个赞赏的人吧！</div></div><div class="Post-topicsAndReviewer"><div class="TopicList Post-Topics"><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20010182&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20010182" target="_blank"><div class="Popover"><div id="Popover4-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover4-content">数据降维</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;20687563&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/20687563" target="_blank"><div class="Popover"><div id="Popover5-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover5-content">降维算法</div></div></a></span></div><div class="Tag Topic" data-za-detail-view-path-module="TopicItem" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Topic&quot;,&quot;token&quot;:&quot;19605693&quot;}}}"><span class="Tag-content"><a class="TopicLink" href="https://www.zhihu.com/topic/19605693" target="_blank"><div class="Popover"><div id="Popover6-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover6-content">分类算法</div></div></a></span></div></div></div><div><div class="Sticky RichContent-actions is-fixed is-bottom" style="width: 690px; bottom: 0px; left: 394.2px;"><div class="ContentItem-actions" data-za-detail-view-path-module="BottomBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;79696530&quot;}}}"><span><button aria-label="赞同 14" type="button" class="Button VoteButton VoteButton--up"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleUp VoteButton-TriangleUp" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></span>赞同 14</button><button aria-label="反对" type="button" class="Button VoteButton VoteButton--down"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--TriangleDown" fill="currentColor" viewBox="0 0 24 24" width="10" height="10"><path d="M20.044 3H3.956C2.876 3 2 3.517 2 4.9c0 .326.087.533.236.896L10.216 19c.355.571.87 1.143 1.784 1.143s1.429-.572 1.784-1.143l7.98-13.204c.149-.363.236-.57.236-.896 0-1.386-.876-1.9-1.956-1.9z" fill-rule="evenodd"></path></svg></span></button></span><button type="button" class="Button BottomActions-CommentBtn Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Comment Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M10.241 19.313a.97.97 0 0 0-.77.2 7.908 7.908 0 0 1-3.772 1.482.409.409 0 0 1-.38-.637 5.825 5.825 0 0 0 1.11-2.237.605.605 0 0 0-.227-.59A7.935 7.935 0 0 1 3 11.25C3 6.7 7.03 3 12 3s9 3.7 9 8.25-4.373 9.108-10.759 8.063z" fill-rule="evenodd"></path></svg></span>添加评论</button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover7-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover7-content"><button type="button" class="Button Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Share Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span>分享</button></div></div><button type="button" class="Button ContentItem-action Button--plain Button--withIcon Button--withLabel"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Star Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5.515 19.64l.918-5.355-3.89-3.792c-.926-.902-.639-1.784.64-1.97L8.56 7.74l2.404-4.871c.572-1.16 1.5-1.16 2.072 0L15.44 7.74l5.377.782c1.28.186 1.566 1.068.64 1.97l-3.89 3.793.918 5.354c.219 1.274-.532 1.82-1.676 1.218L12 18.33l-4.808 2.528c-1.145.602-1.896.056-1.677-1.218z" fill-rule="evenodd"></path></svg></span>收藏</button><div class="Post-ActionMenuButton"><div class="Popover"><div id="Popover8-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover8-content"><button type="button" class="Button Button--plain Button--withIcon Button--iconOnly"><span style="display:inline-flex;align-items:center">​<svg class="Zi Zi--Dots Button-zi" fill="currentColor" viewBox="0 0 24 24" width="1.2em" height="1.2em"><path d="M5 14a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4zm7 0a2 2 0 1 1 0-4 2 2 0 0 1 0 4z" fill-rule="evenodd"></path></svg></span></button></div></div></div></div><div class="Post-SideActions" data-za-detail-view-path-module="LeftTabBar" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:{&quot;type&quot;:&quot;Post&quot;,&quot;id&quot;:&quot;79696530&quot;}}}" style="opacity: 1;"><button class="like"><div class="Post-SideActions-icon"><svg class="Zi Zi--TriangleUp Post-SideActions-upIcon" fill="currentColor" viewBox="0 0 24 24" width="16" height="16"><path d="M2 18.242c0-.326.088-.532.237-.896l7.98-13.203C10.572 3.57 11.086 3 12 3c.915 0 1.429.571 1.784 1.143l7.98 13.203c.15.364.236.57.236.896 0 1.386-.875 1.9-1.955 1.9H3.955c-1.08 0-1.955-.517-1.955-1.9z" fill-rule="evenodd"></path></svg></div><div class="likeCount"><div class="likeCount-inner" data-previous="已赞同 15">赞同 14</div></div></button><div class="Popover ShareMenu"><div class="ShareMenu-toggler" id="Popover15-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover15-content"><img class="ShareMenu-fakeQRCode" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/qrcode" alt="微信二维码"><button><div class="Post-SideActions-icon"><span style="display: inline-flex; align-items: center;">​<svg class="Zi Zi--Share" fill="currentColor" viewBox="0 0 24 24" width="20" height="20"><path d="M2.931 7.89c-1.067.24-1.275 1.669-.318 2.207l5.277 2.908 8.168-4.776c.25-.127.477.198.273.39L9.05 14.66l.927 5.953c.18 1.084 1.593 1.376 2.182.456l9.644-15.242c.584-.892-.212-2.029-1.234-1.796L2.93 7.89z" fill-rule="evenodd"></path></svg></span></div>分享</button></div></div></div></div><div class="Sticky--holder" style="position: static; top: auto; right: auto; bottom: 0px; left: 0px; display: block; float: none; margin: 0px 0px 10px; height: 54px;"></div></div></article><div class="Post-Sub Post-NormalSub"><div class="PostIndex-Contributions" data-za-detail-view-path-module="ColumnList" data-za-detail-view-path-module_name="文章被以下专栏收录" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><h3 class="BlockTitle">文章被以下专栏收录</h3><ul><div class="ContentItem Column-ColumnItem"><div class="ContentItem-main"><div class="ContentItem-image"><a class="ColumnLink" href="https://zhuanlan.zhihu.com/chennan"><div class="Popover"><div id="Popover9-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover9-content"><img class="Avatar Avatar--medium Avatar--round" width="40" height="40" src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-dac72633cc016530b3d119b21c373542_xs.jpg" srcset="https://pic1.zhimg.com/v2-dac72633cc016530b3d119b21c373542_l.jpg 2x" alt="机器学习"></div></div></a></div><div class="ContentItem-head"><h2 class="ContentItem-title"><a class="ColumnLink ColumnItem-Title" href="https://zhuanlan.zhihu.com/chennan"><div class="Popover"><div id="Popover10-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover10-content">机器学习</div></div></a></h2><div class="ContentItem-meta">本专栏主要记录一些学习笔记，目前将持续更新AI圣经中的内容，欢迎大家关注，有问题一起探讨。</div></div><div class="ContentItem-extra"><button type="button" class="Button FollowButton Button--primary Button--blue">关注专栏</button></div></div></div></ul></div><div class="Recommendations-Main" style="width: 1478px;"><h3 class="BlockTitle Recommendations-BlockTitle">推荐阅读</h3><ul class="Recommendations-List"><button class="PagingButton PagingButton-Previous" disabled="" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><svg class="Zi Zi--ArrowLeft" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M14.782 16.78a.737.737 0 0 1-1.052 0L9.218 12.53a.758.758 0 0 1 0-1.063L13.73 7.22a.737.737 0 0 1 1.052 0c.29.294.29.77.001 1.063L11 12l3.782 3.716c.29.294.29.77 0 1.063z" fill-rule="evenodd"></path></svg></button><a href="https://zhuanlan.zhihu.com/p/29846048" class="PostItem"><div><h1 class="PostItem-Title">奇异值分解（SVD）</h1><p class="PostItem-Summary">以下内容来自刘建平Pinard-博客园的学习笔记，总结如下：奇异值分解(Singular Value Decomposition，以下简称SVD)是在机器学习领域广泛应用的算法，它不光可以用于降维算法中的特征分解，还…</p><div class="PostItem-Footer"><span>漫漫成长</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/51341313" class="PostItem"><div><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-0e7110f04b630696248a442fc1958ea1_250x0.jpg" srcset="https://pic4.zhimg.com/v2-0e7110f04b630696248a442fc1958ea1_qhd.jpg 2x" class="PostItem-TitleImage" alt="机器学习算法-PCA降维"><h1 class="PostItem-Title">机器学习算法-PCA降维</h1><div class="PostItem-Footer"><span>超爱学习</span><span class="PostItem-FooterTitle"></span></div></div></a><a href="https://zhuanlan.zhihu.com/p/51124752" class="PostItem"><div><img src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/v2-2963a4c3bdcc9e136c881d16bad41847_250x0.jpg" srcset="https://pic3.zhimg.com/v2-2963a4c3bdcc9e136c881d16bad41847_qhd.jpg 2x" class="PostItem-TitleImage" alt="降维算法原理篇：主成分分析PCA、奇异值分解SVD、因子分析法FA、独立成分分析ICA等原理详推"><h1 class="PostItem-Title">降维算法原理篇：主成分分析PCA、奇异值分解SVD、因子分析法FA、独立成分分析ICA等原理详推</h1><div class="PostItem-Footer"><span>过客</span><span class="PostItem-FooterTitle">发表于机器学习和...</span></div></div></a><a href="https://zhuanlan.zhihu.com/p/33185052" class="PostItem"><div><h1 class="PostItem-Title">LDA（线性判别分析）</h1><p class="PostItem-Summary">原文来自线性判别分析LDA原理总结 - 刘建平Pinard - 博客园，介绍了LDA线性判别分析算法。对经典的降维方法线性判别分析（Linear Discriminant Analysis, 以下简称LDA）做一个总结。LDA在模…</p><div class="PostItem-Footer"><span>漫漫成长</span><span class="PostItem-FooterTitle"></span></div></div></a><button class="PagingButton PagingButton-Next" data-za-detail-view-path-module="Unknown" data-za-detail-view-path-module_name="推荐阅读" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><svg class="Zi Zi--ArrowRight" fill="#d3d3d3" viewBox="0 0 24 24" width="40" height="40"><path d="M9.218 16.78a.737.737 0 0 0 1.052 0l4.512-4.249a.758.758 0 0 0 0-1.063L10.27 7.22a.737.737 0 0 0-1.052 0 .759.759 0 0 0-.001 1.063L13 12l-3.782 3.716a.758.758 0 0 0 0 1.063z" fill-rule="evenodd"></path></svg></button></ul></div><div class="Comments-container" data-za-detail-view-path-module="CommentList" data-za-extra-module="{&quot;card&quot;:{&quot;content&quot;:null}}"><div class="CommentsV2 CommentsV2--withEditor CommentsV2-withPagination"><div class="Topbar CommentTopbar"><div class="Topbar-title"><h2 class="CommentTopbar-title">还没有评论</h2></div><div class="Topbar-options"></div></div><div class="CommentsV2-footer CommentEditorV2--normal"><div class="CommentEditorV2-inputWrap"><div class="InputLike CommentEditorV2-input Editable"><div class="Dropzone Editable-content RichText RichText--editable RichText--clearBoth ztext" style="min-height: 198px;"><div class="DraftEditor-root"><div class="public-DraftEditorPlaceholder-root"><div class="public-DraftEditorPlaceholder-inner" id="placeholder-22d8v" style="white-space: pre-wrap;">写下你的评论...</div></div><div class="DraftEditor-editorContainer"><div aria-describedby="placeholder-22d8v" class="notranslate public-DraftEditor-content" contenteditable="true" role="textbox" spellcheck="true" tabindex="0" style="outline: none; user-select: text; white-space: pre-wrap; word-wrap: break-word;"><div data-contents="true"><div class="Editable-unstyled" data-block="true" data-editor="22d8v" data-offset-key="6a7qv-0-0"><div data-offset-key="6a7qv-0-0" class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span data-offset-key="6a7qv-0-0"><br data-text="true"></span></div></div></div></div></div></div></div><input multiple="" type="file" accept="image/jpg,image/jpeg,image/png,image/gif" style="display: none;"><div></div></div><div class="CommentEditorV2-inputUpload"><div class="CommentEditorV2-popoverWrap"><div class="Popover CommentEditorV2-inputUpLoad-Icon"><button aria-label="插入表情" data-tooltip="插入表情" data-tooltip-position="bottom" data-tooltip-will-hide-on-click="true" id="Popover11-toggle" aria-haspopup="true" aria-expanded="false" aria-owns="Popover11-content" type="button" class="Button Editable-control Button--plain"><svg class="Zi Zi--Emotion" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M7.523 13.5h8.954c-.228 2.47-2.145 4-4.477 4-2.332 0-4.25-1.53-4.477-4zM12 21a9 9 0 1 1 0-18 9 9 0 0 1 0 18zm0-1.5a7.5 7.5 0 1 0 0-15 7.5 7.5 0 0 0 0 15zm-3-8a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3zm6 0a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3z"></path></svg></button></div></div></div></div><button disabled="" type="button" class="Button CommentEditorV2-singleButton Button--primary Button--blue">发布</button></div><div><div class="CommentListV2"></div></div></div></div></div></div></main><div class="CornerButtons"><div class="CornerAnimayedFlex"><button data-tooltip="建议反馈" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="建议反馈" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--Feedback" title="建议反馈" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M19.99 6.99L18 5s-1-1-2-1H8C7 4 6 5 6 5L4 7S3 8 3 9v9s0 2 2.002 2H19c2 0 2-2 2-2V9c0-1-1.01-2.01-1.01-2.01zM16.5 5.5L19 8H5l2.5-2.5h9zm-2 5.5s.5 0 .5.5-.5.5-.5.5h-5s-.5 0-.5-.5.5-.5.5-.5h5z"></path></svg></button></div><div class="CornerAnimayedFlex"><button data-tooltip="回到顶部" data-tooltip-position="left" data-tooltip-will-hide-on-click="true" aria-label="回到顶部" type="button" class="Button CornerButton Button--plain"><svg class="Zi Zi--BackToTop" title="回到顶部" fill="currentColor" viewBox="0 0 24 24" width="24" height="24"><path d="M16.036 19.59a1 1 0 0 1-.997.995H9.032a.996.996 0 0 1-.997-.996v-7.005H5.03c-1.1 0-1.36-.633-.578-1.416L11.33 4.29a1.003 1.003 0 0 1 1.412 0l6.878 6.88c.782.78.523 1.415-.58 1.415h-3.004v7.005z"></path></svg></button></div></div></div></div><script id="js-clientConfig" type="text/json">{"host":"zhihu.com","protocol":"https:","wwwHost":"www.zhihu.com","zhuanlanHost":"zhuanlan.zhihu.com","apiHost":"api.zhihu.com"}</script><script id="js-initialData" type="text/json">{"initialState":{"common":{"ask":{}},"loading":{"global":{"count":0},"local":{"env\u002FgetIpinfo\u002F":false,"article\u002Fget\u002F":false,"brand\u002FgetUrl\u002F":false}},"entities":{"users":{"3c6b05b6fb1b1e26fdc0c495d2032cf5":{"uid":900994210918989800,"userType":"people","id":"3c6b05b6fb1b1e26fdc0c495d2032cf5"},"yao0118":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_{size}.jpg","uid":"758233788097523712","userType":"people","isFollowing":false,"urlToken":"yao0118","id":"f397cbba9f621e860ad11e100b77dc64","description":"机器学习，深度学习","name":"陈楠","isAdvertiser":false,"headline":"从事计算机视觉相关工作","gender":1,"url":"\u002Fpeople\u002Ff397cbba9f621e860ad11e100b77dc64","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"北京邮电大学 电子科学与技术硕士"}],"exposedMedal":{"medalId":"972477022068568064","medalName":"备受瞩目","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cb6e5c42f3c7428aa80cf44ff4868a67_r.png","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e8f194e35bb3a8bd187a957e8764d587_is.png","description":"被 1000 个人关注"}}},"questions":{},"answers":{},"articles":{"79696530":{"trackUrl":["https:\u002F\u002Fsugar.zhihu.com\u002Fplutus_adreaper\u002Fpage_monitor_log?si=__SESSIONID__&ti=__ATOKEN__&at=view&pf=__OS__&ed=BiBUKF0xBSkqGGNXAmN5CV5yCkMO-E7sIKsd&idfa=__IDFA__&imei=__IMEI__&androidid=__ANDROIDID__&oaid=__OAID__&ci=__CREATIVEID__"],"id":79696530,"title":"线性判别分析LDA原理及推导过程（非常详细）","type":"article","excerptTitle":"","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F79696530","imageUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6afb5e13569c05c07be1f17b3f573b32_b.jpg","titleImage":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-6afb5e13569c05c07be1f17b3f573b32_b.jpg","excerpt":"\u003Cimg src=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6afb5e13569c05c07be1f17b3f573b32_200x112.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"451\" data-watermark=\"watermark\" data-original-src=\"v2-6afb5e13569c05c07be1f17b3f573b32\" data-watermark-src=\"v2-3b91b15664720a0cb9198bca2c84bb3b\" data-private-watermark-src=\"\" class=\"origin_image inline-img zh-lightbox-thumb\" data-original=\"https:\u002F\u002Fpic3.zhimg.com\u002Fv2-6afb5e13569c05c07be1f17b3f573b32_r.png\"\u002F\u003E线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常…","created":1566717759,"updated":1567219415,"author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_{size}.jpg","uid":"758233788097523712","userType":"people","isFollowing":false,"urlToken":"yao0118","id":"f397cbba9f621e860ad11e100b77dc64","description":"机器学习，深度学习","name":"陈楠","isAdvertiser":false,"headline":"从事计算机视觉相关工作","gender":1,"url":"\u002Fpeople\u002Ff397cbba9f621e860ad11e100b77dc64","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_l.jpg","isOrg":false,"type":"people","badge":[{"type":"identity","topics":[],"description":"北京邮电大学 电子科学与技术硕士"}],"exposedMedal":{"medalId":"972477022068568064","medalName":"备受瞩目","avatarUrl":"https:\u002F\u002Fpic2.zhimg.com\u002Fv2-cb6e5c42f3c7428aa80cf44ff4868a67_r.png","miniAvatarUrl":"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-e8f194e35bb3a8bd187a957e8764d587_is.png","description":"被 1000 个人关注"}},"commentPermission":"all","state":"published","imageWidth":1308,"imageHeight":451,"content":"\u003Cp\u003E线性判别分析LDA(Linear Discriminant Analysis)又称为Fisher线性判别，是一种监督学习的降维技术，也就是说它的数据集的每个样本都是有类别输出的，这点与PCA（无监督学习）不同。LDA在模式识别领域（比如人脸识别，舰艇识别等图形图像识别领域）中有非常广泛的应用，因此我们有必要了解下它的算法原理。\u003C\u002Fp\u003E\u003Ch3\u003E1. LDA的思想\u003C\u002Fh3\u003E\u003Cp\u003ELDA的思想是：\u003Cb\u003E最大化类间均值，最小化类内方差\u003C\u002Fb\u003E。意思就是将数据投影在低维度上，并且投影后同种类别数据的投影点尽可能的接近，不同类别数据的投影点的中心点尽可能的远。\u003C\u002Fp\u003E\u003Cp\u003E我们先看看最简单的情况。假设我们有两类数据 分别为红色和蓝色，如下图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。\u003C\u002Fp\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3b91b15664720a0cb9198bca2c84bb3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"1308\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3b91b15664720a0cb9198bca2c84bb3b_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;1308&#39; height=&#39;451&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1308\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1308\" data-original=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3b91b15664720a0cb9198bca2c84bb3b_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic4.zhimg.com\u002Fv2-3b91b15664720a0cb9198bca2c84bb3b_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Cp\u003E上图提供了两种投影方式，哪一种能更好的满足我们的标准呢？从直观上可以看出，右图要比左图的投影效果好，因为右图的黑色数据和蓝色数据各个较为集中，且类别之间的距离明显。左图则在边界处数据混杂。以上就是LDA的主要思想了，当然在实际应用中，我们的数据是多个类别的，我们的原始数据一般也是超过二维的，投影后的也一般不是直线，而是一个低维的超平面。\u003C\u002Fp\u003E\u003Ch3\u003E2. 瑞利商(Rayleigh quotient)与广义瑞利商(genralized Rayleigh quotient)\u003C\u002Fh3\u003E\u003Cp\u003E首先来看瑞利商的定义。瑞利商是指这样的函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28A%2Cx%29+\" alt=\"R(A,x) \" eeimg=\"1\"\u002F\u003E :\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28A%2C+x%29%3D%5Cfrac%7Bx%5E%7BH%7D+A+x%7D%7Bx%5E%7BH%7D+x%7D\" alt=\"R(A, x)=\\frac{x^{H} A x}{x^{H} x}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x+\" alt=\" x \" eeimg=\"1\"\u002F\u003E 是非零向量，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A+\" alt=\" A \" eeimg=\"1\"\u002F\u003E 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+n%5Ctimes+n+\" alt=\" n\\times n \" eeimg=\"1\"\u002F\u003E 的Hermitan矩阵。所谓的Hermitan矩阵就是满足\u003Cb\u003E共轭转置矩阵\u003C\u002Fb\u003E和自己相等的矩阵，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A%5EH%3DA+\" alt=\" A^H=A \" eeimg=\"1\"\u002F\u003E ，例如， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A%3D%5Cleft%28%5Cbegin%7Barray%7D%7Bcc%7D%7B1%7D+%26+%7B2%2Bi%7D+%5C%5C+%7B2-i%7D+%26+%7B1%7D%5Cend%7Barray%7D%5Cright%29+\" alt=\" A=\\left(\\begin{array}{cc}{1} &amp; {2+i} \\\\ {2-i} &amp; {1}\\end{array}\\right) \" eeimg=\"1\"\u002F\u003E 的共轭转置等于其本身。当 A 为实矩阵时，如果满足 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A%5ET%3DA+\" alt=\" A^T=A \" eeimg=\"1\"\u002F\u003E ，则 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A+\" alt=\" A \" eeimg=\"1\"\u002F\u003E 为Hermitan矩阵。\u003C\u002Fp\u003E\u003Cp\u003E瑞利商 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28A%2Cx%29\" alt=\"R(A,x)\" eeimg=\"1\"\u002F\u003E 有一个非常重要的性质，即\u003Cb\u003E它的最大值等于矩阵 A 最大的特征值，而最小值等于矩阵 A 的最小的特征值\u003C\u002Fb\u003E，也就是满足  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Clambda_%7B%5Cmin+%7D+%5Cleq+%5Cfrac%7Bx%5E%7BH%7D+A+x%7D%7Bx%5E%7BH%7D+x%7D+%5Cleq+%5Clambda_%7B%5Cmax+%7D\" alt=\"\\lambda_{\\min } \\leq \\frac{x^{H} A x}{x^{H} x} \\leq \\lambda_{\\max }\" eeimg=\"1\"\u002F\u003E\u003C\u002Fb\u003E \u003C\u002Fp\u003E\u003Cp\u003E当向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x+\" alt=\" x \" eeimg=\"1\"\u002F\u003E 是标准正交基，即满足 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x%5E%7BH%7D+x%3D1+\" alt=\" x^{H} x=1 \" eeimg=\"1\"\u002F\u003E 时，瑞利商退化为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28A%2C+x%29%3Dx%5E%7BH%7D+A+x\" alt=\" R(A, x)=x^{H} A x\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cp\u003E下面看一下广义瑞利商。广义瑞利商是指这样的函数 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28A%2CB%2Cx%29+\" alt=\" R(A,B,x) \" eeimg=\"1\"\u002F\u003E :  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28A%2CB%2C+x%29%3D%5Cfrac%7Bx%5E%7BH%7D+A+x%7D%7Bx%5E%7BH%7D+B+x%7D\" alt=\"R(A,B, x)=\\frac{x^{H} A x}{x^{H} B x}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x\" alt=\"x\" eeimg=\"1\"\u002F\u003E 为非零向量，而 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+A%2CB+\" alt=\" A,B \" eeimg=\"1\"\u002F\u003E 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+n%5Ctimes+n+\" alt=\" n\\times n \" eeimg=\"1\"\u002F\u003E 的Hermitan矩阵。 B 为正定矩阵。它的最大值和最小值是什么呢？其实我们只要通过将其通过标准化就可以转化为瑞利商的格式。令 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x%3DB%5E%7B-1+%2F+2%7D+x%5E%7B%5Cprime%7D\" alt=\" x=B^{-1 \u002F 2} x^{\\prime}\" eeimg=\"1\"\u002F\u003E ，则分母转化为：  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x%5E%7BH%7D+B+x%3Dx%5E%7B%5Cprime+H%7D%5Cleft%28B%5E%7B-1+%2F+2%7D%5Cright%29%5E%7BH%7D+B+B%5E%7B-1+%2F+2%7D+x%5E%7B%5Cprime%7D%3Dx%5E%7B%5Cprime+H%7D+B%5E%7B-1+%2F+2%7D+B+B%5E%7B-1+%2F+2%7D+x%5E%7B%5Cprime%7D%3Dx%5E%7B%5Cprime+H%7D+x%5E%7B%5Cprime%7D\" alt=\"x^{H} B x=x^{\\prime H}\\left(B^{-1 \u002F 2}\\right)^{H} B B^{-1 \u002F 2} x^{\\prime}=x^{\\prime H} B^{-1 \u002F 2} B B^{-1 \u002F 2} x^{\\prime}=x^{\\prime H} x^{\\prime}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E而分子转化为：  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=x%5E%7BH%7D+A+x%3Dx%5E%7B%5Cprime+H%7D+B%5E%7B-1+%2F+2%7D+A+B%5E%7B-1+%2F+2%7D+x%5E%7B%5Cprime%7D\" alt=\"x^{H} A x=x^{\\prime H} B^{-1 \u002F 2} A B^{-1 \u002F 2} x^{\\prime}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E此时我们的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28A%2CB%2Cx%29+\" alt=\" R(A,B,x) \" eeimg=\"1\"\u002F\u003E 转化为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28A%2CB%2Cx%27%29\" alt=\" R(A,B,x&#39;)\" eeimg=\"1\"\u002F\u003E :\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+++R%5Cleft%28A%2C+B%2C+x%5E%7B%5Cprime%7D%5Cright%29%3D%5Cfrac%7Bx%5E%7B%5Cprime+H%7D+B%5E%7B-1+%2F+2%7D+A+B%5E%7B-1+%2F+2%7D+x%5E%7B%5Cprime%7D%7D%7Bx%5E%7B%5Cprime+H%7D+x%5E%7B%5Cprime%7D%7D\" alt=\"   R\\left(A, B, x^{\\prime}\\right)=\\frac{x^{\\prime H} B^{-1 \u002F 2} A B^{-1 \u002F 2} x^{\\prime}}{x^{\\prime H} x^{\\prime}}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E利用前面的瑞利商的性质，我们可以很快的知道， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28A%2CB%2Cx%27%29+\" alt=\" R(A,B,x&#39;) \" eeimg=\"1\"\u002F\u003E 的最大值为矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+B%5E%7B-1+%2F+2%7D+A+B%5E%7B-1+%2F+2%7D+\" alt=\" B^{-1 \u002F 2} A B^{-1 \u002F 2} \" eeimg=\"1\"\u002F\u003E 的最大特征值，或者说矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+B%5E%7B%E2%88%921%7DA\" alt=\" B^{−1}A\" eeimg=\"1\"\u002F\u003E 的最大特征值，而最小值为矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=B%5E%7B%E2%88%921%7DA+\" alt=\"B^{−1}A \" eeimg=\"1\"\u002F\u003E 的最小特征值。\u003C\u002Fp\u003E\u003Ch3\u003E2. LDA的原理及推导过程\u003C\u002Fh3\u003E\u003Cp\u003E假设样本共有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 类，每一个类的样本的个数分别为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=N_1%2CN_2%2C...%2CN_k+\" alt=\"N_1,N_2,...,N_k \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_1%5E1%2Cx_1%5E2%2C...%2Cx_1%5E%7BN_1%7D+\" alt=\" x_1^1,x_1^2,...,x_1^{N_1} \" eeimg=\"1\"\u002F\u003E 对应第1类\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_2%5E1%2Cx_2%5E2%2C...%2Cx_2%5E%7BN_2%7D+\" alt=\" x_2^1,x_2^2,...,x_2^{N_2} \" eeimg=\"1\"\u002F\u003E 对应第2类\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_k%5E1%2Cx_k%5E2%2C...%2Cx_k%5E%7BN_k%7D\" alt=\" x_k^1,x_k^2,...,x_k^{N_k}\" eeimg=\"1\"\u002F\u003E 对应第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 类，其中每个样本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_i%5Ej+\" alt=\" x_i^j \" eeimg=\"1\"\u002F\u003E 均为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+n+\" alt=\" n \" eeimg=\"1\"\u002F\u003E 维向量 \u003C\u002Fp\u003E\u003Cp\u003E设 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Ctilde%7Bx%7D_%7Bi%7D%5E%7Bj%7D+%E4%B8%BA+x_i%5Ej+\" alt=\" \\tilde{x}_{i}^{j} 为 x_i^j \" eeimg=\"1\"\u002F\u003E 变化后的样本，则 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7Bx%7D_%7Bi%7D%5E%7Bj%7D%3D%3Cx%2C+u%3Eu%3D%7Cx%7C%7Cu%7Ccos%5Ctheta+%5Ccdot+u+%3D%5Cleft%28x%5E%7BT%7D+u%5Cright%29+u\" alt=\"\\tilde{x}_{i}^{j}=&lt;x, u&gt;u=|x||u|cos\\theta \\cdot u =\\left(x^{T} u\\right) u\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E此处设 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+u+\" alt=\" u \" eeimg=\"1\"\u002F\u003E 为单位向量，即 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+u%5ETu%3D1\" alt=\" u^Tu=1\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E假设第K类样本的数据集为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+D_k+\" alt=\" D_k \" eeimg=\"1\"\u002F\u003E ，变化后的样本的均值向量为： \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Ctilde%7B%5Cmathrm%7Bm%7D%7D%3D%5Cfrac%7B%5Csum_%7B%5Ctilde%7Bx%7D%5Cin+D_k%7D%5Ctilde%7Bx%7D%7D%7BN_k%7D+\" alt=\"\\tilde{\\mathrm{m}}=\\frac{\\sum_{\\tilde{x}\\in D_k}\\tilde{x}}{N_k} \" eeimg=\"1\"\u002F\u003E ，那么，第K类样本的方差为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cfrac%7BS_k%7D%7BN_k%7D+\" alt=\" \\frac{S_k}{N_k} \" eeimg=\"1\"\u002F\u003E ，其中：   \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%5Cmathrm%7BS%7D_%7B%5Cmathrm%7Bk%7D%7D+%26%3D%5Csum_%7B%5Ctilde%7Bx%7D+%5Cin+D_%7Bk%7D%7D%28%5Ctilde%7Bx%7D-%5Ctilde%7B%5Cmathrm%7Bm%7D%7D%29%5E%7BT%7D%28%5Ctilde%7Bx%7D-%5Ctilde%7B%5Cmathrm%7Bm%7D%7D%29%5C%5C++%26%3D%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29+u-%5Cleft%28m%5E%7BT%7D+u%5Cright%29+u%5Cright%5D%5ET%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29+u+-%5Cleft%28m%5E%7BT%7D+u%5Cright%29+u%5Cright%5D+%5C%5C+%26%3D%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29+u%5E%7BT%7D-%5Cleft%28m%5E%7BT%7D+u%5Cright%29+u%5E%7BT%7D%5Cright%5D%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29+u+-%5Cleft%28m%5E%7BT%7D+u%5Cright%29+u%5Cright%5D++%5Cquad+%28x%5ETu%EF%BC%8Cm%5ETu%E4%B8%BA%E5%AE%9E%E6%95%B0%2C%E8%BD%AC%E7%BD%AE%E4%BB%8D%E6%98%AF%E6%9C%AC%E8%BA%AB%29%5C%5C++%26%3D%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29%5E%7B2%7D+u%5E%7BT%7D+u-2%5Cleft%28x%5E%7BT%7D+u%5Cright%29%5Cleft%28m%5E%7BT%7D+u%5Cright%29+u%5E%7BT%7D+u%2B%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D+u%5E%7BT%7D+u%5Cright%5D+%5C%5C++%26%3D%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D%5Cleft%5B%5Cleft%28x%5E%7BT%7D+u%5Cright%29%5E%7B2%7D-2%5Cleft%28x%5E%7BT%7D+u%5Cright%29%5Cleft%28m%5E%7BT%7D+u%5Cright%29%2B%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D%5Cright%5D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} \\mathrm{S}_{\\mathrm{k}} &amp;=\\sum_{\\tilde{x} \\in D_{k}}(\\tilde{x}-\\tilde{\\mathrm{m}})^{T}(\\tilde{x}-\\tilde{\\mathrm{m}})\\\\  &amp;=\\sum_{x \\in D_{k}}\\left[\\left(x^{T} u\\right) u-\\left(m^{T} u\\right) u\\right]^T\\left[\\left(x^{T} u\\right) u -\\left(m^{T} u\\right) u\\right] \\\\ &amp;=\\sum_{x \\in D_{k}}\\left[\\left(x^{T} u\\right) u^{T}-\\left(m^{T} u\\right) u^{T}\\right]\\left[\\left(x^{T} u\\right) u -\\left(m^{T} u\\right) u\\right]  \\quad (x^Tu，m^Tu为实数,转置仍是本身)\\\\  &amp;=\\sum_{x \\in D_{k}}\\left[\\left(x^{T} u\\right)^{2} u^{T} u-2\\left(x^{T} u\\right)\\left(m^{T} u\\right) u^{T} u+\\left(m^{T} u\\right)^{2} u^{T} u\\right] \\\\  &amp;=\\sum_{x \\in D_{k}}\\left[\\left(x^{T} u\\right)^{2}-2\\left(x^{T} u\\right)\\left(m^{T} u\\right)+\\left(m^{T} u\\right)^{2}\\right] \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E第 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K\" alt=\"K\" eeimg=\"1\"\u002F\u003E 类样本的方差：   \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%5Cfrac%7BS_%7Bk%7D%7D%7BN_%7Bk%7D%7D+%26%3D%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D%5Cleft%28x%5E%7BT%7D+u%5Cright%29%5E%7B2%7D%7D%7BN_%7Bk%7D%7D-2+%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_%7Bk%7D%7D+x%5E%7BT%7D%5Cleft%28u+m%5E%7BT%7D+u%5Cright%29%7D%7BN_%7Bk%7D%7D%2B%5Cfrac%7B%5Csum_%7Bx%5Cin+D_%7Bk%7D%7D%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D%7D%7BN_%7Bk%7D%7D+%5C%5C++%26%3D%5Cfrac%7B%5Csum_%7Bx%5Cin+D_k%7D+u%5E%7BT%7D+x+x%5E%7BT%7D+u%7D%7BN_%7Bk%7D%7D-2+%5Cfrac%7B%5Csum_%7Bx%5Cin+D_k%7D+x%5E%7BT%7D%7D%7BN_%7Bk%7D%7D+u+m%5E%7BT%7D+u%2B%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D%5Cquad+%28%E6%B3%A8%EF%BC%9A%5Cfrac%7B%5Csum_%7Bx%5Cin+D_%7Bk%7D%7D%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D%7D%7BN_%7Bk%7D%7D%3D%28m%5ETu%29%5E2%E3%80%82%E5%9B%A0%E4%B8%BAm%5ETu%E4%B8%8Ex%E6%97%A0%E5%85%B3%29%5C%5C++%26%3Du%5E%7BT%7D+%5Cfrac%7B%5Csum_%7Bx%5Cin+D_k%7D+x+x%5E%7BT%7D%7D%7BN_%7Bk%7D%7D+u-%5Cleft%28m%5E%7BT%7D+u%5Cright%29%5E%7B2%7D+%5Cquad+%28%E6%B3%A8%EF%BC%9A%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_k%7Dx%5ET%7D%7BN_k%7D%3Dm%5ET%29%5C%5C++%26%3Du%5E%7BT%7D+%5Cfrac%7B%5Csum_%7Bx%5Cin+D_k%7D+x+x%5E%7BT%7D%7D%7BN_%7Bk%7D%7D+u-u%5E%7BT%7D+m+m%5E%7BT%7D+u+%5C%5C++%26%3Du%5E%7BT%7D%5Cleft%28%5Cfrac%7B%5Csum_%7Bx%5Cin+D_k%7D+x+x%5E%7BT%7D%7D%7BN_%7Bk%7D%7D-m+m%5E%7BT%7D%5Cright%29+u+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} \\frac{S_{k}}{N_{k}} &amp;=\\frac{\\sum_{x \\in D_{k}}\\left(x^{T} u\\right)^{2}}{N_{k}}-2 \\frac{\\sum_{x \\in D_{k}} x^{T}\\left(u m^{T} u\\right)}{N_{k}}+\\frac{\\sum_{x\\in D_{k}}\\left(m^{T} u\\right)^{2}}{N_{k}} \\\\  &amp;=\\frac{\\sum_{x\\in D_k} u^{T} x x^{T} u}{N_{k}}-2 \\frac{\\sum_{x\\in D_k} x^{T}}{N_{k}} u m^{T} u+\\left(m^{T} u\\right)^{2}\\quad (注：\\frac{\\sum_{x\\in D_{k}}\\left(m^{T} u\\right)^{2}}{N_{k}}=(m^Tu)^2。因为m^Tu与x无关)\\\\  &amp;=u^{T} \\frac{\\sum_{x\\in D_k} x x^{T}}{N_{k}} u-\\left(m^{T} u\\right)^{2} \\quad (注：\\frac{\\sum_{x \\in D_k}x^T}{N_k}=m^T)\\\\  &amp;=u^{T} \\frac{\\sum_{x\\in D_k} x x^{T}}{N_{k}} u-u^{T} m m^{T} u \\\\  &amp;=u^{T}\\left(\\frac{\\sum_{x\\in D_k} x x^{T}}{N_{k}}-m m^{T}\\right) u \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E各个类别的样本方差之和：  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+%5Csum_%7Bk%7D%5Cfrac%7BS_k%7D%7BN_k%7D+%26%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7Du%5E%7BT%7D%5Cleft%28%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_k%7D+x+x%5E%7BT%7D%7D%7BN_%7Bk%7D%7D-m_k+m_k%5E%7BT%7D%5Cright%29+u+%5C%5C+%26%3Du%5ET%5Csum_%7Bk%3D1%7D%5E%7BK%7D%28%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_k%7Dxx%5ET%7D%7BN_k%7D-m_km_k%5ET%29u%5C%5C%26%3Du%5ETS_wu+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} \\sum_{k}\\frac{S_k}{N_k} &amp;=\\sum_{k=1}^{K}u^{T}\\left(\\frac{\\sum_{x \\in D_k} x x^{T}}{N_{k}}-m_k m_k^{T}\\right) u \\\\ &amp;=u^T\\sum_{k=1}^{K}(\\frac{\\sum_{x \\in D_k}xx^T}{N_k}-m_km_k^T)u\\\\&amp;=u^TS_wu \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_w%3D%5Csum_%7Bk%3D1%7D%5E%7BK%7D%28%5Cfrac%7B%5Csum_%7Bx+%5Cin+D_k%7Dxx%5ET%7D%7BN_k%7D-m_km_k%5ET%29\" alt=\"S_w=\\sum_{k=1}^{K}(\\frac{\\sum_{x \\in D_k}xx^T}{N_k}-m_km_k^T)\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_w+\" alt=\" S_w \" eeimg=\"1\"\u002F\u003E 一般被称为类内散度矩阵\u003C\u002Fp\u003E\u003Cp\u003E不同类别 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+i%2Cj+\" alt=\" i,j \" eeimg=\"1\"\u002F\u003E 之间的中心距离：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cbegin%7Baligned%7D+S_%7Bi+j%7D+%26%3D%5Cleft%28%5Ctilde%7Bm%7D_%7Bi%7D-%5Ctilde%7Bm%7D_%7Bj%7D%5Cright%29%5E%7BT%7D%5Cleft%28%5Ctilde%7Bm%7D_%7Bi%7D-%5Ctilde%7Bm%7D_%7Bj%7D%5Cright%29+%5C%5C+%26%3D%5B%28u%5ETm_i%29u-%28u%5ETm_j%29u%5D%5ET%5B%28m_i%5ETu%29u-%28m_j%5ETu%29u%5D+%5C%5C+%26%3D%5B%28u%5ETm_i%29u%5ET-%28u%5ETm_j%29u%5ET%5D%5Bu%28m_i%5ETu%29-u%28m_j%5ETu%29%5D+%5C%5C+%26%3Du%5ET%28m_i-m_j%29u%5ETu%28m_i%5ET-m_j%5ET%29u+%5C%5C+%26%3Du%5E%7BT%7D%5Cleft%28m_%7Bi%7D-m_%7Bj%7D%5Cright%29%5Cleft%28m_%7Bi%7D-m_%7Bj%7D%5Cright%29%5E%7BT%7D+u+%5Cend%7Baligned%7D+++\" alt=\" \\begin{aligned} S_{i j} &amp;=\\left(\\tilde{m}_{i}-\\tilde{m}_{j}\\right)^{T}\\left(\\tilde{m}_{i}-\\tilde{m}_{j}\\right) \\\\ &amp;=[(u^Tm_i)u-(u^Tm_j)u]^T[(m_i^Tu)u-(m_j^Tu)u] \\\\ &amp;=[(u^Tm_i)u^T-(u^Tm_j)u^T][u(m_i^Tu)-u(m_j^Tu)] \\\\ &amp;=u^T(m_i-m_j)u^Tu(m_i^T-m_j^T)u \\\\ &amp;=u^{T}\\left(m_{i}-m_{j}\\right)\\left(m_{i}-m_{j}\\right)^{T} u \\end{aligned}   \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E所有类别之间的距离之和为：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+++%5Cbegin%7Baligned%7D+++++%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D+s_%7Bi+j%7D+%26%3D+u%5ET%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5Du+%5C%5C+++++%26%3Du%5ETS_bu+%5Cend%7Baligned%7D\" alt=\"   \\begin{aligned}     \\sum_{i, j \\atop i \\neq j} s_{i j} &amp;= u^T\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T]u \\\\     &amp;=u^TS_bu \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E其中， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_b%3D%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5D\" alt=\"S_b=\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T]\" eeimg=\"1\"\u002F\u003E ， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_b+\" alt=\"S_b \" eeimg=\"1\"\u002F\u003E 一般被称为类间散度矩阵。\u003C\u002Fp\u003E\u003Cp\u003E在已知条件下， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_w%2CS_b+\" alt=\" S_w,S_b \" eeimg=\"1\"\u002F\u003E 均可求出，LDA算法的目标是\u003Cb\u003E“类间距离尽可能大，类内方差尽可能小”\u003C\u002Fb\u003E，即最大化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+u%5ETS_bu+\" alt=\" u^TS_bu \" eeimg=\"1\"\u002F\u003E ,最小化 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=u%5ETS_wu\" alt=\"u^TS_wu\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E令 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+J%28u%29%3D%5Cfrac%7Bu%5E%7BT%7D+S_%7Bb%7D+u%7D%7Bu%5E%7BT%7D+S_%7Bw%7D+u%7D+\" alt=\" J(u)=\\frac{u^{T} S_{b} u}{u^{T} S_{w} u} \" eeimg=\"1\"\u002F\u003E ,则目标函数为：  \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cmax+J%28u%29\" alt=\"\\max J(u)\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E为了使所求最大，可假设 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+u%5ETS_wu%3D1\" alt=\" u^TS_wu=1\" eeimg=\"1\"\u002F\u003E ，则问题转化为：\u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Cbegin%7Barray%7D%7Bl%7D%7B%5Cmax+u%5E%7BT%7D+S_%7Bb%7D+u%7D+%5C%5C+s.t.+%5Cquad+u%5ETS_wu%3D1++%5Cend%7Barray%7D+++\" alt=\" \\begin{array}{l}{\\max u^{T} S_{b} u} \\\\ s.t. \\quad u^TS_wu=1  \\end{array}   \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D++L%28u%2C+%5Clambda%29%26%3Du%5E%7BT%7D+S_%7Bb%7D+u%2B%5Clambda%5Cleft%281-u%5E%7BT%7D+S_%7Bw%7D+u%5Cright%29+%5C%5C++%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+u%7D+%26%3D+S_bu%2BS_b%5ETu-%5Clambda+S_wu-%5Clambda+S_w%5ETu+%5C%5C+%26%3D2%28S_bu-%5Clambda+S_wu%29%5Cquad+%28%E5%9B%A0%E4%B8%BAS_b%EF%BC%8CS_w%E4%B8%BA%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%29%5C%5C+%26%3D0+%5CRightarrow+S_%7Bb%7D+u%3D%5Clambda+S_%7Bw%7D+u+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned}  L(u, \\lambda)&amp;=u^{T} S_{b} u+\\lambda\\left(1-u^{T} S_{w} u\\right) \\\\  \\frac{\\partial L}{\\partial u} &amp;= S_bu+S_b^Tu-\\lambda S_wu-\\lambda S_w^Tu \\\\ &amp;=2(S_bu-\\lambda S_wu)\\quad (因为S_b，S_w为对称矩阵)\\\\ &amp;=0 \\Rightarrow S_{b} u=\\lambda S_{w} u \\end{aligned}\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cblockquote\u003E证明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_b\" alt=\"S_b\" eeimg=\"1\"\u002F\u003E 为对称矩阵：  \u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Baligned%7D+++++S_b%5ET+%26%3D+%28%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5D%29%5ET%5C%5C+++++%26%3D%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5D%5ET+%5C%5C+++++%26%3D%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5D+%5C%5C+++++%26%3DS_b+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned}     S_b^T &amp;= (\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T])^T\\\\     &amp;=\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T]^T \\\\     &amp;=\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T] \\\\     &amp;=S_b \\end{aligned}\" eeimg=\"1\"\u002F\u003E\u003Cbr\u002F\u003E所以， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_b+\" alt=\" S_b \" eeimg=\"1\"\u002F\u003E 为对称矩阵，同理可证明 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_w+\" alt=\" S_w \" eeimg=\"1\"\u002F\u003E 为对称矩阵。\u003C\u002Fblockquote\u003E\u003Cp\u003E\u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbegin%7Barray%7D%7Bl%7D+S_%7Bw%7D%5E%7B-1%7D%7BS_%7Bb%7D++u%3D%5Clambda+u%7D+%5Cend%7Barray%7D+++\" alt=\"\\begin{array}{l} S_{w}^{-1}{S_{b}  u=\\lambda u} \\end{array}   \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E计算矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_w%5E%7B-1%7DS_b+\" alt=\" S_w^{-1}S_b \" eeimg=\"1\"\u002F\u003E 的最大的 d 个特征值和对应的 d 个特征向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%28w_1%2Cw_2%2C...%2Cw_d%29+\" alt=\" (w_1,w_2,...,w_d) \" eeimg=\"1\"\u002F\u003E ，得到投影矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=W%3D%28w_1%2Cw_2%2C...%2Cw_d%29\" alt=\"W=(w_1,w_2,...,w_d)\" eeimg=\"1\"\u002F\u003E 。\u003C\u002Fp\u003E\u003Cblockquote\u003E注意: (1)选取特征值时，如果一些特征值明显大于其他的特征值，则取这些取值较大的特征值，因为它们包含更多的数据分布的信息。相反，如果一些特征值接近于0，我们将这些特征值舍去。\u003Cbr\u002F\u003E(2)由于 W 是一个利用了样本类别得到的投影矩阵，因此它能够降维到的维度d的最大值为 K-1 。为什么不是 K 呢？因为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_b+\" alt=\" S_b \" eeimg=\"1\"\u002F\u003E 中每个 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+m_i-m_j+\" alt=\" m_i-m_j \" eeimg=\"1\"\u002F\u003E 的秩均为1(因为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+m_i+\" alt=\" m_i \" eeimg=\"1\"\u002F\u003E 为1维向量).\u003Cbr\u002F\u003E由 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=R%28AB%29%5Cleq+%5Cmin%28R%28A%29%2C+R%28B%29%29\" alt=\"R(AB)\\leq \\min(R(A), R(B))\" eeimg=\"1\"\u002F\u003E 知，\u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+R%28%28m_i-m_j%29%28m_i-m_j%29%5ET%29%3D1+\" alt=\" R((m_i-m_j)(m_i-m_j)^T)=1 \" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E又  \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%5Cbecause+\" alt=\"\\because \" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_b%3D%5Csum_%7Bi%2C+j+%5Catop+i+%5Cneq+j%7D%5B%28m_i-m_j%29%28m_i-m_j%29%5ET%5D%2C+%5C%5CR%28A%2BB%29%5Cleq+R%28A%29%2BR%28B%29++\" alt=\"S_b=\\sum_{i, j \\atop i \\neq j}[(m_i-m_j)(m_i-m_j)^T], \\\\R(A+B)\\leq R(A)+R(B)  \" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=++%5Ctherefore+R%28S_b%29%5Cleq+K-1+%5CRightarrow+R%28S_w%5E%7B-1%7DS_b%29%5Cleq+K-1++\" alt=\"  \\therefore R(S_b)\\leq K-1 \\Rightarrow R(S_w^{-1}S_b)\\leq K-1  \" eeimg=\"1\"\u002F\u003E \u003Cbr\u002F\u003E \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%5Ctherefore+%5Clambda+%3D0+\" alt=\" \\therefore \\lambda =0 \" eeimg=\"1\"\u002F\u003E 对应的特征向量至少有 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=%28K-%28K-1%29%29%3D1\" alt=\"(K-(K-1))=1\" eeimg=\"1\"\u002F\u003E 个，也就是说 d 最大为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=K-1+\" alt=\"K-1 \" eeimg=\"1\"\u002F\u003E . \u003C\u002Fblockquote\u003E\u003Ch3\u003E3. LDA算法流程\u003C\u002Fh3\u003E\u003Cp\u003E输入：数据集 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+D%3D%7B%28x_1%2Cy_1%29%2C%28x_2%2Cy_2%29%2C...%2C%28x_m%2Cy_m%29%7D+\" alt=\" D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)} \" eeimg=\"1\"\u002F\u003E ,其中，任意样本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_i+\" alt=\" x_i \" eeimg=\"1\"\u002F\u003E 为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+n+\" alt=\" n \" eeimg=\"1\"\u002F\u003E 维向量， \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+y_i%5Cin+%5C%7BC_1%2CC_2%2C...%2CC_k%5C%7D\" alt=\" y_i\\in \\{C_1,C_2,...,C_k\\}\" eeimg=\"1\"\u002F\u003E ,降维到的维度为 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d+\" alt=\"d \" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E输出：降维后的数据集 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=D%27\" alt=\"D&#39;\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E1）计算类内散度矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_w+\" alt=\"S_w \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E2）计算类间散度矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_b\" alt=\"S_b\" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E3）计算矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+S_w%5E%7B-1%7DS_b+\" alt=\" S_w^{-1}S_b \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E4）计算矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=S_w%5E%7B-1%7DS_b\" alt=\"S_w^{-1}S_b\" eeimg=\"1\"\u002F\u003E 的特征值与特征向量，按从小到大的顺序选取前 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d+\" alt=\"d \" eeimg=\"1\"\u002F\u003E 个特征值和对应的 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=d\" alt=\"d\" eeimg=\"1\"\u002F\u003E 个特征向量 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+%28w_1%2Cw_2%2C...%2Cw_d%29+\" alt=\" (w_1,w_2,...,w_d) \" eeimg=\"1\"\u002F\u003E ，得到投影矩阵 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+W\" alt=\" W\" eeimg=\"1\"\u002F\u003E .\u003C\u002Fp\u003E\u003Cp\u003E5）对样本集中的每一个样本特征 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+x_i+\" alt=\" x_i \" eeimg=\"1\"\u002F\u003E ，转化为新的样本 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+z_i%3DW%5ETx_i+\" alt=\" z_i=W^Tx_i \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Cp\u003E6）得到输出样本集 \u003Cimg src=\"https:\u002F\u002Fwww.zhihu.com\u002Fequation?tex=+D%27%3D%5C%7B%28z_1%2Cy_1%29%2C%28z_2%2Cy_2%29%2C...%2C%28z_m%2Cy_m%29%5C%7D+\" alt=\" D&#39;=\\{(z_1,y_1),(z_2,y_2),...,(z_m,y_m)\\} \" eeimg=\"1\"\u002F\u003E \u003C\u002Fp\u003E\u003Ch3\u003E4. LDA与PCA对比\u003C\u002Fh3\u003E\u003Cp\u003ELDA与PCA都可用于降维，因此有很多相同的地方，也有很多不同的地方\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003E相同点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E  两者均可用于数据降维\u003C\u002Fli\u003E\u003Cli\u003E  两者在降维时均使用了矩阵特征分解的思想\u003C\u002Fli\u003E\u003Cli\u003E  两者都假设数据符合高斯分布\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cb\u003E不同点：\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E  LDA是有监督的降维方法，而PCA是无监督降维方法\u003C\u002Fli\u003E\u003Cli\u003E  当总共有K个类别时，LDA最多降到K-1维，而PCA没有这个限制\u003C\u002Fli\u003E\u003Cli\u003E  LDA除了用于降维，还可以用于分类\u003C\u002Fli\u003E\u003Cli\u003E  LCA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。这点可以从下图形象的看出，在某些数据分布下LDA比PCA降维较优（如下图的左图）。当然，某些数据分布下PCA比LDA降维较优（如下图的右图）。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cfigure data-size=\"normal\"\u003E\u003Cnoscript\u003E\u003Cimg src=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8821ac9f8448b5bf6d39c46c2e94a708_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"787\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8821ac9f8448b5bf6d39c46c2e94a708_r.jpg\"\u002F\u003E\u003C\u002Fnoscript\u003E\u003Cimg src=\"data:image\u002Fsvg+xml;utf8,&lt;svg xmlns=&#39;http:\u002F\u002Fwww.w3.org\u002F2000\u002Fsvg&#39; width=&#39;787&#39; height=&#39;315&#39;&gt;&lt;\u002Fsvg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"787\" data-original=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8821ac9f8448b5bf6d39c46c2e94a708_r.jpg\" data-actualsrc=\"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-8821ac9f8448b5bf6d39c46c2e94a708_b.jpg\"\u002F\u003E\u003C\u002Ffigure\u003E\u003Ch3\u003E5. LDA算法小结\u003C\u002Fh3\u003E\u003Cp\u003ELDA算法既可以用来降维，也可以用来分来，但是目前来说，LDA主要用于降维，在进行与图像识别相关的数据分析时，LDA是一个有力的工具。下面总结一下LDA算法的优缺点。\u003C\u002Fp\u003E\u003Cp\u003E\u003Cb\u003ELDA算法的优点\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E  在降维过程中可以使用类别的先验知识经验，而像PCA这样的无监督学习无法使用先验知识；\u003C\u002Fli\u003E\u003Cli\u003E  LDA在样本分类信息依赖均值而不是方差的时候，比PCA算法较优。\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cp\u003E\u003Cb\u003ELDA算法的缺点\u003C\u002Fb\u003E\u003C\u002Fp\u003E\u003Cul\u003E\u003Cli\u003E  LDA与PCA均不适合对非高斯分布样本进行降维\u003C\u002Fli\u003E\u003Cli\u003E  LDA降维算法最多降到类别数K-1的维度，当降维的维度大于K-1时，则不能使用LDA。当然目前有一些改进的LDA算法可以绕过这个问题\u003C\u002Fli\u003E\u003Cli\u003E  LDA在样本分类信息依赖方差而非均值的时候，降维效果不好\u003C\u002Fli\u003E\u003Cli\u003E  LDA可能过度拟合数据\u003C\u002Fli\u003E\u003C\u002Ful\u003E\u003Cblockquote\u003E参考：\u003Ca href=\"https:\u002F\u002Flink.zhihu.com\u002F?target=https%3A\u002F\u002Fwww.cnblogs.com\u002Fpinard\u002Fp\u002F6244265.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\"\u003E线性判别分析LDA原理总结 - 刘建平Pinard - 博客园\u003C\u002Fa\u003E\u003C\u002Fblockquote\u003E","adminClosedComment":false,"topics":[{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20010182","type":"topic","id":"20010182","name":"数据降维"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F20687563","type":"topic","id":"20687563","name":"降维算法"},{"url":"https:\u002F\u002Fwww.zhihu.com\u002Fapi\u002Fv4\u002Ftopics\u002F19605693","type":"topic","id":"19605693","name":"分类算法"}],"voteupCount":14,"voting":0,"column":{"description":"本专栏主要记录一些学习笔记，目前将持续更新AI圣经中的内容，欢迎大家关注，有问题一起探讨。","canManage":false,"intro":"本专栏将持续更新机器学习方向相关内容","isFollowing":false,"urlToken":"chennan","id":"chennan","articlesCount":10,"acceptSubmission":true,"title":"机器学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fchennan","commentPermission":"all","created":1566806337,"updated":1567925045,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-dac72633cc016530b3d119b21c373542_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_{size}.jpg","uid":"758233788097523712","userType":"people","isFollowing":false,"urlToken":"yao0118","id":"f397cbba9f621e860ad11e100b77dc64","description":"机器学习，深度学习","name":"陈楠","isAdvertiser":false,"headline":"从事计算机视觉相关工作","gender":1,"url":"\u002Fpeople\u002Ff397cbba9f621e860ad11e100b77dc64","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_l.jpg","isOrg":false,"type":"people"},"followers":523,"type":"column"},"commentCount":0,"contributions":[{"id":21604067,"state":"accepted","type":"include","column":{"description":"本专栏主要记录一些学习笔记，目前将持续更新AI圣经中的内容，欢迎大家关注，有问题一起探讨。","canManage":false,"intro":"本专栏将持续更新机器学习方向相关内容","isFollowing":false,"urlToken":"chennan","id":"chennan","articlesCount":10,"acceptSubmission":true,"title":"机器学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fchennan","commentPermission":"all","created":1566806337,"updated":1567925045,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-dac72633cc016530b3d119b21c373542_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_{size}.jpg","uid":"758233788097523712","userType":"people","isFollowing":false,"urlToken":"yao0118","id":"f397cbba9f621e860ad11e100b77dc64","description":"机器学习，深度学习","name":"陈楠","isAdvertiser":false,"headline":"从事计算机视觉相关工作","gender":1,"url":"\u002Fpeople\u002Ff397cbba9f621e860ad11e100b77dc64","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_l.jpg","isOrg":false,"type":"people"},"followers":523,"type":"column"}}],"isTitleImageFullScreen":false,"upvotedFollowees":[],"commercialInfo":{"isCommercial":false,"plugin":{}},"suggestEdit":{"status":false,"reason":"","tip":"","url":"","title":""},"reason":"","annotationAction":[],"canTip":true,"tipjarorsCount":0,"isLabeled":false,"hasPublishingDraft":false,"isFavorited":false,"isNormal":true,"status":0,"shareText":"线性判别分析LDA原理及推导过程（非常详细） - 来自知乎专栏「机器学习」，作者: 陈楠 https:\u002F\u002Fzhuanlan.zhihu.com\u002Fp\u002F79696530 （想看更多？下载 @知乎 App：http:\u002F\u002Fweibo.com\u002Fp\u002F100404711598 ）","canComment":{"status":true,"reason":""},"mcnFpShow":-1}},"columns":{"chennan":{"description":"本专栏主要记录一些学习笔记，目前将持续更新AI圣经中的内容，欢迎大家关注，有问题一起探讨。","canManage":false,"intro":"本专栏将持续更新机器学习方向相关内容","isFollowing":false,"urlToken":"chennan","id":"chennan","articlesCount":10,"acceptSubmission":true,"title":"机器学习","url":"https:\u002F\u002Fzhuanlan.zhihu.com\u002Fchennan","commentPermission":"all","created":1566806337,"updated":1567925045,"imageUrl":"https:\u002F\u002Fpic1.zhimg.com\u002Fv2-dac72633cc016530b3d119b21c373542_b.jpg","author":{"isFollowed":false,"avatarUrlTemplate":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_{size}.jpg","uid":"758233788097523712","userType":"people","isFollowing":false,"urlToken":"yao0118","id":"f397cbba9f621e860ad11e100b77dc64","description":"机器学习，深度学习","name":"陈楠","isAdvertiser":false,"headline":"从事计算机视觉相关工作","gender":1,"url":"\u002Fpeople\u002Ff397cbba9f621e860ad11e100b77dc64","avatarUrl":"https:\u002F\u002Fpic1.zhimg.com\u002F961a81b83e4f59391d0401f0c77ee9ea_l.jpg","isOrg":false,"type":"people"},"followers":523,"type":"column"}},"topics":{},"roundtables":{},"favlists":{},"comments":{},"notifications":{},"ebooks":{},"activities":{},"feeds":{},"pins":{},"promotions":{},"drafts":{},"chats":{},"posts":{},"clubs":{},"clubTags":{}},"currentUser":"3c6b05b6fb1b1e26fdc0c495d2032cf5","account":{"lockLevel":{},"unlockTicketStatus":false,"unlockTicket":null,"challenge":[],"errorStatus":false,"message":"","isFetching":false,"accountInfo":{},"urlToken":{"loading":false}},"settings":{"socialBind":null,"inboxMsg":null,"notification":{},"email":{},"privacyFlag":null,"blockedUsers":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"blockedFollowees":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"ignoredTopics":{"isFetching":false,"paging":{"pageNo":1,"pageSize":6},"data":[]},"restrictedTopics":null,"laboratory":{}},"notification":{},"people":{"profileStatus":{},"activitiesByUser":{},"answersByUser":{},"answersSortByVotesByUser":{},"answersIncludedByUser":{},"votedAnswersByUser":{},"thankedAnswersByUser":{},"voteAnswersByUser":{},"thankAnswersByUser":{},"topicAnswersByUser":{},"articlesByUser":{},"articlesSortByVotesByUser":{},"articlesIncludedByUser":{},"pinsByUser":{},"questionsByUser":{},"commercialQuestionsByUser":{},"favlistsByUser":{},"followingByUser":{},"followersByUser":{},"mutualsByUser":{},"followingColumnsByUser":{},"followingQuestionsByUser":{},"followingFavlistsByUser":{},"followingTopicsByUser":{},"publicationsByUser":{},"columnsByUser":{},"allFavlistsByUser":{},"brands":null,"creationsByUser":{},"creationsSortByVotesByUser":{}},"env":{"ab":{"config":{"experiments":[{"expId":"launch-li_qa_btn_text-2","expPrefix":"li_qa_btn_text","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-li_se_media_icon-3","expPrefix":"li_se_media_icon","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-nw_zhuantigaiban-2","expPrefix":"nw_zhuantigaiban","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-nw_zhuantikapian-2","expPrefix":"nw_zhuantikapian","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_answer_update-2","expPrefix":"qa_answer_update","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-qa_column_invite-2","expPrefix":"qa_column_invite","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_filter-6","expPrefix":"rec_filter","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_category-2","expPrefix":"rec_km_category","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_item_cf-2","expPrefix":"rec_km_item_cf","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_recallnum-2","expPrefix":"rec_km_recallnum","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_special-2","expPrefix":"rec_km_special","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_km_zann-2","expPrefix":"rec_km_zann","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_paid_mix-4","expPrefix":"rec_paid_mix","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_prerank_heat-2","expPrefix":"rec_prerank_heat","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-rec_recall_score-10","expPrefix":"rec_recall_score","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_collegecm-3","expPrefix":"se_collegecm","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_hot__timebox-2","expPrefix":"se_hot__timebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_ios_spb309-5","expPrefix":"se_ios_spb309","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_payconsult-3","expPrefix":"se_payconsult","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_p_slideshow-2","expPrefix":"se_p_slideshow","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_subtext-2","expPrefix":"se_subtext","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_wannasearch-5","expPrefix":"se_wannasearch","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_webtimebox-2","expPrefix":"se_webtimebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-se_whitelist-2","expPrefix":"se_whitelist","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-top_vote-2","expPrefix":"top_vote","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-tp_club_qa-2","expPrefix":"tp_club_qa","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_bigone-10","expPrefix":"us_bigone","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_foltopic_user-10","expPrefix":"us_foltopic_user","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_newguide3-11","expPrefix":"us_newguide3","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_notification-2","expPrefix":"us_notification","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-us_n_web_msg-5","expPrefix":"us_n_web_msg","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_video_ad-1","expPrefix":"vd_video_ad","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"launch-vd_zvideo_lic-2","expPrefix":"vd_zvideo_lic","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_card_ab_test-7","expPrefix":"qa_card_ab_test","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_topic_onebox-10","expPrefix":"li_topic_onebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_banner_type-16","expPrefix":"li_banner_type","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_magazine_sec-8","expPrefix":"li_magazine_sec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"li_se_video_sec-8","expPrefix":"li_se_video_sec","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_entity_model2-1","expPrefix":"se_entity_model2","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_opm-4","expPrefix":"se_opm","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_aa_base-1","expPrefix":"se_aa_base","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"se_zu_onebox-2","expPrefix":"se_zu_onebox","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_bignew-3","expPrefix":"us_bignew","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_update-8","expPrefix":"us_update","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_leave_rcmd-3","expPrefix":"us_leave_rcmd","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"us_broadcast2-5","expPrefix":"us_broadcast2","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_native_ans-10","expPrefix":"top_native_ans","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_hotlist_ui-1","expPrefix":"top_hotlist_ui","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"top_billhead-8","expPrefix":"top_billhead","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_expslotpaid-10","expPrefix":"rec_expslotpaid","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_slotpaidexp-9","expPrefix":"rec_slotpaidexp","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"rec_test_aa1-7","expPrefix":"rec_test_aa1","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false},{"expId":"qa_paycqaedit-2","expPrefix":"qa_paycqaedit","isDynamicallyUpdated":true,"isRuntime":false,"includeTriggerInfo":false}],"params":[{"id":"se_col_boost","type":"String","value":"1","chainId":"_all_"},{"id":"se_site_onebox","type":"String","value":"0","chainId":"_all_"},{"id":"se_payconsult","type":"String","value":"5","chainId":"_all_"},{"id":"gue_goods_card","type":"String","value":"0"},{"id":"zr_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"zr_recall_heatscore","type":"String","value":"4_6","chainId":"_all_"},{"id":"ls_videoad","type":"String","value":"2","chainId":"_all_"},{"id":"se_entity_model","type":"String","value":"0","chainId":"_all_"},{"id":"se_whitelist","type":"String","value":"1","chainId":"_all_"},{"id":"se_hot_timebox","type":"String","value":"1","chainId":"_all_"},{"id":"zr_paid_answer_mix","type":"String","value":"mixed_15","chainId":"_all_"},{"id":"se_expired_ob","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr","type":"String","value":"0","chainId":"_all_"},{"id":"se_college_cm","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bigone","type":"String","value":"1","chainId":"_all_"},{"id":"li_paid_answer_exp","type":"String","value":"0","chainId":"_all_"},{"id":"zr_ans_rec","type":"String","value":"gbrank","chainId":"_all_"},{"id":"zr_km_feed_nlp","type":"String","value":"old","chainId":"_all_"},{"id":"zr_km_feed_prerank","type":"String","value":"new","chainId":"_all_"},{"id":"zr_km_style","type":"String","value":"base","chainId":"_all_"},{"id":"se_hotsearch","type":"String","value":"1","chainId":"_all_"},{"id":"se_lottery","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_metacard","type":"String","value":"1","chainId":"_all_"},{"id":"qap_thanks","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_item_prerank","type":"String","value":"old","chainId":"_all_"},{"id":"zr_rel_search","type":"String","value":"base","chainId":"_all_"},{"id":"soc_yxzl_zcfw","type":"String","value":"0","chainId":"_all_"},{"id":"pf_creator_card","type":"String","value":"1","chainId":"_all_"},{"id":"ug_follow_topic_1","type":"String","value":"2","chainId":"_all_"},{"id":"ls_zvideo_license","type":"String","value":"1","chainId":"_all_"},{"id":"ls_fmp4","type":"String","value":"0","chainId":"_all_"},{"id":"se_multi_task_new","type":"String","value":"0","chainId":"_all_"},{"id":"soc_leave_recommend","type":"String","value":"2","chainId":"_all_"},{"id":"ug_zero_follow_0","type":"String","value":"0","chainId":"_all_"},{"id":"se_college","type":"String","value":"default","chainId":"_all_"},{"id":"li_qa_new_cover","type":"String","value":"1","chainId":"_all_"},{"id":"zr_rewrite_query","type":"String","value":"0","chainId":"_all_"},{"id":"se_spb309","type":"String","value":"0","chainId":"_all_"},{"id":"se_billboardsearch","type":"String","value":"0","chainId":"_all_"},{"id":"se_time_threshold","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sticky_android","type":"String","value":"2","chainId":"_all_"},{"id":"soc_zcfw_broadcast","type":"String","value":"0","chainId":"_all_"},{"id":"gue_new_special_page","type":"String","value":"1"},{"id":"zr_art_rec","type":"String","value":"base","chainId":"_all_"},{"id":"se_webmajorob","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr_user","type":"String","value":"0","chainId":"_all_"},{"id":"top_root","type":"String","value":"0","chainId":"_all_"},{"id":"soc_stickypush","type":"String","value":"0","chainId":"_all_"},{"id":"li_se_heat","type":"String","value":"1","chainId":"_all_"},{"id":"web_answer_list_ad","type":"String","value":"1"},{"id":"se_duration","type":"String","value":"0","chainId":"_all_"},{"id":"zr_book_chap","type":"String","value":"0","chainId":"_all_"},{"id":"se_likebutton","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_dnn_cp","type":"String","value":"0","chainId":"_all_"},{"id":"se_mclick1","type":"String","value":"0","chainId":"_all_"},{"id":"se_waterfall","type":"String","value":"0","chainId":"_all_"},{"id":"zr_prerank_heatscore","type":"String","value":"true","chainId":"_all_"},{"id":"zr_infinity_member","type":"String","value":"close","chainId":"_all_"},{"id":"zr_video_rank","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_auto_syn","type":"String","value":"0","chainId":"_all_"},{"id":"se_movietab","type":"String","value":"1","chainId":"_all_"},{"id":"top_quality","type":"String","value":"0","chainId":"_all_"},{"id":"se_rel_search","type":"String","value":"0","chainId":"_all_"},{"id":"zr_rec_answer_cp","type":"String","value":"close","chainId":"_all_"},{"id":"se_webtimebox","type":"String","value":"1","chainId":"_all_"},{"id":"se_search_feed","type":"String","value":"N","chainId":"_all_"},{"id":"zr_video_recall","type":"String","value":"current_recall","chainId":"_all_"},{"id":"se_ios_spb309","type":"String","value":"1","chainId":"_all_"},{"id":"se_hotmore","type":"String","value":"0","chainId":"_all_"},{"id":"se_websearch","type":"String","value":"3","chainId":"_all_"},{"id":"ug_newtag","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_section","type":"String","value":"1","chainId":"_all_"},{"id":"qap_question_visitor","type":"String","value":" 0","chainId":"_all_"},{"id":"zr_km_slot_style","type":"String","value":"event_card","chainId":"_all_"},{"id":"se_mclick","type":"String","value":"0","chainId":"_all_"},{"id":"se_aa_base","type":"String","value":"0","chainId":"_all_"},{"id":"se_preset_tech","type":"String","value":"0","chainId":"_all_"},{"id":"se_use_zitem","type":"String","value":"0","chainId":"_all_"},{"id":"web_n_web_msg","type":"String","value":"1"},{"id":"top_ydyq","type":"String","value":"X","chainId":"_all_"},{"id":"gue_thanks","type":"String","value":"0"},{"id":"zr_new_commodity","type":"String","value":"0","chainId":"_all_"},{"id":"qap_question_author","type":"String","value":"0","chainId":"_all_"},{"id":"zr_answer_rec_cp","type":"String","value":"open","chainId":"_all_"},{"id":"se_colorfultab","type":"String","value":"1","chainId":"_all_"},{"id":"top_ebook","type":"String","value":"0","chainId":"_all_"},{"id":"web_sem_ab","type":"String","value":"1"},{"id":"li_tjys_ec_ab","type":"String","value":"0","chainId":"_all_"},{"id":"gue_card_test","type":"String","value":"5"},{"id":"se_go_ztext","type":"String","value":"0","chainId":"_all_"},{"id":"se_multianswer","type":"String","value":"0","chainId":"_all_"},{"id":"se_cardrank_1","type":"String","value":"0","chainId":"_all_"},{"id":"se_subtext","type":"String","value":"1","chainId":"_all_"},{"id":"se_adxtest","type":"String","value":"1","chainId":"_all_"},{"id":"li_se_media_icon","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_btn_text","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_topic_zann","type":"String","value":"new","chainId":"_all_"},{"id":"ug_follow_answerer","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_hotlist_ui","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zuichangfangwen","type":"String","value":"0","chainId":"_all_"},{"id":"pf_noti_entry_num","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_category","type":"String","value":"open","chainId":"_all_"},{"id":"zr_km_recall_num","type":"String","value":"open","chainId":"_all_"},{"id":"tp_sft_v2","type":"String","value":"d","chainId":"_all_"},{"id":"soc_zcfw_badcase","type":"String","value":"0","chainId":"_all_"},{"id":"pf_newguide_vertical","type":"String","value":"0","chainId":"_all_"},{"id":"web_question_invite","type":"String","value":"B"},{"id":"zr_item_nn_recall","type":"String","value":"close","chainId":"_all_"},{"id":"zr_km_answer","type":"String","value":"open_cvr","chainId":"_all_"},{"id":"zr_filter","type":"String","value":"ignore_topics","chainId":"_all_"},{"id":"tp_header_style","type":"String","value":"1","chainId":"_all_"},{"id":"tp_m_intro_re_topic","type":"String","value":"1","chainId":"_all_"},{"id":"soc_special","type":"String","value":"0","chainId":"_all_"},{"id":"web_ad_banner","type":"String","value":"0"},{"id":"qap_ques_invite","type":"String","value":"0","chainId":"_all_"},{"id":"tp_qa_toast","type":"String","value":"1","chainId":"_all_"},{"id":"se_new_topic","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr_pyc","type":"String","value":"0","chainId":"_all_"},{"id":"se_ctr_topic","type":"String","value":"0","chainId":"_all_"},{"id":"tp_sft","type":"String","value":"a","chainId":"_all_"},{"id":"top_native_answer","type":"String","value":"6","chainId":"_all_"},{"id":"li_android_vip","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_special","type":"String","value":"open","chainId":"_all_"},{"id":"se_entity_model_14","type":"String","value":"0","chainId":"_all_"},{"id":"soc_update","type":"String","value":"0","chainId":"_all_"},{"id":"soc_zcfw_shipinshiti","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_recall","type":"String","value":"default","chainId":"_all_"},{"id":"se_dnn_unbias","type":"String","value":"1","chainId":"_all_"},{"id":"se_topicfeed","type":"String","value":"0","chainId":"_all_"},{"id":"se_club_post","type":"String","value":"5","chainId":"_all_"},{"id":"se_ctx","type":"String","value":"0","chainId":"_all_"},{"id":"web_answer_update","type":"String","value":"1"},{"id":"zw_payc_qaedit","type":"String","value":"0","chainId":"_all_"},{"id":"zr_km_item_cf","type":"String","value":"open","chainId":"_all_"},{"id":"se_amovietab","type":"String","value":"1","chainId":"_all_"},{"id":"ug_fw_answ_aut_1","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_billboardhead","type":"String","value":"2","chainId":"_all_"},{"id":"pf_foltopic_usernum","type":"String","value":"50","chainId":"_all_"},{"id":"ug_follow_answerer_0","type":"String","value":"0","chainId":"_all_"},{"id":"li_salt_hot","type":"String","value":"0","chainId":"_all_"},{"id":"gue_zhuantikapian","type":"String","value":"1"},{"id":"tp_meta_card","type":"String","value":"0","chainId":"_all_"},{"id":"tp_club_qa_pic","type":"String","value":"1","chainId":"_all_"},{"id":"tp_topic_head","type":"String","value":"0","chainId":"_all_"},{"id":"top_hotcommerce","type":"String","value":"1","chainId":"_all_"},{"id":"top_test_4_liguangyi","type":"String","value":"1","chainId":"_all_"},{"id":"soc_zcfw_broadcast2","type":"String","value":"0","chainId":"_all_"},{"id":"li_purchase_test","type":"String","value":"0","chainId":"_all_"},{"id":"qap_payc_invite","type":"String","value":"0","chainId":"_all_"},{"id":"se_dnn_mt","type":"String","value":"0","chainId":"_all_"},{"id":"ug_goodcomment_0","type":"String","value":"1","chainId":"_all_"},{"id":"ls_zvideo_trans","type":"String","value":"0","chainId":"_all_"},{"id":"tsp_vote","type":"String","value":"2","chainId":"_all_"},{"id":"ls_zvideo_like","type":"String","value":"0","chainId":"_all_"},{"id":"gue_anonymous","type":"String","value":"show"},{"id":"web_heifetz_grow_ad","type":"String","value":"1"},{"id":"li_vip_no_ad_mon","type":"String","value":"0","chainId":"_all_"},{"id":"se_zu_onebox","type":"String","value":"3","chainId":"_all_"},{"id":"tp_club_header","type":"String","value":"1","chainId":"_all_"},{"id":"soc_bignew","type":"String","value":"1","chainId":"_all_"},{"id":"soc_notification","type":"String","value":"1","chainId":"_all_"},{"id":"sem_up_growth","type":"String","value":"in_app","chainId":"_all_"},{"id":"web_column_auto_invite","type":"String","value":"1"},{"id":"se_ad_index","type":"String","value":"10","chainId":"_all_"},{"id":"zr_intervene","type":"String","value":"0","chainId":"_all_"},{"id":"se_featured","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_3","type":"String","value":"0","chainId":"_all_"},{"id":"se_backsearch","type":"String","value":"0","chainId":"_all_"},{"id":"ug_zero_follow","type":"String","value":"0","chainId":"_all_"},{"id":"li_album_liutongab","type":"String","value":"0","chainId":"_all_"},{"id":"li_pay_banner_type","type":"String","value":"3","chainId":"_all_"},{"id":"zr_slot_cold_start","type":"String","value":"aver","chainId":"_all_"},{"id":"se_sug","type":"String","value":"0","chainId":"_all_"},{"id":"se_wannasearch","type":"String","value":"a","chainId":"_all_"},{"id":"li_cln_vl","type":"String","value":"no","chainId":"_all_"},{"id":"li_video_section","type":"String","value":"1","chainId":"_all_"},{"id":"web_answerlist_ad","type":"String","value":"0"},{"id":"se_topiclabel","type":"String","value":"1","chainId":"_all_"},{"id":"se_famous","type":"String","value":"1","chainId":"_all_"},{"id":"tp_club_qa","type":"String","value":"1","chainId":"_all_"},{"id":"top_v_album","type":"String","value":"1","chainId":"_all_"},{"id":"soc_ri_merge","type":"String","value":"0","chainId":"_all_"},{"id":"li_ebook_audio","type":"String","value":"0","chainId":"_all_"},{"id":"zr_expslotpaid","type":"String","value":"10","chainId":"_all_"},{"id":"zr_test_aa1","type":"String","value":"1","chainId":"_all_"},{"id":"se_webrs","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_2","type":"String","value":"1","chainId":"_all_"},{"id":"zr_km_prerank","type":"String","value":"new","chainId":"_all_"},{"id":"zr_article_new","type":"String","value":"close","chainId":"_all_"},{"id":"zr_slotpaidexp","type":"String","value":"9","chainId":"_all_"},{"id":"se_zu_recommend","type":"String","value":"0","chainId":"_all_"},{"id":"se_mobileweb","type":"String","value":"1","chainId":"_all_"},{"id":"se_ltr_user","type":"String","value":"0","chainId":"_all_"},{"id":"li_hot_score_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_ab","type":"String","value":"0","chainId":"_all_"},{"id":"se_ltr_cp_new","type":"String","value":"0","chainId":"_all_"},{"id":"top_universalebook","type":"String","value":"1","chainId":"_all_"},{"id":"pf_fuceng","type":"String","value":"1","chainId":"_all_"},{"id":"li_qa_cover","type":"String","value":"old","chainId":"_all_"},{"id":"zr_video_rank_nn","type":"String","value":"new_rank","chainId":"_all_"},{"id":"se_perf","type":"String","value":"0","chainId":"_all_"},{"id":"top_new_feed","type":"String","value":"5","chainId":"_all_"},{"id":"se_p_slideshow","type":"String","value":"1","chainId":"_all_"},{"id":"se_cardrank_4","type":"String","value":"1","chainId":"_all_"},{"id":"se_agency","type":"String","value":" 0","chainId":"_all_"},{"id":"zw_sameq_sorce","type":"String","value":"999","chainId":"_all_"},{"id":"tp_qa_metacard_top","type":"String","value":"top","chainId":"_all_"}],"chains":[{"chainId":"_all_"}]},"triggers":{}},"userAgent":{"Edge":false,"Wechat":false,"Weibo":false,"QQ":false,"MQQBrowser":false,"Qzone":false,"Mobile":false,"Android":false,"iOS":false,"isAppleDevice":false,"Zhihu":false,"ZhihuHybrid":false,"isBot":false,"Tablet":false,"UC":false,"Sogou":false,"Qihoo":false,"Baidu":false,"BaiduApp":false,"Safari":false,"GoogleBot":false,"isWebView":false,"origin":"Mozilla\u002F5.0 (Windows NT 10.0; WOW64) AppleWebKit\u002F537.36 (KHTML, like Gecko) Chrome\u002F69.0.3947.100 Safari\u002F537.36"},"appViewConfig":{},"ctx":{"path":"\u002Fp\u002F79696530"},"trafficSource":"production","edition":{"baidu":false,"sogou":false,"baiduBeijing":false,"sogouBeijing":false,"sogouInput":false},"theme":"light","enableShortcut":true,"referer":"https:\u002F\u002Fwww.zhihu.com\u002Fsearch?type=content&q=%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90LDA","conf":{},"ipInfo":{"cityName":"Beijing","countryName":"China","regionName":"Beijing","countryCode":"CN"},"logged":true},"me":{"columnContributions":[]},"label":{"recognizerLists":{}},"ecommerce":{},"comments":{"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"parent":{}},"commentsV2":{"stickers":[],"commentWithPicPermission":{},"notificationsComments":{},"pagination":{},"collapsed":{},"reverse":{},"reviewing":{},"conversation":{},"conversationMore":{},"parent":{}},"pushNotifications":{"default":{"isFetching":false,"isDrained":false,"ids":[]},"follow":{"isFetching":false,"isDrained":false,"ids":[]},"vote_thank":{"isFetching":false,"isDrained":false,"ids":[]},"currentTab":"default","notificationsCount":{"default":0,"follow":0,"vote_thank":0}},"messages":{"data":{},"currentTab":"common","messageCount":0},"register":{"registerValidateSucceeded":null,"registerValidateErrors":{},"registerConfirmError":null,"sendDigitsError":null,"registerConfirmSucceeded":null},"login":{"loginUnregisteredError":false,"loginBindWechatError":false,"loginConfirmError":null,"sendDigitsError":null,"needSMSIdentify":false,"validateDigitsError":false,"loginConfirmSucceeded":null,"qrcodeLoginToken":"","qrcodeLoginScanStatus":0,"qrcodeLoginError":null,"qrcodeLoginReturnNewToken":false},"active":{"sendDigitsError":null,"activeConfirmSucceeded":null,"activeConfirmError":null},"switches":{},"captcha":{"captchaNeeded":false,"captchaValidated":false,"captchaBase64String":null,"captchaValidationMessage":null,"loginCaptchaExpires":false},"sms":{"supportedCountries":[]},"chat":{"chats":{},"inbox":{"recents":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"strangers":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"friends":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"search":{"isFetching":false,"isDrained":false,"isPrevDrained":false,"result":[],"next":null,"key":null},"config":{"newCount":0,"strangerMessageSwitch":false,"strangerMessageUnread":false,"friendCount":0}},"global":{"isChatMqttExisted":false}},"emoticons":{"emoticonGroupList":[],"emoticonGroupDetail":{}},"creator":{"currentCreatorUrlToken":null,"homeData":{"recommendQuestions":[]},"tools":{"question":{"invitationCount":{"questionFolloweeCount":0,"questionTotalCount":0},"goodatTopics":[]},"customPromotion":{"itemLists":{}},"recommend":{"recommendTimes":{}}},"explore":{"academy":{"tabs":[],"article":{}}},"rights":[],"rightsStatus":{},"levelUpperLimit":10,"account":{"growthLevel":{}},"applyStatus":{}},"recommendation":{"homeRecommendations":[]},"shareTexts":{},"articles":{"voters":{}},"previewPost":{},"favlists":{"relations":{}},"columns":{"voters":{}},"reward":{"answer":{},"article":{},"question":{}},"video":{"data":{},"shareVideoDetail":{},"last":{}},"topstory":{"recommend":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"follow":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followWonderful":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"sidebar":null,"announcement":{},"hotListCategories":[],"hotList":[],"guestFeeds":{"isFetching":false,"isDrained":false,"afterId":0,"items":[],"next":null},"followExtra":{"isNewUser":null,"isFetched":false,"followCount":0,"followers":[]}},"readStatus":{},"column":{},"requestColumn":{"categories":[],"error":null},"articleContribution":{"contributeRequests":[],"deleteContributeIdList":[],"handledContributeIdList":[],"recommendedColumns":[],"pinnedColumns":[],"sentContributeRequestsIdList":[null,null,null,null,null,null,null,null,null,null,null,null,null,null,null,"chennan",null]},"columnContribution":{"contributeRequests":[],"autoInviteEnabled":false,"recommendedContributors":[],"contributionInvitation":null},"draftHistory":{"history":{},"drafts":{}},"upload":{},"articleDraft":{"titleImage":"","titleImageSize":{},"isTitleImageFullScreen":false,"canTitleImageFullScreen":false,"title":"","titleImageUploading":false,"error":"","content":"","draftLoading":false,"updating":false,"globalLoading":false,"pendingVideo":{"resource":null,"error":null},"deleteFail":{"fail":false},"recommendTopics":[],"selectedColumn":0},"articleDrafts":{"isDrained":false,"isLoading":false,"items":[]},"columnAutocomplete":{"users":[],"friends":[]},"mcn":{"bindInfo":{},"memberCategoryList":[],"categoryList":[],"taobaoCategoryList":[]}},"fetchHost":"www.zhihu.com","subAppName":"column"}</script><script src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/vendor.1c14c07589f9cc850a64.js"></script><script src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/column.app.16c32f0129c1cb8acce9.js"></script><script></script><script src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/hm.js" async=""></script><div><div style="display: none;">想来知乎工作？请发送邮件到 jobs@zhihu.com</div></div><script src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/zap.js"></script><script src="./线性判别分析LDA原理及推导过程（非常详细） - 知乎_files/js"></script><div><div><div></div></div></div><div><div><div class="Editable-languageSuggestions" style="left: -1179px; top: -999px;"><div><div class="Popover"><label class="Editable-languageSuggestionsInput Input-wrapper"><input autocomplete="off" role="combobox" aria-expanded="false" aria-autocomplete="list" aria-activedescendant="AutoComplete14-0" id="Popover13-toggle" aria-haspopup="true" aria-owns="Popover13-content" class="Input" placeholder="选择语言" value=""><svg class="Zi Zi--Select" fill="#afbdcf" viewBox="0 0 24 24" width="24" height="24"><path d="M12 16.183l2.716-2.966a.757.757 0 0 1 1.064.001.738.738 0 0 1 0 1.052l-3.247 3.512a.758.758 0 0 1-1.064 0L8.22 14.27a.738.738 0 0 1 0-1.052.758.758 0 0 1 1.063 0L12 16.183zm0-9.365L9.284 9.782a.758.758 0 0 1-1.064 0 .738.738 0 0 1 0-1.052l3.248-3.512a.758.758 0 0 1 1.065 0L15.78 8.73a.738.738 0 0 1 0 1.052.757.757 0 0 1-1.063.001L12 6.818z" fill-rule="evenodd"></path></svg></label></div></div></div></div></div></body></html>